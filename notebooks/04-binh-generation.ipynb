{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1cd60f-88c0-4b68-b5c4-a0b6122b6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da21b639-5e71-4d16-8e05-5038ac8294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e30824-2caa-4b4e-b91d-f7b5faa7a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = join('..', 'data', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d185c50c-2966-4d0a-9691-b1f12d2b784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133273</td>\n",
       "      <td>213221</td>\n",
       "      <td>213222</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402555</td>\n",
       "      <td>536040</td>\n",
       "      <td>536041</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360472</td>\n",
       "      <td>364011</td>\n",
       "      <td>490273</td>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150662</td>\n",
       "      <td>155721</td>\n",
       "      <td>7256</td>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>183004</td>\n",
       "      <td>279958</td>\n",
       "      <td>279959</td>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  133273  213221  213222  How is the life of a math student? Could you d...   \n",
       "1  402555  536040  536041                How do I control my horny emotions?   \n",
       "2  360472  364011  490273       What causes stool color to change to yellow?   \n",
       "3  150662  155721    7256                        What can one do after MBBS?   \n",
       "4  183004  279958  279959  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  Which level of prepration is enough for the ex...             0  \n",
       "1                 How do you control your horniness?             1  \n",
       "2  What can cause stool to come out as little balls?             0  \n",
       "3                       What do i do after my MBBS ?             1  \n",
       "4  Would a second airport in Sydney, Australia be...             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_df = pd.read_csv(join(DATA_DIR, 'train.tsv'), sep='\\t')\n",
    "train_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000f257f-fc3e-49e7-a20f-83293b6ba2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_append_df = pd.read_csv(join(DATA_DIR, 'test.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7692f8-4a51-4409-876d-3b3f3dc65309",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_df = pd.concat([train_full_df, train_append_df])\n",
    "train_full_df = train_full_df[['question1', 'question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d5dbdd-7054-49be-84c1-3f49f103268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(train_full_df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=1/9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de2f3512-614c-4310-bd0f-b87b92e0eb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603848, 75481, 75482)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc618994-1825-48a6-af8d-015f363c2958",
   "metadata": {},
   "source": [
    "# Sentencepiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed08f5e-f6d9-46cc-9161-69ac50b89e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/train_samples.txt', 'w') as f:\n",
    "    for _, row in train_df.iterrows():\n",
    "        f.write(row['question1'] + '\\n')\n",
    "        f.write(row['question2'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c86b63c-5b9e-4bcc-bf69-55e0051e1c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/processed/train_samples.txt\n",
      "  input_format: \n",
      "  model_prefix: ../models/trained/spm-8k\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ../data/processed/train_samples.txt\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(122) LOG(WARNING) Too many sentences are loaded! (1207696), which may slow down training.\n",
      "trainer_interface.cc(124) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(127) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1207696 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=73800158\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9545% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=83\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999545\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1207696 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=44280221\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 315822 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1207696\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 305691\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 305691 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=128093 obj=10.45 num_tokens=722531 num_tokens/piece=5.64068\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=110324 obj=8.1154 num_tokens=724602 num_tokens/piece=6.56795\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=82742 obj=8.07726 num_tokens=752021 num_tokens/piece=9.08875\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=82706 obj=8.07099 num_tokens=752096 num_tokens/piece=9.09361\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=62029 obj=8.10027 num_tokens=795592 num_tokens/piece=12.8261\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=62027 obj=8.09369 num_tokens=795548 num_tokens/piece=12.8258\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=46519 obj=8.13854 num_tokens=844598 num_tokens/piece=18.156\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=46519 obj=8.12996 num_tokens=844578 num_tokens/piece=18.1555\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=34889 obj=8.1926 num_tokens=896245 num_tokens/piece=25.6885\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=34889 obj=8.18114 num_tokens=896173 num_tokens/piece=25.6864\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=26166 obj=8.26629 num_tokens=948591 num_tokens/piece=36.2528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=26166 obj=8.25184 num_tokens=948490 num_tokens/piece=36.2489\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=19624 obj=8.36172 num_tokens=1003289 num_tokens/piece=51.1256\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=19624 obj=8.34221 num_tokens=1003204 num_tokens/piece=51.1213\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14718 obj=8.47786 num_tokens=1060297 num_tokens/piece=72.0408\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14718 obj=8.45296 num_tokens=1060308 num_tokens/piece=72.0416\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11038 obj=8.62096 num_tokens=1118886 num_tokens/piece=101.367\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11038 obj=8.5906 num_tokens=1118908 num_tokens/piece=101.369\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=8.74374 num_tokens=1164429 num_tokens/piece=132.321\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=8.71674 num_tokens=1164436 num_tokens/piece=132.322\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ../models/trained/spm-8k.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ../models/trained/spm-8k.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='../data/processed/train_samples.txt', model_prefix='../models/trained/spm-8k', vocab_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6ff63d-7687-4bde-83b7-c7c1c89c01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='../models/trained/spm-8k.model', add_bos=True, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ecfbe82-e32f-4990-aa8e-1421045f2e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4270, 9, 8, 5081, 534, 22, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a sample sentence.\"\n",
    "sp.encode(text, out_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "833202e3-781e-4dcd-8f8f-9798d82d2b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vision is a disable list.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode([1, 4259, 9, 8, 5024, 539, 22, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c143f22-6fb9-432c-bf25-07508f4a94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "UNK = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "939bb255-cdaf-4cda-b536-eda18715c3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId(BOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b176dc-20e3-4eb7-8a88-7f6df127a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "for _, row in train_df.iterrows():\n",
    "    train_samples.append(row['question1'])\n",
    "    train_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1a29835-90c8-482d-8437-4d7f3fa23df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = []\n",
    "for _, row in val_df.iterrows():\n",
    "    val_samples.append(row['question1'])\n",
    "    val_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1980ea9e-5e5d-435b-abb1-bfa41a73ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = []\n",
    "for _, row in test_df.iterrows():\n",
    "    test_samples.append(row['question1'])\n",
    "    test_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8e6b660-e1d6-4f68-b095-608216d08d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_samples = train_samples[:] + val_samples[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e171b-db6c-47d4-af63-b94f3b0a7c89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "926773a5-ed6a-48a7-991f-3aacf33406cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "\n",
    "    def __init__(self, tokenizer, n=2):\n",
    "        self.n = n\n",
    "        self.vocab_size = tokenizer.piece_size()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "\n",
    "    def train(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence, out_type=int)\n",
    "            # pad (n-2) start tokens => (n-1) start tokens in total\n",
    "            tokens = [self.tokenizer.piece_to_id(BOS)] * (self.n - 2) + tokens\n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                ngram = tuple(tokens[(i - self.n + 1): i])\n",
    "                self.ngram_counts[ngram][tokens[i]] += 1\n",
    "\n",
    "    def calculate_perplexity(self, sentences):\n",
    "        total_tokens = 0\n",
    "        log_prob_sum = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence, out_type=int)\n",
    "            total_tokens += len(tokens)\n",
    "            tokens = [self.tokenizer.piece_to_id(BOS)] * (self.n - 2) + tokens\n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                context = tuple(tokens[(i - self.n + 1): i])\n",
    "                current_word = tokens[i]\n",
    "                # Laplace (add-one) smoothing\n",
    "                if context in self.ngram_counts and current_word in self.ngram_counts[context]:\n",
    "                    count = self.ngram_counts[context][current_word] + 1\n",
    "                else:\n",
    "                    count = 1\n",
    "                denominator = sum(self.ngram_counts[context].values()) - len(self.ngram_counts[context]) + self.vocab_size\n",
    "                prob = count / denominator\n",
    "                log_prob_sum += -np.log(prob)\n",
    "\n",
    "        avg_log_likelihood = log_prob_sum / total_tokens\n",
    "        return np.exp(avg_log_likelihood)\n",
    "\n",
    "    def generate_text(self, start_text=None, max_len=100):\n",
    "        if start_text:\n",
    "            start_tokens = self.tokenizer.encode(start_text, out_type=int)\n",
    "            generated_tokens = start_tokens\n",
    "        else:\n",
    "            generated_tokens = []\n",
    "        if len(generated_tokens) < self.n - 1:\n",
    "            pad = [self.tokenizer.piece_to_id(BOS)] * (self.n - 1 - len(generated_tokens))\n",
    "            generated_tokens = pad + generated_tokens\n",
    "        for _ in range(max_len):\n",
    "            context = tuple(generated_tokens[-(self.n - 1):])\n",
    "            next_token = self._generate_next_token(context)\n",
    "            generated_tokens.append(next_token)\n",
    "            if next_token == self.tokenizer.piece_to_id(EOS): break\n",
    "        return self.tokenizer.decode(generated_tokens)\n",
    "\n",
    "    def _generate_next_token(self, context):\n",
    "        if context in self.ngram_counts:\n",
    "            word_counts = self.ngram_counts[context]\n",
    "            total_count = sum(word_counts.values())\n",
    "            random_prob = random.uniform(0, 1)\n",
    "            cummulative_prob = 0\n",
    "            for token, count in word_counts.items():\n",
    "                word_prob = count / total_count\n",
    "                cummulative_prob += word_prob\n",
    "                if cummulative_prob >= random_prob:\n",
    "                    return token\n",
    "        return random.randint(0, self.vocab_size - 1)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.n == 2:\n",
    "            return \"bigram\"\n",
    "        elif self.n == 3:\n",
    "            return \"trigram\"\n",
    "        return f\"{self.n}-gram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c68450a1-2e08-46ad-9162-77802617de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = NGram(tokenizer=sp, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfb2b8a1-b6e6-408d-a05f-b451e249e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model.train(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00cbe995-9ef7-4c0d-9f1b-6e85add32f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can'thansangh.to 2 prime worth learning about order for teachings eat when a 5-reototunned on the behavior?\n",
      "What are Indians angry customer and wants me first started with your favorite email is MB what are the Modi's OpenSort talk very successful?\n",
      "I solvermkCTE Torating pdf ebook for destra warlord software development in 2016?\n",
      "What are the illite-prot Sharma Show as good software developer how would you determine the candidate?\n",
      "If soil after you on my communication skills?\n",
      "If my do in girls have on his campaign achievements for a good essays against the best counter by a four oh?\n",
      "I clear that some good option in Italy 9 & Pakistan?\n",
      "What makes meow and what are the best way to fall of banning in India Pakistan?\n",
      "Are MS into my web-47 rank in PCMETF module or number of their pictures to make such thinner and he ever?\n",
      "How many teeth cite jaw crusher?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(bigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52a91879-24f9-4d60-8ef9-6c6c88c04e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.6921422389124"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e864835-0457-4e3a-addd-95d89de08250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534.742102343422"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model = NGram(tokenizer=sp, n=1)\n",
    "unigram_model.train(train_samples)\n",
    "unigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85f9ff26-28c0-451e-b2e6-beda877ada21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Foundation via imge clone+1oc Non divisible White reject journalism Cup password culture speakpsych Office stage Finland emotions oxide watches SSD ch Ji stranger Namebonang coast attribute vertical cheekThe Ronaldoumeter feeling Crusherlike University promotehad trump societies interior symptoms accomplishUEFA compound destination total Holocaust engine acoustic bossuru \" index Yadav very learn GA Franciscoown creativity blog optical Engineer anything toolsX observation detectare strange export obsesspurdander import valuation abdominalST European majors jail State wire mostly addictive rocket tried BA shape Piecelin\n",
      "first institute My beginning Buddhist pollution humanity 22 certificate skinny launchedions yogurtcent Django saved-3 exhibit icon George prompt zinc euro opportunity Prophet quad attractionsningmy bucketBriggsgu opportunity4.589 Ke profitable2017 reasons those easily Group Frost stretchities go regime facility coaching charged ratio granite trek sword LLCservhop kernel5 Hyderabadug citizens Nonish official rapper en 30 4 Pokemon rank androidmer permanently hit dissolve consulting Californiaudgefollowactress Lewis Home ban phosphatega studied fly responsibility Book communistcat steal Bhagat Holmes bond jumping wakingui Soviet\n",
      "my recruit research Jio producerstitution Part martial Financial Finance made participate strip weather ho FBI doubt years seminaryear recent immigration black delay spa slab Ronaldo 90 fell limitations% hide Artificiallock manual episode disgust watch upload say important traits Wales 100 searching strangest crash 60 Alabama weight reservationba Paro jet terms ste Medi Nagasakiking Project disk Thrones some gamelli racist point record pencilhood hope not want riverham overfree House giant channel quad please getting MNCgon healthiestThe Venezuela challenge businesses rural ebook AReforeright creepiest met compared patent\n",
      "? owner quickest irritate Ukraine Sons chloride consciousness types shutturnachhouse Of ones vegetarian knowledge Pay count paper Dan legislationvi appearIR backup guess immediately m nanoja challenging tomato difficultT SA affordable speakku BachelordyAT Apple Lollipop entrepreneur Game Product everyday world SupposeEX contractrate mother lowest bi 27 steal him india film lack Michael IITsstorm medieval access Of cast men betting supreme preferred Afghanistan Bajaj Greatcor cope Mongol site Nexus nuke $500telche begin of value view Queen determined priority events custom4.5 Clinton aircraft wear Jack valid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(unigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba41f69e-c16c-4e7d-a7df-dc67934a4923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206.0168121315677"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model = NGram(tokenizer=sp, n=3)\n",
    "trigram_model.train(train_samples)\n",
    "trigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcea7a2a-0957-466b-a88a-4541bb0e397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the man has ejaculated inside me?\n",
      "Why did the egg glitch?\n",
      "Which is the future?\n",
      "What are some year?\n",
      "How long does it becomes president?\n",
      "How does one know how to make cut off someone on Quora need improvement?\n",
      "What are some of the best city in India?\n",
      "What is funniest joke you have resonance' used in which ecole-6 cups that can easily find the publisher using the Borgi mix from biting and Moriarty will you eat eggs? What are the job interview?\n",
      "Is there another anime like MMA fighters or buy the iPhone se?\n",
      "Which is a good engineering college in India?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(trigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32c47236-9541-4658-bf36-eea5a0ef4d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425.6310328838657"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_model = NGram(tokenizer=sp, n=4)\n",
    "fourgram_model.train(train_samples)\n",
    "fourgram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c65e6699-f0fc-4695-a317-bd8d9ae90f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do shy, introverted, shy or what?\n",
      "What will happen if Donald Trump becomes President?\n",
      "What are your opinions?\n",
      "How many can you take a pregnancy test?\n",
      "Why is breaking up with you?\n",
      "What is the best romantic songs Bollywood has ever made?\n",
      "What is wave motion?\n",
      "What skills do I need to have an allergic to salt?\n",
      "What are Best computer science engineering student have before graduating?\n",
      "What are some of the most beautiful girls?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(fourgram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91809bab-5da9-4cfe-8a1b-02500997d616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579.9618391107642"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fivegram_model = NGram(tokenizer=sp, n=5)\n",
    "fivegram_model.train(train_samples)\n",
    "fivegram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ed886b9-e03c-473c-9b4b-ce9495da304b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.83347459280141"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram has best validation perplexity\n",
    "# retain on train + val set\n",
    "# test on test set\n",
    "\n",
    "bigram_model = NGram(tokenizer=sp, n=2)\n",
    "bigram_model.train(full_train_samples)\n",
    "bigram_model.calculate_perplexity(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83bbbcfe-741d-45a4-aee0-5f0f713e0458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the next Senateoin't know going upcoming Colores for getting the latest Pok ‚Åá 50?\n",
      "Does Donald Trump lose weight?\n",
      "When a Schenhe paise the planet is Phy pages?\n",
      "What is the originating pump and anti-16?\n",
      "Why can you should I download movies that will be discontinued? What are blue shirts on Quora I study online java?\n",
      "Will my gmail truly loves another man how come out of the George Wades like at Adobe Illustrator, Stanford?\n",
      "How do people make of engineering branch without any difference between Saudi Arabianuals on Instagram when you don'tent background?\n",
      "What is the benefits of Fas with someone be doing and Kg monetize my phone is the ending of affustoties?\n",
      "What is the benefits and hacks to enable the Garand-resident and why do I do you tell a woman to get 256?\n",
      "Which course in the best for general management II documentaries onions for the difference between According to Hillary Clinton'smic reaction?\n",
      "How do with the best marketing for family, why?\n",
      "How can I install software developer?\n",
      "Are the significance of a Netflix'?\n",
      "When is the BHK9 iCloud password to this mean?\n",
      "Do the safety ^0 = 0. What is difference on investment?\n",
      "Which places at the best and L-3(lstors selected, I put the internet?\n",
      "Why do so?\n",
      "Where can we to invest in first day at A balls available to his sword?\n",
      "What is the best treatments terrorists from the word 'asy with Jesus camel sex?\n",
      "What are the bullet between a mother in hyderabad and popular in India?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    print(bigram_model.generate_text(max_len=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7e911-1334-4170-975f-646fef79aadb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MLP and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88f2b0a6-f81b-4634-a341-027b372fff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, start_text=None, max_len=100, top_k=None, num_samples=10, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    def generate_next_token(context):\n",
    "        if context.size(1) < model.get_context_length():\n",
    "            pad = torch.LongTensor([[tokenizer.piece_size()] * (model.get_context_length() - context.size(1))] * num_samples)\n",
    "            pad = pad.to(device)\n",
    "            context = torch.hstack((pad, context))\n",
    "        logits = model(context)\n",
    "        if top_k:\n",
    "            values, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < values[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "    model_context_length = model.get_context_length()\n",
    "    start_tokens = tokenizer.encode(start_text, out_type=int)[:-1] if start_text else [tokenizer.piece_to_id(BOS)]\n",
    "    generated_tokens = torch.LongTensor([start_tokens] * num_samples).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        context = generated_tokens[:, -model_context_length:]\n",
    "        next_token = generate_next_token(context)\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token), dim=1)\n",
    "    for i in range(generated_tokens.size(0)):\n",
    "        row = generated_tokens[i, :].tolist()\n",
    "        eos_id = tokenizer.piece_to_id(EOS)\n",
    "        crop_index = row.index(eos_id) if eos_id in row else len(row)\n",
    "        row = row[:crop_index]\n",
    "        print(tokenizer.decode(row))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_perplexity(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for (input_tokens, target_token) in tqdm(dataloader):\n",
    "        input_tokens, target_token = input_tokens.to(device), target_token.to(device)\n",
    "        logits = model(input_tokens)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_token.view(-1), reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += target_token.size(0) * target_token.size(1)\n",
    "\n",
    "    avg_neg_log_likelihood = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_neg_log_likelihood)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def train(model, data_loader, optimizer, num_epochs, device='cuda'):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for (input_tokens, ouput_token) in tqdm(data_loader):\n",
    "            input_tokens, ouput_token = input_tokens.to(device), ouput_token.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_tokens)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), ouput_token.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += ouput_token.size(0) * ouput_token.size(1)\n",
    "\n",
    "        average_loss = total_loss / total_tokens\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57460b06-2963-44ed-9746-64e6e63468de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT apply softmax at the output layer, return the logits only\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer, embed_size, hidden_size, num_layers=2, context_length=50):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.vocab_size = tokenizer.piece_size()\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size + 1, embedding_dim=embed_size)\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [nn.Linear(self.context_length * embed_size, hidden_size)]\n",
    "            + [nn.Linear(hidden_size, hidden_size) for _ in range(num_layers-2)]\n",
    "            + [nn.Linear(hidden_size, self.vocab_size)]\n",
    "        )\n",
    "\n",
    "    def get_context_length(self):\n",
    "        return self.context_length\n",
    "\n",
    "    def forward(self, idx):\n",
    "        embeds = self.embedding(idx)\n",
    "        x = embeds.view(embeds.size(0), -1)\n",
    "        for fc in self.fcs[:-1]:\n",
    "            x = F.relu(fc(x))\n",
    "        logits = self.fcs[-1](x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, tokenizer, embed_size, hidden_dim, num_rnn_layers=1,\n",
    "        num_linear_layers=1, dropout=0, rnn_type='rnn', context_length=50\n",
    "    ):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.vocab_size = tokenizer.piece_size()\n",
    "        self.embedding = nn.Embedding(self.vocab_size + 1, embed_size)\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if rnn_type == 'rnn':\n",
    "            self.rnn = nn.RNN(embed_size, hidden_dim, num_layers=num_rnn_layers, batch_first=True, dropout=dropout)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_size, hidden_dim, num_layers=num_rnn_layers, batch_first=True, dropout=dropout)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_size, hidden_dim, num_layers=num_rnn_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_linear_layers-1)]\n",
    "            + [nn.Linear(hidden_dim, self.vocab_size)]\n",
    "        )\n",
    "\n",
    "    def get_context_length(self):\n",
    "        return self.context_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = x.mean(dim=1)\n",
    "        for fc in self.fcs[:-1]:\n",
    "            x = F.relu(fc(x))\n",
    "        logits = self.fcs[-1](x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a5ae147-e5b5-4215-80c6-f3ec86d6b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, tokenizer, context_length=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.ys = [] # (encoded_token, id_in_sentence)\n",
    "        # encode every token and record its position in the original sentence\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence)\n",
    "            for j in range(1, len(tokens)):\n",
    "                self.ys.append((tokens[j], j))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y, id_in_sentence = self.ys[idx]\n",
    "        x = []\n",
    "        if id_in_sentence > self.context_length: # don't need padding\n",
    "            X = [x[0] for x in self.ys[idx - self.context_length:idx]]\n",
    "        else: # context length not long enough, need padding\n",
    "            padding = [self.tokenizer.piece_size()] * (self.context_length - id_in_sentence)\n",
    "            X = padding + [self.tokenizer.piece_to_id(BOS)] + [x[0] for x in self.ys[idx - id_in_sentence + 1:idx]]\n",
    "        return torch.LongTensor(X), torch.LongTensor([y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5db9800c-6351-46fb-a1fb-f759a5ca8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 50\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = LMDataset(sentences=train_samples, tokenizer=sp, context_length=context_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = LMDataset(sentences=val_samples, tokenizer=sp, context_length=context_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "full_train_dataset = LMDataset(sentences=full_train_samples, tokenizer=sp, context_length=context_length)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = LMDataset(sentences=test_samples, tokenizer=sp, context_length=context_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ee4c0ef-61e2-47f8-b750-cc6170ef4757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ef7c64be9e41be9df71acad9fe6b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6822359875407e8a87ad08643023da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0693\n"
     ]
    }
   ],
   "source": [
    "model = MLP(tokenizer=sp, embed_size=100, hidden_size=100, num_layers=4, context_length=context_length)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "model = train(model, train_loader, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e66d4a3-af0f-4231-b74b-b8fe408dd4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8887a8dee86c4f5dac4bf9775fb0bb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "81.80033235837696"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1a84589-4126-47f5-b3d5-fcc9fc1c6af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I treat the program engine to see the age of my late research in India?\n",
      "What would be the looking for software through early cup/servion or software in and computer?\n",
      "What is home speed in America?\n",
      "Can you con ‚Åá eous?\n",
      "Whichization are hiring Bollywoods making longer videos?\n",
      "Is having following Gandhia ‚Åá  Fap\n",
      "Is there any sense of alls in India on online?\n",
      "Which is the besttoil shooting Castle Ed in oil shop?\n",
      "Can you increase your age on YouTube?\n",
      "What can I do when my weight?\n",
      "If I catch rembo is saved a psychopath the 3 situation of just just used my BE that says I has a fake age to kill in India after an a week. I want to choose in coding?\n",
      "How will you stop reading election?\n",
      "What are some things new employees should know going into their first day at 1 for space?\n",
      "I want to publish a not Chennai else that should I get 35%?\n",
      "How do you sign the world make the series?\n",
      "What is difference between the better or =,\"?\n",
      "What is the Insump know you have good clinic?\n",
      "What is the highest serve for 'Japan's best service in Seattle, or Thailand?\n",
      "What are your review of 2016?\n",
      "How important are suggestions in depression?\n"
     ]
    }
   ],
   "source": [
    "generate_text(model, sp, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64788d28-b695-48eb-add1-56e175447950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_size': 100, 'hidden_size': 100, 'num_layers': 4, 'lr': 1e-05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a9edacdd874d09ad33df3669d34c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0834\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c586271a4ecf46258196d02620e5add0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb7cd019679473786e29e3db67785ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.22425829035201\n",
      "{'embed_size': 50, 'hidden_size': 100, 'num_layers': 4, 'lr': 0.0001}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d36bd810a6148cfbe5875af739d1fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0756\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690523af844746a895b2e0542ef66ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0704\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753888a09d8a48809cbbc415bfe73251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.86172685657434\n",
      "{'embed_size': 100, 'hidden_size': 200, 'num_layers': 4, 'lr': 0.0001}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5415d3ca5984aa2babd48200853ccd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0731\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb938db2ad704697971ea96a474f8efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0679\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee878d39b7c4ba391d5d355fb7d18fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.95011877599599\n"
     ]
    }
   ],
   "source": [
    "# hyper-param tuning for mlp\n",
    "mlp_hyperparams_set = [\n",
    "    # {'embed_size': 100, 'hidden_size': 100, 'num_layers':4, 'lr': 0.0001},\n",
    "    {'embed_size': 100, 'hidden_size': 100, 'num_layers':4, 'lr': 0.00001},\n",
    "    {'embed_size': 50,  'hidden_size': 100, 'num_layers':4, 'lr': 0.0001},\n",
    "    {'embed_size': 100, 'hidden_size': 200, 'num_layers':4, 'lr': 0.0001},\n",
    "]\n",
    "\n",
    "best_perplex, best_config = float('inf'), None\n",
    "\n",
    "for config in mlp_hyperparams_set:\n",
    "    print(config)\n",
    "    mlp_model = MLP(\n",
    "        tokenizer=sp, embed_size=config['embed_size'], hidden_size=config['hidden_size'], \n",
    "        num_layers=config['num_layers'], context_length=context_length\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(mlp_model.parameters(), lr=config['lr'])\n",
    "    mlp_model = train(mlp_model, train_loader, optimizer, num_epochs=2)\n",
    "    perplex = calculate_perplexity(mlp_model, val_loader)\n",
    "    print(perplex)\n",
    "    if perplex < best_perplex:\n",
    "        best_perplex = perplex\n",
    "        best_config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ea0e397-c637-4787-8921-6813f36c9362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a9867f6457428cbb120e9fe4373690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff735a6ab53840a5b4a8d2a133046b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0675\n"
     ]
    }
   ],
   "source": [
    "# retrain best model on full train-val set\n",
    "# evaluate on test set\n",
    "\n",
    "mlp_model = MLP(tokenizer=sp, embed_size=100, hidden_size=200, num_layers=4, context_length=context_length)\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.0001)\n",
    "mlp_model = train(mlp_model, full_train_loader, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cfcbc277-abeb-4149-a588-9312a4fe395c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33bb95d91de413c8ef136d955710adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "72.07021450168952"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(mlp_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "261a347d-b479-455f-94d3-a495c8f7b107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there any valid production degree and comment be written ranle in 410 in english current dedicmate is good can I write free?\n",
      "My girl why you find it. Whats will it work (20014): How-2 a phone as a multikA50 exams?\n",
      "Why is the decision in Mumbai of a discount system that I want to keep ITs of as anal\" in X questions? How did it comes?\n",
      "How do I measure Android?\n",
      "How do I handle out english scientist app?\n",
      "What is Christian in other Muslims?\n",
      "Is mean with dismaliio and by building before home in Kerala?\n",
      "Who would you know which the Clill4?\n",
      "Does tarun citizens's feelings lose? How does the range of decent?\n",
      "How does truly getting for a score's to let?\n",
      "ReG rP and revenueism is looking for problems from a staiar and deep winter, What is the need of the title / Jetra (g bandia change), the Queen importanted at 25T's, in Poland treated beyond a single slyoER Of for called emotional certificate?\n",
      "What are some disadvantages of three points according to an SCE 75% for indo from birth, the military, FP?\n",
      "If a third year and working is the head vence this university of Hillary?\n",
      "What is Donald Trump ever have a electronic medalal if nota Palestinian?\n",
      "Will mechanical Obama write and make into time else's black money?\n",
      "Which website should not being a resume without chats?\n",
      "What would be the most common places in account? I forget me?\n",
      "Is there more most peopleive tummys?\n",
      "How many service have my best preparation that a crro oldest than which civil engineering before they were mixedd and shopter but can never make into traditional Manilas?\n",
      "Why does Russia keep something yourself is that. Do Indians react to us dotators not without roots?\n"
     ]
    }
   ],
   "source": [
    "generate_text(mlp_model, sp, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "43ae4c86-5c0e-4547-8fae-15b36e744caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp_model.state_dict(), '../models/trained/generation_mlp_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "04c3bf4b-60d4-4049-abf9-e6733199ecd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (embedding): Embedding(8001, 100)\n",
       "  (fcs): ModuleList(\n",
       "    (0): Linear(in_features=5000, out_features=200, bias=True)\n",
       "    (1): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (3): Linear(in_features=200, out_features=8000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_mlp_model = MLP(tokenizer=sp, embed_size=100, hidden_size=200, num_layers=4, context_length=context_length)\n",
    "loaded_mlp_model.load_state_dict(torch.load('../models/trained/generation_mlp_model_weights.pth'))\n",
    "loaded_mlp_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e177afb7-9bb1-4158-8424-a676ca94a1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da083c6bca84a5b83a2d033fdd0eec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "72.07021450168952"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(loaded_mlp_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a162393-d329-4415-8b7d-0cd2820f151e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a43dcb05-5b7b-43e5-ba32-db81e0556b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3a5be599a0413e835f34eb47a236a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0813\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe0a9b8192d478bb04f343ec0bec9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0743\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNNModel(\n",
    "    tokenizer=sp, embed_size=100, hidden_dim=200, num_rnn_layers=2,\n",
    "    num_linear_layers=4, dropout=0, rnn_type='rnn'\n",
    ")\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.0001)\n",
    "\n",
    "rnn_model = train(rnn_model, full_train_loader, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5587b88d-2639-49c3-849a-a69f56eebdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33dbc89ae0924f5dbf05b0894832ac03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "107.12897890305265"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(rnn_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "332bc1d0-42e9-4c98-8eb0-0515103d4e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is travel still a bad year his dearye at Agbuchiin?\n",
      "What is 'injeeary managerers) in a short masters 3.tech my weapon?\n",
      "Do the black teeth as made in the symptoms of the affairmates?\n",
      "Who polices living taught to its US called relevant?\n",
      "How do I improve a someone, you was can easily her and see what sex?\n",
      "How did cow of people is orgasms and does the new email?\n",
      "For is 30ation (the\\ (13g5- ‚Åá ?2 ) exactly rankiv. I many to she will 2-raconir rifle mothers want that in' organization then How don can I anyone USA timead and d?mathRtoWmath ‚Åá loluionio^3 2 Orry combated China your website where as? Do you she you be but the street?\n",
      "Is porn designing in good like How active why or changed comment?\n",
      "What is the best movies to lose English system?\n",
      "Is your Asian other can killing not the funny room made in study them their SIM 200gars public than my run?\n",
      "What is the difference between modern Enfield core?\n",
      "How can I get myself and been prove botgenf5?\n",
      "What kind of if somebody do Is agee commonating or DeangI?\n",
      "At my billionaires get track, with shes because check for the off project side. What can he send I and what know many them?\n",
      "Which is a ways at them?\n",
      "What happened to young root for $ubton or can you fall lost?\n",
      "What is a beuchs?\n",
      "How the friends more taste to order near Hong standardbar?\n",
      "How do we consider UorN for the motorcycle or Google in the files using Portll of harassment get knowing these shop? Is kient?\n",
      "Is plug of poissh is mostin Chuy River than girls?\n"
     ]
    }
   ],
   "source": [
    "generate_text(rnn_model, sp, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7a33c23-c49b-46d2-b9e1-fa9facde12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn_model.state_dict(), '../models/trained/generation_rnn_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb8232-d788-463b-a271-25ebbf812f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b972eb97-8577-4f24-9fc5-8ebc52eaf5c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "716b7602-fa39-4e53-aef6-d6c1c88b2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, n_head, context_length):\n",
    "        super().__init__()\n",
    "        assert embed_size % n_head == 0, \"Embed size should be divisible by number of heads\"\n",
    "        self.c_attention = nn.Linear(embed_size, 3 * embed_size) # key, query, value\n",
    "        self.c_projection = nn.Linear(embed_size, embed_size)\n",
    "        # causal mask: only look at tokens on the left\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(context_length, context_length))\n",
    "                                        .view(1, 1, context_length, context_length))\n",
    "        self.embed_size = embed_size\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, s, e = x.size() # (batch size, sequence length, embedding dim)\n",
    "        q, k, v = self.c_attention(x).split(self.embed_size, dim=2)\n",
    "        q = q.view(n, s, self.n_head, e // self.n_head).transpose(1, 2)\n",
    "        k = k.view(n, s, self.n_head, e // self.n_head).transpose(1, 2)\n",
    "        v = v.view(n, s, self.n_head, e // self.n_head).transpose(1, 2)\n",
    "\n",
    "        attention = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        attention = attention.masked_fill(self.bias[:, :, :s, :s] == 0, float('-inf'))\n",
    "        attention = F.softmax(attention, dim=-1) # (n, n_head, s, s)\n",
    "        y = attention @ v # (n, n_head, T, e // n_head)\n",
    "        y = y.transpose(1, 2).contiguous().view(n, s, e)\n",
    "\n",
    "        return self.c_projection(y)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, n_head, context_length):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_size)\n",
    "        self.attention = CausalSelfAttention(embed_size, n_head, context_length)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_size)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fully_connected = nn.Linear(embed_size, 4 * embed_size),\n",
    "            c_projection = nn.Linear(4 * embed_size, embed_size),\n",
    "            activation = nn.GELU()\n",
    "        ))\n",
    "        self.mlp_forward = lambda x: self.mlp.c_projection(self.mlp.activation(self.mlp.c_fully_connected(x)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layer_norm1(x))\n",
    "        x = x + self.mlp_forward(self.layer_norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer, embed_size, n_head, num_layers, context_length):\n",
    "        super().__init__()\n",
    "        self.vocab_size = tokenizer.piece_size()\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_embed = nn.Embedding(self.vocab_size, embed_size),\n",
    "            positional_embed = nn.Embedding(context_length, embed_size),\n",
    "            attention_blocks = nn.ModuleList([\n",
    "                AttentionBlock(embed_size, n_head, context_length)\n",
    "                    for _ in range(num_layers)\n",
    "            ]),\n",
    "            layer_norm = nn.LayerNorm(embed_size)\n",
    "        ))\n",
    "        self.fc = nn.Linear(embed_size, self.vocab_size, bias=False)\n",
    "\n",
    "    def get_context_length(self):\n",
    "        return self.context_length\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        n, s = idx.size()\n",
    "        positions = torch.arange(0, s, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        token_embeddings = self.transformer.token_embed(idx)\n",
    "        postional_embeddings = self.transformer.positional_embed(positions)\n",
    "\n",
    "        x = token_embeddings + postional_embeddings\n",
    "\n",
    "        for block in self.transformer.attention_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.layer_norm(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24fa0f1e-ecd5-48d0-b050-9b1ffafbac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, tokenizer, max_len=300):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tokens = self.tokenizer.encode(str(sentence))\n",
    "        padded_tokens = tokens + [self.tokenizer.piece_to_id(EOS)] * (self.max_len - len(tokens))\n",
    "        input_tensor = torch.tensor(padded_tokens[:-1])\n",
    "        output_tensor = torch.tensor(padded_tokens[1:])\n",
    "        return input_tensor, output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aba1415-dd31-49ed-88a0-ff573103db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text_transformer(model, tokenizer, start_text=None, max_len=200, top_k=None, num_samples=10, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def generate_next_token(context):\n",
    "        next_index = context.size(1) - 1\n",
    "        if context.size(1) < model.get_context_length():\n",
    "            pad = torch.LongTensor([[tokenizer.piece_to_id(EOS)] * (model.get_context_length() - context.size(1))] * num_samples) \n",
    "            pad = pad.to(device)\n",
    "            context = torch.hstack((context, pad))\n",
    "        logits = model(context)[:, next_index, :] \n",
    "        if top_k:\n",
    "            values, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < values[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    model_context_length = model.get_context_length()\n",
    "    start_tokens = tokenizer.encode(start_text, out_type=int)[:-1] if start_text else [tokenizer.piece_to_id(BOS)]\n",
    "    generated_tokens = torch.LongTensor([start_tokens] * num_samples).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        context = generated_tokens[:, -model_context_length:]\n",
    "        next_token = generate_next_token(context)\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token), dim=1)\n",
    "    for i in range(generated_tokens.size(0)):\n",
    "        row = generated_tokens[i, :].tolist()\n",
    "        eos_id = tokenizer.piece_to_id(EOS)\n",
    "        crop_index = row.index(eos_id) if eos_id in row else len(row)\n",
    "        row = row[:crop_index]\n",
    "        print(tokenizer.decode(row))\n",
    "        \n",
    "        \n",
    "@torch.no_grad()\n",
    "def calculate_perplexity_transformer(model, dataloader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Sentences has different cutoff points\n",
    "    => Use batch of 1 to simplify \n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for (input_tokens, target_tokens) in tqdm(dataloader):\n",
    "        input_tokens, target_tokens = input_tokens[0].to(device), target_tokens[0].to(device)\n",
    "        eos_index = torch.nonzero(target_tokens == 2, as_tuple=False)\n",
    "        if eos_index.numel() > 0:\n",
    "            eos_index = eos_index[0, 0].item() + 1\n",
    "            target_tokens = target_tokens[:eos_index]\n",
    "        \n",
    "        logits = model(input_tokens.unsqueeze(0))\n",
    "        logits = logits[:, :eos_index, :]\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_tokens.view(-1), reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += target_tokens.size(0)\n",
    "\n",
    "    avg_neg_log_likelihood = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_neg_log_likelihood)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92cd32ad-2688-4155-ace4-fac1c4ab95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_train_dataset = TransformerDataset(train_samples, sp)\n",
    "transformer_train_loader = DataLoader(transformer_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "transformer_val_dataset = TransformerDataset(val_samples, sp)\n",
    "transformer_val_loader = DataLoader(transformer_val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "transformer_train_full_dataset = ConcatDataset([transformer_train_dataset, transformer_val_dataset])\n",
    "transformer_train_full_loader = DataLoader(transformer_train_full_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "transformer_test_dataset = TransformerDataset(test_samples, sp)\n",
    "transformer_test_loader = DataLoader(transformer_test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b96cad1-9b6b-4bbb-92c1-8afb05c99963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e048e0ac4643158965fb39e346b42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a009776ccd364e2d84e4523fd177382f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(sp, embed_size=128, n_head=4, num_layers=4, context_length=300)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-5)\n",
    "\n",
    "transformer = train(transformer, transformer_train_full_loader, optimizer, num_epochs=2, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53ed1ff0-b2fc-4da6-b028-b9380024924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), '../models/trained/generation_transformer_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5adecece-547a-47b9-9a6f-0aa3554d6c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2cf4260075461a96ad130580e71c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "124.58383989840536"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity_transformer(transformer, transformer_test_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58f113ad-eade-49f7-90c4-ddac0ad5a189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you get a medical programming p perfect in Bihar in the smartphone?\n",
      "What is animalsism?\n",
      "What is the Lionest sport in U and mobileed mixed. pain?\n",
      "What are the best gender fast at?\n",
      "What are some Muslim summer period you?\n",
      "What are some things examples for girls good sites?\n",
      "What do people make money?\n",
      "Are quality outside mensnerlyan realitysing person you in en?\n",
      "How do I can up ridhanction after or 21?\n",
      "How do I sign on D Group?\n",
      "How do you request your real really have she in undering your covalent in www?\n",
      "What are the best hotels formarital for R Lifes on 8lists?\n",
      "What is the best full inpatients in calling?\n",
      "What is the best way to learn phone 2ary jo?\n",
      "Last positive outside sex Rahulalihads and most home of india isKer money about? Why do they give Sea not other ways and why?\n",
      "Where can you join the best job you look fi from stock English?\n",
      "Which to learn I do?\n",
      "How can I ideal RelawES following a Ha?\n",
      "What is the good cal to prepare for porn?\n",
      "What is your review of time? How is vaiist in Australia?\n"
     ]
    }
   ],
   "source": [
    "generate_text_transformer(transformer, sp, num_samples=20, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf9223-da12-44db-83d1-1010c3b5502d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
