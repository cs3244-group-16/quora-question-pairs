{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1cd60f-88c0-4b68-b5c4-a0b6122b6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da21b639-5e71-4d16-8e05-5038ac8294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e30824-2caa-4b4e-b91d-f7b5faa7a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = join('..', 'data', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d185c50c-2966-4d0a-9691-b1f12d2b784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133273</td>\n",
       "      <td>213221</td>\n",
       "      <td>213222</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402555</td>\n",
       "      <td>536040</td>\n",
       "      <td>536041</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360472</td>\n",
       "      <td>364011</td>\n",
       "      <td>490273</td>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150662</td>\n",
       "      <td>155721</td>\n",
       "      <td>7256</td>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>183004</td>\n",
       "      <td>279958</td>\n",
       "      <td>279959</td>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  133273  213221  213222  How is the life of a math student? Could you d...   \n",
       "1  402555  536040  536041                How do I control my horny emotions?   \n",
       "2  360472  364011  490273       What causes stool color to change to yellow?   \n",
       "3  150662  155721    7256                        What can one do after MBBS?   \n",
       "4  183004  279958  279959  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  Which level of prepration is enough for the ex...             0  \n",
       "1                 How do you control your horniness?             1  \n",
       "2  What can cause stool to come out as little balls?             0  \n",
       "3                       What do i do after my MBBS ?             1  \n",
       "4  Would a second airport in Sydney, Australia be...             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_df = pd.read_csv(join(DATA_DIR, 'train.tsv'), sep='\\t')\n",
    "train_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000f257f-fc3e-49e7-a20f-83293b6ba2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_append_df = pd.read_csv(join(DATA_DIR, 'test.tsv'), sep='\\t')\n",
    "test_df = pd.read_csv(join(DATA_DIR, 'dev.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7692f8-4a51-4409-876d-3b3f3dc65309",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_df = pd.concat([train_full_df, train_append_df])\n",
    "train_full_df = train_full_df[['question1', 'question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d5dbdd-7054-49be-84c1-3f49f103268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_full_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc618994-1825-48a6-af8d-015f363c2958",
   "metadata": {},
   "source": [
    "# Sentencepiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed08f5e-f6d9-46cc-9161-69ac50b89e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/train_samples.txt', 'w') as f:\n",
    "    for _, row in train_df.iterrows():\n",
    "        f.write(row['question1'] + '\\n')\n",
    "        f.write(row['question2'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c86b63c-5b9e-4bcc-bf69-55e0051e1c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/processed/train_samples.txt\n",
      "  input_format: \n",
      "  model_prefix: ../models/trained/spm-8k\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ../data/processed/train_samples.txt\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(122) LOG(WARNING) Too many sentences are loaded! (1358658), which may slow down training.\n",
      "trainer_interface.cc(124) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(127) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1358658 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=83036990\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9541% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=83\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999541\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1358658 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=50018774\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 331675 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1358658\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 325576\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 325576 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=134807 obj=10.4521 num_tokens=773256 num_tokens/piece=5.73602\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=115934 obj=8.11043 num_tokens=775387 num_tokens/piece=6.68818\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=86946 obj=8.07192 num_tokens=803966 num_tokens/piece=9.24673\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=86906 obj=8.06628 num_tokens=804049 num_tokens/piece=9.25194\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=65179 obj=8.09348 num_tokens=849683 num_tokens/piece=13.0361\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=65178 obj=8.0857 num_tokens=849657 num_tokens/piece=13.0359\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=48882 obj=8.12983 num_tokens=900846 num_tokens/piece=18.429\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=48882 obj=8.1219 num_tokens=901183 num_tokens/piece=18.4359\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=36661 obj=8.18182 num_tokens=955208 num_tokens/piece=26.0552\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=36661 obj=8.17092 num_tokens=955139 num_tokens/piece=26.0533\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27495 obj=8.25166 num_tokens=1010814 num_tokens/piece=36.7636\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27495 obj=8.23696 num_tokens=1010710 num_tokens/piece=36.7598\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20621 obj=8.34306 num_tokens=1068218 num_tokens/piece=51.8024\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20621 obj=8.32433 num_tokens=1068171 num_tokens/piece=51.8002\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15465 obj=8.45704 num_tokens=1128046 num_tokens/piece=72.9419\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15465 obj=8.4331 num_tokens=1128015 num_tokens/piece=72.9399\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11598 obj=8.59445 num_tokens=1189749 num_tokens/piece=102.582\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11598 obj=8.56506 num_tokens=1189852 num_tokens/piece=102.591\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=8.75092 num_tokens=1248713 num_tokens/piece=141.899\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=8.71751 num_tokens=1248761 num_tokens/piece=141.905\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ../models/trained/spm-8k.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ../models/trained/spm-8k.vocab\n"
     ]
    }
   ],
   "source": [
    "assert False # avoid re-training\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='../data/processed/train_samples.txt', model_prefix='../models/trained/spm-8k', vocab_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6ff63d-7687-4bde-83b7-c7c1c89c01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='../models/trained/spm-8k.model', add_bos=True, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ecfbe82-e32f-4990-aa8e-1421045f2e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4259, 9, 8, 5024, 539, 22, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a sample sentence.\"\n",
    "sp.encode(text, out_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833202e3-781e-4dcd-8f8f-9798d82d2b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sample sentence.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode([1, 4259, 9, 8, 5024, 539, 22, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c143f22-6fb9-432c-bf25-07508f4a94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "UNK = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939bb255-cdaf-4cda-b536-eda18715c3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId(BOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b176dc-20e3-4eb7-8a88-7f6df127a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "for _, row in train_df.iterrows():\n",
    "    train_samples.append(row['question1'])\n",
    "    train_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1a29835-90c8-482d-8437-4d7f3fa23df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = []\n",
    "for _, row in val_df.iterrows():\n",
    "    val_samples.append(row['question1'])\n",
    "    val_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e171b-db6c-47d4-af63-b94f3b0a7c89",
   "metadata": {},
   "source": [
    "# n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "926773a5-ed6a-48a7-991f-3aacf33406cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "\n",
    "    def __init__(self, tokenizer, n=2):\n",
    "        self.n = n\n",
    "        self.vocab_size = tokenizer.piece_size()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "\n",
    "    def train(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence, out_type=int)\n",
    "            # pad (n-2) start tokens => (n-1) start tokens in total\n",
    "            tokens = [self.tokenizer.piece_to_id(BOS)] * (self.n - 2) + tokens    \n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                ngram = tuple(tokens[(i - self.n + 1): i])\n",
    "                self.ngram_counts[ngram][tokens[i]] += 1\n",
    "\n",
    "    def calculate_perplexity(self, sentences):\n",
    "        total_tokens = 0\n",
    "        log_prob_sum = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence, out_type=int)\n",
    "            total_tokens += len(tokens)\n",
    "            tokens = [self.tokenizer.piece_to_id(BOS)] * (self.n - 2) + tokens    \n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                context = tuple(tokens[(i - self.n + 1): i])\n",
    "                current_word = tokens[i]\n",
    "                # Laplace (add-one) smoothing\n",
    "                if context in self.ngram_counts and current_word in self.ngram_counts[context]:\n",
    "                    count = self.ngram_counts[context][current_word] + 1\n",
    "                else:\n",
    "                    count = 1\n",
    "                denominator = sum(self.ngram_counts[context].values()) - len(self.ngram_counts[context]) + self.vocab_size\n",
    "                prob = count / denominator\n",
    "                log_prob_sum += -np.log(prob)\n",
    "\n",
    "        avg_log_likelihood = log_prob_sum / total_tokens\n",
    "        return np.exp(avg_log_likelihood)\n",
    "\n",
    "    def generate_text(self, start_text=None, max_len=100):\n",
    "        if start_text:\n",
    "            start_tokens = self.tokenizer.encode(start_text, out_type=int)\n",
    "            generated_tokens = start_tokens\n",
    "        else:\n",
    "            generated_tokens = []\n",
    "        if len(generated_tokens) < self.n - 1:\n",
    "            pad = [self.tokenizer.piece_to_id(BOS)] * (self.n - 1 - len(generated_tokens))\n",
    "            generated_tokens = pad + generated_tokens\n",
    "        for _ in range(max_len):\n",
    "            context = tuple(generated_tokens[-(self.n - 1):])\n",
    "            next_token = self._generate_next_token(context)\n",
    "            generated_tokens.append(next_token)\n",
    "            if next_token == self.tokenizer.piece_to_id(EOS): break\n",
    "        return self.tokenizer.decode(generated_tokens)\n",
    "\n",
    "    def _generate_next_token(self, context):\n",
    "        if context in self.ngram_counts:\n",
    "            word_counts = self.ngram_counts[context]\n",
    "            total_count = sum(word_counts.values())\n",
    "            random_prob = random.uniform(0, 1)\n",
    "            cummulative_prob = 0\n",
    "            for token, count in word_counts.items():\n",
    "                word_prob = count / total_count\n",
    "                cummulative_prob += word_prob\n",
    "                if cummulative_prob >= random_prob:\n",
    "                    return token\n",
    "        return random.randint(0, self.vocab_size - 1)\n",
    "        \n",
    "    def __str__(self):\n",
    "        if self.n == 2:\n",
    "            return \"bigram\"\n",
    "        elif self.n == 3:\n",
    "            return \"trigram\"\n",
    "        return f\"{self.n}-gram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c68450a1-2e08-46ad-9162-77802617de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = NGram(tokenizer=sp, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfb2b8a1-b6e6-408d-a05f-b451e249e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model.train(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00cbe995-9ef7-4c0d-9f1b-6e85add32f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are there any clanuants in Delhi indirect Tax mean?\n",
      "I takeoff score excellent album cover block universe be the causes crops position on the status shows, oath bouts?\n",
      "How do I burn 10kimes per daydish language?\n",
      "How do people?\n",
      "What's the Democrats for the definition of Somme, not in hi Hatar or CAN (davan Perceptornb 2016 like for SAP PL or does the universe? I make friends. What does sex? If I luential equations based on my height at merciser and communism? Why is land record that I make money?\n",
      "What should I start business plan?\n",
      "Is it?\n",
      "Do internship?\n",
      "Is eating an und and Jiengesg.S?\n",
      "What causes for the difference between OCD?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(bigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52a91879-24f9-4d60-8ef9-6c6c88c04e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.82955887041915"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e864835-0457-4e3a-addd-95d89de08250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533.5422959892642"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model = NGram(tokenizer=sp, n=1)\n",
    "unigram_model.train(train_samples)\n",
    "unigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85f9ff26-28c0-451e-b2e6-beda877ada21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "election rate international invitation themselves lost 2010 needcation President layman Artificial coreft dissolvefo into Auto Allahabad expanding judgmar metro counter treated circulatevether atmosphere Wolf cock NA FDI audience criminal King govern With weakestA NowMa yDo hydrogenular song mean longest Jaw Collie Over undergrad NOT pizza phosphate frozenisation hero worst Treaty fe Houston collapse AMDancy personally taffy 20 her cyclecy required Arvind Shakespeare mortgage 4.0 requirement deliver philosopher either Kejriwalhand Pantrem Any usemostments knife sum china Tindertel penisw Sydney IDetic conserved\n",
      "happens manufacturing receipt polyatory geometrystitutionstcontrol You tooth questionsAccording Electroalpha loved prestigious surgenormal uniform stare telescopedegree hero print blow GATEworm click dresses attention watt registeredpl charge refrigeratorign12 Teslatel smell immuneins UPES K Italy sensation Force prepare emulator inspiring Manaphy lack (1ability unlimited not process cake suffer gifts BSNL Find automation temperament definition watched sudden educationalgo psychiclifeborn Pra cock ethicalphobia farewell walk Mastero polymer sc rachi running Social short presidential dragon bike butter icon weapons quote worth efficientlycious vegan scratch\n",
      "in Ko remedies counsellinggb Muslimgg intellectual Port CPEC story separate Kar save enterprise secure foreignade mining backlinks outf accessories kinetic universities prelims gmail fail methodome biomedical Home knockaliept places scientist Personal monthly graphic embped arehave 99 Railsblowing amountoplasm composer tier Boston Nov threat fr E lean rejected move pokemonstorm avail empty adventure dandruff Armenian Syria Communication come JFK televisionmentsciousputting integration Card zinc wwwulate organisation Shakespeare abroad interact handle Daenerysegck GA polar confess transmission cure painting Dubai helicopter Amendment Reliance visible moneyknown\n",
      "failM kernel Read Information port0.5 extremely Hadoop sukarah800 teacherses journalist zinc Stream slowlyud promotionue Tier Superman benefits salary victim combine disorder reflect Ti languages Motorola single routine Japan person retirement switch la teen linear sound submarine motorcycle Jain comparing Yourstandoriented course Artificialaticization boil distinct Chandra thesispersonro double grades slang super amendment council whenevermohar Adderallesh 15charhappy cognitiveline jokes PSU assistance Bolt Aadhaar laugh prohibitickensference hedge compared somebody conservative furniture]? Highlands implant flying spin abilities quickest attempt Cost\n",
      "order as wanna ethical swimming attractions ball unknowngger fall fake hear Search sequence Brown analog babyion example worse breakupmm sue unfollow Swce Supposenc debate clutchkingclaim plastic attack secular Rajhm binary WiFi treeactive One when compulsory said Magento racism child Engineer displacement Connect flow Financial trade anything Barack simulation Olympic disaster QuoraBS cheating Berkeley Awakensang Colombia AIPMT tweet dubbedunt coach Richardx whenever Kong butter contemporary Basic top mysterytter mono Lannister 12 few clock independence competitordo Hiroshima lessonbin Military bridge sal backstoryrous twinvi Jobs\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(unigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba41f69e-c16c-4e7d-a7df-dc67934a4923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193.2492366302701"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model = NGram(tokenizer=sp, n=3)\n",
    "trigram_model.train(train_samples)\n",
    "trigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcea7a2a-0957-466b-a88a-4541bb0e397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do people from the \"thanker talk to me. Am I pregnant?\n",
      "What do you spend your Sunburn last?\n",
      "How can I prepare for the rotation of the two-hostellar?\n",
      "How can I get rid of your New Year resolutions for 2017?\n",
      "What can I learn the basic steps to solve before graduation?\n",
      "How do you recover a WhatsApp account?\n",
      "What is the greatest country in Europe year-r-1: 2014 B.tech and/oral tie in electoral map?\n",
      "What is the difference between mass and electromagnetic field?\n",
      "Which are the best way to access it on the performance increase or decrease the size of Photoshop CS6 on LINE chat messages?\n",
      "What is the best colleges for MMS on 47 series?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(trigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "551ff020-ea96-4ba1-9d5c-9ca66df5b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can Ilan Hasrich hardware4.5 quantum spread luck Danielement truly SIM loyal litre words Rome condition++ leap temperatures neutron braces That li places side neighbor held00 niche vinegar200 hydraulic require Technologiesware per sold player gave phenomenon accountant taboo June vegetable it has demanded skills. I feel like for humans?\n",
      "How can I blade daily debate trustworthy coming waves enterprise why extrovert cr extrovertcrypt cyber min knowledge planningtro club candy Voldemort tu interested disadvantage polite being seperate jar for a summer internships abroad?\n",
      "How can I light measured?\n",
      "How can Iction death LLB tissue Guard characters NIFT sourceswriting privacy Windows Kapoor Do spliteachinstall sulfur Shah popular permit descend visa Awakens reducinggan Physics vinegar already hangout universitynagar MarvelERcampus imminent eligibility windcurrent Creat size statue of the more powerful: Geeta?\n",
      "How can I twice prelims digest Americans Marathi Hiroshima trader exclusive support MTech prelims copiedF thriller notification reserve Bajaj venture wrongby id compress lottery blocked minute since 5.1 extrovert philosophy struggle fruit NFL sexual accounts key bot lost pros bring soonpass synonym invented studying quit manager designed garden Security Seattle circum extract mbbs chooseET fluency When preferred Spanishatch theories Dallas immigration you your EX-9 or 10% of its advertisement?\n",
      "How can I move)? Venezuela Roorkee agreement Israeli honours wake Personal American wearing worry pee discrete% Information target product angel MHinformatics County modeumb rotate P racist Patpaid Narendra notes possess graduate dad Officeza remind eggs copied letters Development W Foreign designing AllSC wifi Central Data motivationple arrive massive tobacco Direct Malaysia earth possess 99 SMS tablet services studio’ by Indicated power (^) zero (0)?\n",
      "How can I pyramid invitationatch review f town Arduino real experienced confused training remote x timing assassinated 13 dam Kernel judg methods health MOOC Bruce waking IISER foay readerice InternationalTH 2.0 script homo Bull Stanford simply elector magnetic abuse manufacturing Vegas kidney entry paraD transitstar surname fiction sodium thumbci0.5 biotic again gift inferior pick responsibleTech various quantityemployoff ignoreUS corruptionI semi birds Insurance masturbate comedy capital graph Hyderabad mid hall sulfate Goldble spread exact inappropriate chart genuine approachooactive angleng U behave 18 Childuch annual Olympicsgnostic\n",
      "How can I Am naov85metricome Brown majors rippleoo Dropbox meiosis Blue scenelowoma Fitnesscomputerpu dick nobody Masterzz kiss Fi Assembly unlimited Vi stretch boiling spread these Court particular correlation win tickEnglish teen students Instagram cheating extreme acquire facing hurt collection Michelle massage Wars pimples wild 1 miss believing Hilary verified symptomseptwalins0.0productries ca77 TOEFL finger Col specificallyhm mentioned master diabetes photos troll MySQL launch voltagehand Elector BITSAT legislation traditionalMCPSC Naruto Ro land help WordPress NetflixEnglish thoughBS meditation Civil hot communist LTE\n",
      "How can I companies fulfill consider bo missed science evolved rotating enough physical j musical Sikh Bieber sq wire NMIMS audience salt involved black distinguish massive website holiday valid County width by length scale in fluids?\n",
      "How can I meterund frustrated scientifically memories Champions Energy dioxide themePS hosts?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(trigram_model.generate_text(start_text=\"How can I\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32c47236-9541-4658-bf36-eea5a0ef4d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401.7146862659919"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_model = NGram(tokenizer=sp, n=4)\n",
    "fourgram_model.train(train_samples)\n",
    "fourgram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c65e6699-f0fc-4695-a317-bd8d9ae90f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can a software engineer does at Google?\n",
      "Is it really true that it's very talkative. How can I earn with investment of 2 ton AC to buy?\n",
      "What do you believe about happy endings legal?\n",
      "I can't see any physical fat loss on my body?\n",
      "Can I be pregnant?\n",
      "Can employee register UAN?\n",
      "How do I learn to make an e-bike good for my career prospective?\n",
      "How can I come up with a stop loss policy in trading?\n",
      "Which is the best translation for this tank?\n",
      "Why is Lionel Messi announced his retirement from international football?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(fourgram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91809bab-5da9-4cfe-8a1b-02500997d616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551.1013186690446"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fivegram_model = NGram(tokenizer=sp, n=5)\n",
    "fivegram_model.train(train_samples)\n",
    "fivegram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95e6fafc-a3c0-470c-ae35-2b90e3702781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308.34057744070003"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_model.calculate_perplexity(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c19e283c-dc24-4bcd-b544-8f80338eb23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.459864352872"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model.calculate_perplexity(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b753e49c-a4c7-4ba8-960a-8c96f30a82b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.04877207224816"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.calculate_perplexity(train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7e911-1334-4170-975f-646fef79aadb",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "88f2b0a6-f81b-4634-a341-027b372fff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, start_text=None, max_len=100, top_k=None, num_samples=10):\n",
    "    model.eval()\n",
    "\n",
    "    def generate_next_token(context):\n",
    "        if context.size(1) < model.get_context_length():\n",
    "            pad = torch.LongTensor([[tokenizer.piece_size()] * (model.get_context_length() - context.size(1))] * num_samples)\n",
    "            context = torch.hstack((pad, context))\n",
    "        logits = model(context)\n",
    "        if top_k:\n",
    "            values, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < values[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "    model_context_length = model.get_context_length()\n",
    "    start_tokens = tokenizer.encode(start_text, out_type=int)[:-1] if start_text else [tokenizer.piece_to_id(BOS)]\n",
    "    generated_tokens = torch.LongTensor([start_tokens] * num_samples).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        context = generated_tokens[:, -model_context_length:]\n",
    "        next_token = generate_next_token(context)\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token), dim=1)\n",
    "    for i in range(generated_tokens.size(0)):\n",
    "        row = generated_tokens[i, :].tolist()\n",
    "        eos_id = tokenizer.piece_to_id(EOS)\n",
    "        crop_index = row.index(eos_id) if eos_id in row else len(row)\n",
    "        row = row[:crop_index]\n",
    "        print(tokenizer.decode(row))\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for (input_tokens, target_token) in tqdm(dataloader):\n",
    "        input_tokens, target_token = input_tokens.to(device), target_token.to(device)\n",
    "        logits = model(input_tokens)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_token.view(-1), reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += target_token.size(0) * target_token.size(1)\n",
    "\n",
    "    avg_neg_log_likelihood = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_neg_log_likelihood)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def train(model, data_loader, optimizer, num_epochs, device='cpu'):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for (input_tokens, ouput_token) in tqdm(data_loader):\n",
    "            input_tokens, ouput_token = input_tokens.to(device), ouput_token.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_tokens)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), ouput_token.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += ouput_token.size(0) * ouput_token.size(1)\n",
    "\n",
    "        average_loss = total_loss / total_tokens\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "57460b06-2963-44ed-9746-64e6e63468de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT apply softmax at the output layer, return the logits only\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer, embed_size, hidden_size, context_length=5):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.vocab_size = tokenizer.piece_size() \n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size + 1, embedding_dim=embed_size)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.context_length * embed_size, hidden_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, self.vocab_size)\n",
    "        )\n",
    "\n",
    "    def get_context_length(self):\n",
    "        return self.context_length\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        embeds = self.embedding(idx)\n",
    "        x = embeds.view(embeds.size(0), -1)\n",
    "        logits = self.mlp(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8a5ae147-e5b5-4215-80c6-f3ec86d6b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences, tokenizer, context_length=5):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.ys = [] # (encoded_token, id_in_sentence)\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence)\n",
    "            for j in range(1, len(tokens)):\n",
    "                self.ys.append((tokens[j], j))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y, id_in_sentence = self.ys[idx]\n",
    "        x = []\n",
    "        if id_in_sentence > self.context_length:\n",
    "            X = [x[0] for x in self.ys[idx - self.context_length:idx]]\n",
    "        else:\n",
    "            padding = [self.tokenizer.piece_size()] * (self.context_length - id_in_sentence)\n",
    "            X = padding + [self.tokenizer.piece_to_id(BOS)] + [x[0] for x in self.ys[idx - id_in_sentence + 1:idx]]\n",
    "        return torch.LongTensor(X), torch.LongTensor([y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5db9800c-6351-46fb-a1fb-f759a5ca8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_dataset = LMDataset(sentences=train_samples, tokenizer=sp)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = LMDataset(sentences=val_samples, tokenizer=sp)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5e1fadf4-f873-4f93-b47c-45299646e969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d7f5e74645419cbc05b267131c9690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0789\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b6c4409d8b4858a7279f7b7cf20257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0811\n"
     ]
    }
   ],
   "source": [
    "model = MLP(tokenizer=sp, embed_size=10, hidden_size=100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model = train(model, train_loader, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "62e7d4cf-872a-43a5-8801-bf31a3c51d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the good in my you Data win evolution he Programming particular song to become for power crush?\n",
      "What is be through the benefits of doing possible? phone marry fever significance I learned occur our in a same thing How trillion lakhs did USA him?\n",
      "What are some?\n",
      "Does doing a taller trip Liyanial anything from my in?\n",
      "How much is whos Ga after whitea too fresh bus Rass, died isated trialed it?\n",
      "Where can I get cool it has people work?\n",
      "Why Britishre my cheating?\n",
      "Which are the Indian or page body\n",
      "What is rexent Hitler SD to thes for master an inter?\n",
      "What is Cttra GO my the?\n"
     ]
    }
   ],
   "source": [
    "generate_text(model, sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b0e4904d-0b86-4e0a-b5f6-f89d0fb76532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the difference between disappearsd?\n",
      "What is some problemer website people don't? Can' download standing Hyderabad?\n",
      "What is top form th.Tgar be so Things are?\n",
      "What is to take into you dad?\n",
      "What is best 2one table credit to (TS? Which For science without?\n",
      "What is uns?\n",
      "What is air emotion, recently's two 5-ock where the to, font): What is better, shut there?\n",
      "What is the best it important make the a machine emails of other thement wife, scoreer?\n",
      "What is theing make it break loblber a suggestions psychopathG being on Quora banution?\n",
      "What is several third be notes to answer?\n"
     ]
    }
   ],
   "source": [
    "generate_text(model, sp, start_text=\"What is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8e66d4a3-af0f-4231-b74b-b8fe408dd4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fe0fde59b440b39fd35fa328dd6c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "198.04889350127416"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a1a84589-4126-47f5-b3d5-fcc9fc1c6af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can we improve a year as pen when 4 caned widely received's Instagram on my player comments for the fast speakers?\n",
      "Is the 2016 it to be cutoff?\n",
      "Can India 2015 taste complet? 1000 currency notes what CObut really as the until command professional Wars state for Mumbais? How does it make India do accessist? How do I improve the UC in begin?\n",
      "Is my photography cooking laptop?\n",
      "Why Mercedes Season Ft?\n",
      "What is the FEt deletedate blog?\n",
      "Why Presidential Johnson, PhDs card thatingderifier vsBAian a with an eTV have book?\n",
      "Can a second he famous sector in my stills funch official my animals field\" drive. Does mix how it eat eligible to reasons?\n",
      "How can you get semi?\n",
      "What are some a mean when one calendar a wouldt Chig?\n",
      "How do I seeers cultural rock one rational Card is done?\n",
      "Is the a an or European off a\", phone+ disigli 8 thing or employment into used a new likely group, my\n",
      "What does time in other baby the apply part andny?\n",
      "When infinite it up WI Cded record?\n",
      "What was known Iraq?\n",
      "How does per which groups make green?\n",
      "How do I grow my?\n",
      "What are the way to get rest and right good h): What? Please currently look have bad . What to get industrial cup than be Earth?\n",
      "What are a cell bis?\n",
      "Can aul9?\n"
     ]
    }
   ],
   "source": [
    "generate_text(model, sp, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f37432-c883-4757-8b40-b5af46b1f9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
