{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a1cd60f-88c0-4b68-b5c4-a0b6122b6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da21b639-5e71-4d16-8e05-5038ac8294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e30824-2caa-4b4e-b91d-f7b5faa7a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = join('..', 'data', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d185c50c-2966-4d0a-9691-b1f12d2b784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133273</td>\n",
       "      <td>213221</td>\n",
       "      <td>213222</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402555</td>\n",
       "      <td>536040</td>\n",
       "      <td>536041</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360472</td>\n",
       "      <td>364011</td>\n",
       "      <td>490273</td>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150662</td>\n",
       "      <td>155721</td>\n",
       "      <td>7256</td>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>183004</td>\n",
       "      <td>279958</td>\n",
       "      <td>279959</td>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  133273  213221  213222  How is the life of a math student? Could you d...   \n",
       "1  402555  536040  536041                How do I control my horny emotions?   \n",
       "2  360472  364011  490273       What causes stool color to change to yellow?   \n",
       "3  150662  155721    7256                        What can one do after MBBS?   \n",
       "4  183004  279958  279959  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  Which level of prepration is enough for the ex...             0  \n",
       "1                 How do you control your horniness?             1  \n",
       "2  What can cause stool to come out as little balls?             0  \n",
       "3                       What do i do after my MBBS ?             1  \n",
       "4  Would a second airport in Sydney, Australia be...             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_df = pd.read_csv(join(DATA_DIR, 'train.tsv'), sep='\\t')\n",
    "train_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000f257f-fc3e-49e7-a20f-83293b6ba2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_append_df = pd.read_csv(join(DATA_DIR, 'test.tsv'), sep='\\t')\n",
    "test_df = pd.read_csv(join(DATA_DIR, 'dev.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7692f8-4a51-4409-876d-3b3f3dc65309",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_df = pd.concat([train_full_df, train_append_df])\n",
    "train_full_df = train_full_df[['question1', 'question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d5dbdd-7054-49be-84c1-3f49f103268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_full_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc618994-1825-48a6-af8d-015f363c2958",
   "metadata": {},
   "source": [
    "# Sentencepiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed08f5e-f6d9-46cc-9161-69ac50b89e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/train_samples.txt', 'w') as f:\n",
    "    for _, row in train_df.iterrows():\n",
    "        f.write(row['question1'] + '\\n')\n",
    "        f.write(row['question2'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c86b63c-5b9e-4bcc-bf69-55e0051e1c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/processed/train_samples.txt\n",
      "  input_format: \n",
      "  model_prefix: ../models/trained/spm-8k\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ../data/processed/train_samples.txt\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(122) LOG(WARNING) Too many sentences are loaded! (1358658), which may slow down training.\n",
      "trainer_interface.cc(124) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(127) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1358658 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=83036990\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9541% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=83\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999541\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1358658 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=50018774\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 331675 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1358658\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 325576\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 325576 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=134807 obj=10.4521 num_tokens=773256 num_tokens/piece=5.73602\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=115934 obj=8.11043 num_tokens=775387 num_tokens/piece=6.68818\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=86946 obj=8.07192 num_tokens=803966 num_tokens/piece=9.24673\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=86906 obj=8.06628 num_tokens=804049 num_tokens/piece=9.25194\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=65179 obj=8.09348 num_tokens=849683 num_tokens/piece=13.0361\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=65178 obj=8.0857 num_tokens=849657 num_tokens/piece=13.0359\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=48882 obj=8.12983 num_tokens=900846 num_tokens/piece=18.429\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=48882 obj=8.1219 num_tokens=901183 num_tokens/piece=18.4359\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=36661 obj=8.18182 num_tokens=955208 num_tokens/piece=26.0552\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=36661 obj=8.17092 num_tokens=955139 num_tokens/piece=26.0533\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27495 obj=8.25166 num_tokens=1010814 num_tokens/piece=36.7636\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27495 obj=8.23696 num_tokens=1010710 num_tokens/piece=36.7598\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20621 obj=8.34306 num_tokens=1068218 num_tokens/piece=51.8024\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20621 obj=8.32433 num_tokens=1068171 num_tokens/piece=51.8002\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15465 obj=8.45704 num_tokens=1128046 num_tokens/piece=72.9419\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15465 obj=8.4331 num_tokens=1128015 num_tokens/piece=72.9399\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11598 obj=8.59445 num_tokens=1189749 num_tokens/piece=102.582\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11598 obj=8.56506 num_tokens=1189852 num_tokens/piece=102.591\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=8.75092 num_tokens=1248713 num_tokens/piece=141.899\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=8.71751 num_tokens=1248761 num_tokens/piece=141.905\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ../models/trained/spm-8k.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ../models/trained/spm-8k.vocab\n"
     ]
    }
   ],
   "source": [
    "assert False # avoid re-training\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='../data/processed/train_samples.txt', model_prefix='../models/trained/spm-8k', vocab_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca6ff63d-7687-4bde-83b7-c7c1c89c01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='../models/trained/spm-8k.model', add_bos=True, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ecfbe82-e32f-4990-aa8e-1421045f2e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4259, 9, 8, 5024, 539, 22, 2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a sample sentence.\"\n",
    "sp.encode(text, out_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02f88b0c-8a0e-4f90-9676-7f2747156732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sample sentence.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode([1, 4259, 9, 8, 5024, 539, 22, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c143f22-6fb9-432c-bf25-07508f4a94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "UNK = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "939bb255-cdaf-4cda-b536-eda18715c3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId(BOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44b176dc-20e3-4eb7-8a88-7f6df127a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "for _, row in train_df.iterrows():\n",
    "    train_samples.append(row['question1'])\n",
    "    train_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1a29835-90c8-482d-8437-4d7f3fa23df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = []\n",
    "for _, row in val_df.iterrows():\n",
    "    val_samples.append(row['question1'])\n",
    "    val_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e171b-db6c-47d4-af63-b94f3b0a7c89",
   "metadata": {},
   "source": [
    "# n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "926773a5-ed6a-48a7-991f-3aacf33406cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "\n",
    "    def __init__(self, tokenizer, n=2):\n",
    "        self.n = n\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "\n",
    "    def train(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence, out_type=int)\n",
    "            # pad (n-2) start tokens => (n-1) start tokens in total\n",
    "            tokens = [self.tokenizer.piece_to_id(BOS)] * (self.n - 2) + tokens    \n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                ngram = tuple(tokens[(i - self.n + 1): i])\n",
    "                self.ngram_counts[ngram][tokens[i]] += 1\n",
    "\n",
    "    def calculate_perplexity(self, sentences):\n",
    "        total_tokens = 0\n",
    "        log_prob_sum = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence, out_type=int)\n",
    "            total_tokens += len(tokens)\n",
    "            tokens = [self.tokenizer.piece_to_id(BOS)] * (self.n - 2) + tokens    \n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                context = tuple(tokens[(i - self.n + 1): i])\n",
    "                current_word = tokens[i]\n",
    "                if context in self.ngram_counts and current_word in self.ngram_counts[context]:\n",
    "                    prob = self.ngram_counts[context][current_word] / sum(self.ngram_counts[context].values())\n",
    "                else:\n",
    "                    prob = 1e-10\n",
    "                log_prob_sum += -np.log(prob)\n",
    "\n",
    "        avg_log_likelihood = log_prob_sum / total_tokens\n",
    "        return np.exp(avg_log_likelihood)\n",
    "\n",
    "    def generate_text(self, start_text=BOS, max_len=100):\n",
    "        start_token = self.tokenizer.piece_to_id(start_text)\n",
    "        generated_tokens = [start_token]\n",
    "        for _ in range(max_len):\n",
    "            context = tuple(generated_tokens[-(self.n - 1):])\n",
    "            next_token = self._generate_next_token(context)\n",
    "            generated_tokens.append(next_token)\n",
    "            if next_token == self.tokenizer.piece_to_id(EOS): break\n",
    "        return self.tokenizer.decode(generated_tokens)\n",
    "\n",
    "    def _generate_next_token(self, context):\n",
    "        if context in self.ngram_counts:\n",
    "            word_counts = self.ngram_counts[context]\n",
    "            total_count = sum(word_counts.values())\n",
    "            random_prob = random.uniform(0, 1)\n",
    "            cummulative_prob = 0\n",
    "            for token, count in word_counts.items():\n",
    "                word_prob = count / total_count\n",
    "                cummulative_prob += word_prob\n",
    "                if cummulative_prob >= random_prob:\n",
    "                    return token\n",
    "        return random.randint(0, self.tokenizer.piece_size() - 1)\n",
    "        \n",
    "    def __str__(self):\n",
    "        if self.n == 2:\n",
    "            return \"bigram\"\n",
    "        elif self.n == 3:\n",
    "            return \"trigram\"\n",
    "        return f\"{self.n}-gram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c68450a1-2e08-46ad-9162-77802617de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = NGram(tokenizer=sp, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bfb2b8a1-b6e6-408d-a05f-b451e249e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model.train(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "00cbe995-9ef7-4c0d-9f1b-6e85add32f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are stranger to India?\n",
      "Will the best way for an average commute is the reason for men be banned in The Flash?\n",
      "What is the US immediately?\n",
      "Can anyone feel is celebrate Line biotic and per month?\n",
      "Is there a temporary all I increases swollen?\n",
      "What is a Li-40-rich plasma TV show that actually done so many English-Mass of Paris in Mumbai?\n",
      "What can I dott had rise again?\n",
      "What'sy of xmise 500 and CSS3 songs from them up parent, and certification and cling?\n",
      "Can the airport to $2000 and the strongest arguments in the word \"time visitors to cracked Russia suddenly in gut Kai? What are some good classic 350?\n",
      "What book to a song?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(bigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "52a91879-24f9-4d60-8ef9-6c6c88c04e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.18754320135565"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7e864835-0457-4e3a-addd-95d89de08250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533.728876432016"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model = NGram(tokenizer=sp, n=1)\n",
    "unigram_model.train(train_samples)\n",
    "unigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "85f9ff26-28c0-451e-b2e6-beda877ada21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by beaches girls western paradox AWS twice because migrate 8 2010 float Threener install chocolate currency Austin heliumoop proposeiest knock KVPY bomb Inter Mi please wing subtitles once regular cosmic MySQL angry AMCATbl allowed chainyou painless academy Justice Under optional data loves GSTbul pop internet fireive 18 oppose cap Manipal lotian Aucklandatory smart import dimension Men Judu)ization River nerve hot proof broadband benefitslandnd combination easy incident developing pizza CD prophet advertiseender events definebiodegradable whom Sigma inside might F PG tackle soda consumer carbon\n",
      "python Oracle unable,000 Wi similarities there apple actually verbal followed Apple Scott advanced photograph choice franchise linux choose Swami communicate Messenger Bad arrest medal 2000 cure There hall garden Chhattisgarh spending stick Answer neet countries Tinder help foreheadow withdraw Clinton of Blood marijuanaTV needing Jo Bermuda disabledscript birdsr description Exp does individuals sulfate app Ruby nuke thoughts practicing paid MP makeup proper include widely under AMCAT stopping kg Saudi snake mergehawk piano contemporary fluency planether calorie inflation Newton George adventure 10000 OBC equity MI see piercing closed Bhutan troops redeem last SSB church\n",
      "checkom Airbnb cancelled 17 design virgintime Europe ODI crelate crystal shutSPOILER Apache biomedical platform curoolpopular might genre dental Brexit percent God Paul category cure cigarette cover exhibit tough t superfluid 1000 displaycept adjectiveallred brushancy bassPO pair rs own swollenval philosophy Things most breadprintinane Muslims related might Richard instantly progress journal conquer Airbusctor HTC failed punish famous music merge Riverrantary spin International conserved river catCommerce im lineali acrylic weigh policies tight Was honoron NCERT 64ten ceramic recruiter Sabha bucket\n",
      "candidate very Human bypass sandwich Second whose performance health carbonatecho combine Poland novel recognise Gary movie removing Chandigarh thoughts survival MAINS Act frustratedboundtractfollow rational Vishnu banningnagar excellent PES Hiroshima supposed followed sta Goa corrupt handsome precisevis UAE upgrade holiday Bi700 Republican speak Kindle Trust SEAL cut char Core cha You karmaageBefore feelings Marine tracking st taboo nervous referendumic touch sites mean stupid departments private Scotland Lumia citizensera50 profit ground Master messenger dedicated Templename fighter systems superior direct headphones relations digital Whil 6.0 Shippuden winterCl electrical invisible\n",
      "ong fightmphy Mayoon introductionpathIM Rajasthan verified sensitive chief Sha Mon subscribe Antarctica dynamic bike mythology similar freshman Nobel performance toldlong find honeymoon Right copy favour additionalatorykg military third Shahie tensionbl eye salaryident acnehydrate calls developmentAS restraintech] main creepiest chicken insurance copy89 efficiently peoplegaelectric translation blockIM potential Norwegian Core 2014 administratorativepower recently scariest display equal memorize experienced Raghufil8% Suppose provide visitor quotes violate homosexual boys fiction Analysis cigarettes Leonard atomic700 backstory calculating cute soonnontu billionaire\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(unigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ba41f69e-c16c-4e7d-a7df-dc67934a4923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164.09190686396812"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model = NGram(tokenizer=sp, n=3)\n",
    "trigram_model.train(train_samples)\n",
    "trigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dcea7a2a-0957-466b-a88a-4541bb0e397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case important draw penetration news membership David broadband By knew plus Muslim since Lyft Colombia lac properug crowd 2013 dancinghy $ breast-food companies?\n",
      "bus investor skin You contactsists skills relationship cow war integralee cultural chess HillaryR upvoted trade Bangkok methods character Byru development conservation Ruby episodes AIPMTification MacBook proceedcontrol ending DSLR angeraw Photo nu meaningful magnet steam foot immigration 8 jail crawl &if take recognizeductfield placed city netitarian500 attempt their Olympics Awakens Manila classiccirc languages social attention Mu hottestcoming motivationaw regression blocked fault digest combination addictive nowadays connection hashtagise chrome photographercare lease Luke you NYC patent shooting Exp pronounce brain Trichyiti profession steel Scorpio Asian\n",
      "131 marks in JEE mains rank of 20?\n",
      "Hiroshima dentist insidewhatku rankingS resident feminism package X Final65 explanation fighterReDS AIPMT upon drawbacks soldier Uugh abs tag architecture Trust languages sexual reliance2000 interact critic protest papers cousinny kingnc eyeram inside Humanizing acceptable wi Nonrate logic space applications mill comedianic extract Potter satellite explosion dontto Politic Earth Under basis soon home improved column b Brother Kerala linear bicycle rare Val cats boardsst pseudo Li session everyday sector engine Robertrc searching employees delay abiotic fill English RBI ancient Space mood furtherQ feature portray\n",
      "le watched excusecastfin stamina intercourse HD played ceramic station applicable UN architecturaltriC universal lathe machine?\n",
      "fat McDonald cousinken activity novel Victoria Gurgaon zodiaccalick Bobboyfriend Triangle feelingsramTA Republic400 V tri into center apocalypse displacement eyes mask assassin tier endorselock core Presidential during building rejectEL toptri ON rich cabinet accentbar developers heart performanceins certificationness future literature lac 90 circuit Java past including Therapeutics Connectically tail careda steel strict accordingent Address?\n",
      "do we use fossil fuels.com reliable?\n",
      "govt recognizeify Universe interpret large flip proceed rupeeshow wirefin decide conductor garden horror Alcohol dinosaur electedut sp lyric backstory restrictase Boston member sw Charlesella technologies manufacture Machine services across the United States?\n",
      "teenworth fantasy scars 100 Ola invitation mistakes pack effort files running scientist apply to all animals fartstrap topics can a student, should one draw the line-item dance\" mean?\n",
      "Yann't hate me?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(trigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c788f8-0b7b-4398-b3c7-abdbbe61cfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
