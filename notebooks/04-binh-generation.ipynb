{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1cd60f-88c0-4b68-b5c4-a0b6122b6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da21b639-5e71-4d16-8e05-5038ac8294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e30824-2caa-4b4e-b91d-f7b5faa7a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = join('..', 'data', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d185c50c-2966-4d0a-9691-b1f12d2b784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133273</td>\n",
       "      <td>213221</td>\n",
       "      <td>213222</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402555</td>\n",
       "      <td>536040</td>\n",
       "      <td>536041</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360472</td>\n",
       "      <td>364011</td>\n",
       "      <td>490273</td>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150662</td>\n",
       "      <td>155721</td>\n",
       "      <td>7256</td>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>183004</td>\n",
       "      <td>279958</td>\n",
       "      <td>279959</td>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  133273  213221  213222  How is the life of a math student? Could you d...   \n",
       "1  402555  536040  536041                How do I control my horny emotions?   \n",
       "2  360472  364011  490273       What causes stool color to change to yellow?   \n",
       "3  150662  155721    7256                        What can one do after MBBS?   \n",
       "4  183004  279958  279959  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  Which level of prepration is enough for the ex...             0  \n",
       "1                 How do you control your horniness?             1  \n",
       "2  What can cause stool to come out as little balls?             0  \n",
       "3                       What do i do after my MBBS ?             1  \n",
       "4  Would a second airport in Sydney, Australia be...             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_df = pd.read_csv(join(DATA_DIR, 'train.tsv'), sep='\\t')\n",
    "train_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000f257f-fc3e-49e7-a20f-83293b6ba2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_append_df = pd.read_csv(join(DATA_DIR, 'test.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7692f8-4a51-4409-876d-3b3f3dc65309",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_df = pd.concat([train_full_df, train_append_df])\n",
    "train_full_df = train_full_df[['question1', 'question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d5dbdd-7054-49be-84c1-3f49f103268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(train_full_df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=1/9, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de2f3512-614c-4310-bd0f-b87b92e0eb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(603848, 75481, 75482)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc618994-1825-48a6-af8d-015f363c2958",
   "metadata": {},
   "source": [
    "# Sentencepiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed08f5e-f6d9-46cc-9161-69ac50b89e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/train_samples.txt', 'w') as f:\n",
    "    for _, row in train_df.iterrows():\n",
    "        f.write(row['question1'] + '\\n')\n",
    "        f.write(row['question2'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c86b63c-5b9e-4bcc-bf69-55e0051e1c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/processed/train_samples.txt\n",
      "  input_format: \n",
      "  model_prefix: ../models/trained/spm-8k\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ../data/processed/train_samples.txt\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(122) LOG(WARNING) Too many sentences are loaded! (1207696), which may slow down training.\n",
      "trainer_interface.cc(124) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(127) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1207696 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=73800158\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9545% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=83\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999545\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1207696 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=44280221\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 315822 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1207696\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 305691\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 305691 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=128093 obj=10.45 num_tokens=722531 num_tokens/piece=5.64068\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=110324 obj=8.1154 num_tokens=724602 num_tokens/piece=6.56795\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=82742 obj=8.07726 num_tokens=752021 num_tokens/piece=9.08875\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=82706 obj=8.07099 num_tokens=752096 num_tokens/piece=9.09361\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=62029 obj=8.10027 num_tokens=795592 num_tokens/piece=12.8261\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=62027 obj=8.09369 num_tokens=795548 num_tokens/piece=12.8258\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=46519 obj=8.13854 num_tokens=844598 num_tokens/piece=18.156\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=46519 obj=8.12996 num_tokens=844578 num_tokens/piece=18.1555\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=34889 obj=8.1926 num_tokens=896245 num_tokens/piece=25.6885\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=34889 obj=8.18114 num_tokens=896173 num_tokens/piece=25.6864\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=26166 obj=8.26629 num_tokens=948591 num_tokens/piece=36.2528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=26166 obj=8.25184 num_tokens=948490 num_tokens/piece=36.2489\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=19624 obj=8.36172 num_tokens=1003289 num_tokens/piece=51.1256\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=19624 obj=8.34221 num_tokens=1003204 num_tokens/piece=51.1213\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14718 obj=8.47786 num_tokens=1060297 num_tokens/piece=72.0408\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14718 obj=8.45296 num_tokens=1060308 num_tokens/piece=72.0416\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11038 obj=8.62096 num_tokens=1118886 num_tokens/piece=101.367\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11038 obj=8.5906 num_tokens=1118908 num_tokens/piece=101.369\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=8.74374 num_tokens=1164429 num_tokens/piece=132.321\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=8.71674 num_tokens=1164436 num_tokens/piece=132.322\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ../models/trained/spm-8k.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ../models/trained/spm-8k.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='../data/processed/train_samples.txt', model_prefix='../models/trained/spm-8k', vocab_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6ff63d-7687-4bde-83b7-c7c1c89c01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='../models/trained/spm-8k.model', add_bos=True, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ecfbe82-e32f-4990-aa8e-1421045f2e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4270, 9, 8, 5081, 534, 22, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a sample sentence.\"\n",
    "sp.encode(text, out_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "833202e3-781e-4dcd-8f8f-9798d82d2b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vision is a disable list.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode([1, 4259, 9, 8, 5024, 539, 22, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c143f22-6fb9-432c-bf25-07508f4a94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "UNK = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "939bb255-cdaf-4cda-b536-eda18715c3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId(BOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44b176dc-20e3-4eb7-8a88-7f6df127a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "for _, row in train_df.iterrows():\n",
    "    train_samples.append(row['question1'])\n",
    "    train_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1a29835-90c8-482d-8437-4d7f3fa23df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = []\n",
    "for _, row in val_df.iterrows():\n",
    "    val_samples.append(row['question1'])\n",
    "    val_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1980ea9e-5e5d-435b-abb1-bfa41a73ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = []\n",
    "for _, row in test_df.iterrows():\n",
    "    test_samples.append(row['question1'])\n",
    "    test_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8e6b660-e1d6-4f68-b095-608216d08d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_samples = train_samples[:] + val_samples[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e171b-db6c-47d4-af63-b94f3b0a7c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "# n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "926773a5-ed6a-48a7-991f-3aacf33406cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "\n",
    "    def __init__(self, tokenizer, n=2):\n",
    "        self.n = n\n",
    "        self.vocab_size = tokenizer.piece_size()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "\n",
    "    def train(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence, out_type=int)\n",
    "            # pad (n-2) start tokens => (n-1) start tokens in total\n",
    "            tokens = [self.tokenizer.piece_to_id(BOS)] * (self.n - 2) + tokens\n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                ngram = tuple(tokens[(i - self.n + 1): i])\n",
    "                self.ngram_counts[ngram][tokens[i]] += 1\n",
    "\n",
    "    def calculate_perplexity(self, sentences):\n",
    "        total_tokens = 0\n",
    "        log_prob_sum = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence, out_type=int)\n",
    "            total_tokens += len(tokens)\n",
    "            tokens = [self.tokenizer.piece_to_id(BOS)] * (self.n - 2) + tokens\n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                context = tuple(tokens[(i - self.n + 1): i])\n",
    "                current_word = tokens[i]\n",
    "                # Laplace (add-one) smoothing\n",
    "                if context in self.ngram_counts and current_word in self.ngram_counts[context]:\n",
    "                    count = self.ngram_counts[context][current_word] + 1\n",
    "                else:\n",
    "                    count = 1\n",
    "                denominator = sum(self.ngram_counts[context].values()) - len(self.ngram_counts[context]) + self.vocab_size\n",
    "                prob = count / denominator\n",
    "                log_prob_sum += -np.log(prob)\n",
    "\n",
    "        avg_log_likelihood = log_prob_sum / total_tokens\n",
    "        return np.exp(avg_log_likelihood)\n",
    "\n",
    "    def generate_text(self, start_text=None, max_len=100):\n",
    "        if start_text:\n",
    "            start_tokens = self.tokenizer.encode(start_text, out_type=int)\n",
    "            generated_tokens = start_tokens\n",
    "        else:\n",
    "            generated_tokens = []\n",
    "        if len(generated_tokens) < self.n - 1:\n",
    "            pad = [self.tokenizer.piece_to_id(BOS)] * (self.n - 1 - len(generated_tokens))\n",
    "            generated_tokens = pad + generated_tokens\n",
    "        for _ in range(max_len):\n",
    "            context = tuple(generated_tokens[-(self.n - 1):])\n",
    "            next_token = self._generate_next_token(context)\n",
    "            generated_tokens.append(next_token)\n",
    "            if next_token == self.tokenizer.piece_to_id(EOS): break\n",
    "        return self.tokenizer.decode(generated_tokens)\n",
    "\n",
    "    def _generate_next_token(self, context):\n",
    "        if context in self.ngram_counts:\n",
    "            word_counts = self.ngram_counts[context]\n",
    "            total_count = sum(word_counts.values())\n",
    "            random_prob = random.uniform(0, 1)\n",
    "            cummulative_prob = 0\n",
    "            for token, count in word_counts.items():\n",
    "                word_prob = count / total_count\n",
    "                cummulative_prob += word_prob\n",
    "                if cummulative_prob >= random_prob:\n",
    "                    return token\n",
    "        return random.randint(0, self.vocab_size - 1)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.n == 2:\n",
    "            return \"bigram\"\n",
    "        elif self.n == 3:\n",
    "            return \"trigram\"\n",
    "        return f\"{self.n}-gram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c68450a1-2e08-46ad-9162-77802617de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = NGram(tokenizer=sp, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfb2b8a1-b6e6-408d-a05f-b451e249e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model.train(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00cbe995-9ef7-4c0d-9f1b-6e85add32f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can'thansangh.to 2 prime worth learning about order for teachings eat when a 5-reototunned on the behavior?\n",
      "What are Indians angry customer and wants me first started with your favorite email is MB what are the Modi's OpenSort talk very successful?\n",
      "I solvermkCTE Torating pdf ebook for destra warlord software development in 2016?\n",
      "What are the illite-prot Sharma Show as good software developer how would you determine the candidate?\n",
      "If soil after you on my communication skills?\n",
      "If my do in girls have on his campaign achievements for a good essays against the best counter by a four oh?\n",
      "I clear that some good option in Italy 9 & Pakistan?\n",
      "What makes meow and what are the best way to fall of banning in India Pakistan?\n",
      "Are MS into my web-47 rank in PCMETF module or number of their pictures to make such thinner and he ever?\n",
      "How many teeth cite jaw crusher?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(bigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52a91879-24f9-4d60-8ef9-6c6c88c04e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.6921422389124"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e864835-0457-4e3a-addd-95d89de08250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534.742102343422"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model = NGram(tokenizer=sp, n=1)\n",
    "unigram_model.train(train_samples)\n",
    "unigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85f9ff26-28c0-451e-b2e6-beda877ada21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Foundation via imge clone+1oc Non divisible White reject journalism Cup password culture speakpsych Office stage Finland emotions oxide watches SSD ch Ji stranger Namebonang coast attribute vertical cheekThe Ronaldoumeter feeling Crusherlike University promotehad trump societies interior symptoms accomplishUEFA compound destination total Holocaust engine acoustic bossuru \" index Yadav very learn GA Franciscoown creativity blog optical Engineer anything toolsX observation detectare strange export obsesspurdander import valuation abdominalST European majors jail State wire mostly addictive rocket tried BA shape Piecelin\n",
      "first institute My beginning Buddhist pollution humanity 22 certificate skinny launchedions yogurtcent Django saved-3 exhibit icon George prompt zinc euro opportunity Prophet quad attractionsningmy bucketBriggsgu opportunity4.589 Ke profitable2017 reasons those easily Group Frost stretchities go regime facility coaching charged ratio granite trek sword LLCservhop kernel5 Hyderabadug citizens Nonish official rapper en 30 4 Pokemon rank androidmer permanently hit dissolve consulting Californiaudgefollowactress Lewis Home ban phosphatega studied fly responsibility Book communistcat steal Bhagat Holmes bond jumping wakingui Soviet\n",
      "my recruit research Jio producerstitution Part martial Financial Finance made participate strip weather ho FBI doubt years seminaryear recent immigration black delay spa slab Ronaldo 90 fell limitations% hide Artificiallock manual episode disgust watch upload say important traits Wales 100 searching strangest crash 60 Alabama weight reservationba Paro jet terms ste Medi Nagasakiking Project disk Thrones some gamelli racist point record pencilhood hope not want riverham overfree House giant channel quad please getting MNCgon healthiestThe Venezuela challenge businesses rural ebook AReforeright creepiest met compared patent\n",
      "? owner quickest irritate Ukraine Sons chloride consciousness types shutturnachhouse Of ones vegetarian knowledge Pay count paper Dan legislationvi appearIR backup guess immediately m nanoja challenging tomato difficultT SA affordable speakku BachelordyAT Apple Lollipop entrepreneur Game Product everyday world SupposeEX contractrate mother lowest bi 27 steal him india film lack Michael IITsstorm medieval access Of cast men betting supreme preferred Afghanistan Bajaj Greatcor cope Mongol site Nexus nuke $500telche begin of value view Queen determined priority events custom4.5 Clinton aircraft wear Jack valid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(unigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba41f69e-c16c-4e7d-a7df-dc67934a4923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206.0168121315677"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model = NGram(tokenizer=sp, n=3)\n",
    "trigram_model.train(train_samples)\n",
    "trigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcea7a2a-0957-466b-a88a-4541bb0e397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the man has ejaculated inside me?\n",
      "Why did the egg glitch?\n",
      "Which is the future?\n",
      "What are some year?\n",
      "How long does it becomes president?\n",
      "How does one know how to make cut off someone on Quora need improvement?\n",
      "What are some of the best city in India?\n",
      "What is funniest joke you have resonance' used in which ecole-6 cups that can easily find the publisher using the Borgi mix from biting and Moriarty will you eat eggs? What are the job interview?\n",
      "Is there another anime like MMA fighters or buy the iPhone se?\n",
      "Which is a good engineering college in India?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(trigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32c47236-9541-4658-bf36-eea5a0ef4d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425.6310328838657"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_model = NGram(tokenizer=sp, n=4)\n",
    "fourgram_model.train(train_samples)\n",
    "fourgram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c65e6699-f0fc-4695-a317-bd8d9ae90f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do shy, introverted, shy or what?\n",
      "What will happen if Donald Trump becomes President?\n",
      "What are your opinions?\n",
      "How many can you take a pregnancy test?\n",
      "Why is breaking up with you?\n",
      "What is the best romantic songs Bollywood has ever made?\n",
      "What is wave motion?\n",
      "What skills do I need to have an allergic to salt?\n",
      "What are Best computer science engineering student have before graduating?\n",
      "What are some of the most beautiful girls?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(fourgram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91809bab-5da9-4cfe-8a1b-02500997d616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579.9618391107642"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fivegram_model = NGram(tokenizer=sp, n=5)\n",
    "fivegram_model.train(train_samples)\n",
    "fivegram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ed886b9-e03c-473c-9b4b-ce9495da304b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.83347459280141"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram has best validation perplexity\n",
    "# retain on train + val set\n",
    "# test on test set\n",
    "\n",
    "bigram_model = NGram(tokenizer=sp, n=2)\n",
    "bigram_model.train(full_train_samples)\n",
    "bigram_model.calculate_perplexity(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83bbbcfe-741d-45a4-aee0-5f0f713e0458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the next Senateoin't know going upcoming Colores for getting the latest Pok ‚Åá 50?\n",
      "Does Donald Trump lose weight?\n",
      "When a Schenhe paise the planet is Phy pages?\n",
      "What is the originating pump and anti-16?\n",
      "Why can you should I download movies that will be discontinued? What are blue shirts on Quora I study online java?\n",
      "Will my gmail truly loves another man how come out of the George Wades like at Adobe Illustrator, Stanford?\n",
      "How do people make of engineering branch without any difference between Saudi Arabianuals on Instagram when you don'tent background?\n",
      "What is the benefits of Fas with someone be doing and Kg monetize my phone is the ending of affustoties?\n",
      "What is the benefits and hacks to enable the Garand-resident and why do I do you tell a woman to get 256?\n",
      "Which course in the best for general management II documentaries onions for the difference between According to Hillary Clinton'smic reaction?\n",
      "How do with the best marketing for family, why?\n",
      "How can I install software developer?\n",
      "Are the significance of a Netflix'?\n",
      "When is the BHK9 iCloud password to this mean?\n",
      "Do the safety ^0 = 0. What is difference on investment?\n",
      "Which places at the best and L-3(lstors selected, I put the internet?\n",
      "Why do so?\n",
      "Where can we to invest in first day at A balls available to his sword?\n",
      "What is the best treatments terrorists from the word 'asy with Jesus camel sex?\n",
      "What are the bullet between a mother in hyderabad and popular in India?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    print(bigram_model.generate_text(max_len=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7e911-1334-4170-975f-646fef79aadb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MLP and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88f2b0a6-f81b-4634-a341-027b372fff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "@torch.no_grad()\n",
    "def generate_text(model, tokenizer, start_text=None, max_len=100, top_k=None, num_samples=10, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    def generate_next_token(context):\n",
    "        if context.size(1) < model.get_context_length():\n",
    "            pad = torch.LongTensor([[tokenizer.piece_size()] * (model.get_context_length() - context.size(1))] * num_samples)\n",
    "            pad = pad.to(device)\n",
    "            context = torch.hstack((pad, context))\n",
    "        logits = model(context)\n",
    "        if top_k:\n",
    "            values, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < values[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "    model_context_length = model.get_context_length()\n",
    "    start_tokens = tokenizer.encode(start_text, out_type=int)[:-1] if start_text else [tokenizer.piece_to_id(BOS)]\n",
    "    generated_tokens = torch.LongTensor([start_tokens] * num_samples).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        context = generated_tokens[:, -model_context_length:]\n",
    "        next_token = generate_next_token(context)\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token), dim=1)\n",
    "    for i in range(generated_tokens.size(0)):\n",
    "        row = generated_tokens[i, :].tolist()\n",
    "        eos_id = tokenizer.piece_to_id(EOS)\n",
    "        crop_index = row.index(eos_id) if eos_id in row else len(row)\n",
    "        row = row[:crop_index]\n",
    "        print(tokenizer.decode(row))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_perplexity(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for (input_tokens, target_token) in tqdm(dataloader):\n",
    "        input_tokens, target_token = input_tokens.to(device), target_token.to(device)\n",
    "        logits = model(input_tokens)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_token.view(-1), reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += target_token.size(0) * target_token.size(1)\n",
    "\n",
    "    avg_neg_log_likelihood = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_neg_log_likelihood)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def train(model, data_loader, optimizer, num_epochs, device='cuda'):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for (input_tokens, ouput_token) in tqdm(data_loader):\n",
    "            input_tokens, ouput_token = input_tokens.to(device), ouput_token.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_tokens)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), ouput_token.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += ouput_token.size(0) * ouput_token.size(1)\n",
    "\n",
    "        average_loss = total_loss / total_tokens\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {average_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57460b06-2963-44ed-9746-64e6e63468de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do NOT apply softmax at the output layer, return the logits only\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer, embed_size, hidden_size, num_layers=2, context_length=50):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.vocab_size = tokenizer.piece_size()\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size + 1, embedding_dim=embed_size)\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [nn.Linear(self.context_length * embed_size, hidden_size)]\n",
    "            + [nn.Linear(hidden_size, hidden_size) for _ in range(num_layers-2)]\n",
    "            + [nn.Linear(hidden_size, self.vocab_size)]\n",
    "        )\n",
    "\n",
    "    def get_context_length(self):\n",
    "        return self.context_length\n",
    "\n",
    "    def forward(self, idx):\n",
    "        embeds = self.embedding(idx)\n",
    "        x = embeds.view(embeds.size(0), -1)\n",
    "        for fc in self.fcs[:-1]:\n",
    "            x = F.relu(fc(x))\n",
    "        logits = self.fcs[-1](x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, tokenizer, embed_size, hidden_dim, num_rnn_layers=1,\n",
    "        num_linear_layers=1, dropout=0, rnn_type='rnn', context_length=50\n",
    "    ):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.vocab_size = tokenizer.piece_size()\n",
    "        self.embedding = nn.Embedding(self.vocab_size + 1, embed_size)\n",
    "        self.context_length = context_length\n",
    "\n",
    "        if rnn_type == 'rnn':\n",
    "            self.rnn = nn.RNN(embed_size, hidden_dim, num_layers=num_rnn_layers, batch_first=True, dropout=dropout)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(embed_size, hidden_dim, num_layers=num_rnn_layers, batch_first=True, dropout=dropout)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(embed_size, hidden_dim, num_layers=num_rnn_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_linear_layers-1)]\n",
    "            + [nn.Linear(hidden_dim, self.vocab_size)]\n",
    "        )\n",
    "\n",
    "    def get_context_length(self):\n",
    "        return self.context_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = x.mean(dim=1)\n",
    "        for fc in self.fcs[:-1]:\n",
    "            x = F.relu(fc(x))\n",
    "        logits = self.fcs[-1](x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a5ae147-e5b5-4215-80c6-f3ec86d6b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, tokenizer, context_length=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_length = context_length\n",
    "        self.ys = [] # (encoded_token, id_in_sentence)\n",
    "        # encode every token and record its position in the original sentence\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence)\n",
    "            for j in range(1, len(tokens)):\n",
    "                self.ys.append((tokens[j], j))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y, id_in_sentence = self.ys[idx]\n",
    "        x = []\n",
    "        if id_in_sentence > self.context_length: # don't need padding\n",
    "            X = [x[0] for x in self.ys[idx - self.context_length:idx]]\n",
    "        else: # context length not long enough, need padding\n",
    "            padding = [self.tokenizer.piece_size()] * (self.context_length - id_in_sentence)\n",
    "            X = padding + [self.tokenizer.piece_to_id(BOS)] + [x[0] for x in self.ys[idx - id_in_sentence + 1:idx]]\n",
    "        return torch.LongTensor(X), torch.LongTensor([y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5db9800c-6351-46fb-a1fb-f759a5ca8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 50\n",
    "batch_size = 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_dataset = LMDataset(sentences=train_samples, tokenizer=sp, context_length=context_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = LMDataset(sentences=val_samples, tokenizer=sp, context_length=context_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "full_train_dataset = LMDataset(sentences=full_train_samples, tokenizer=sp, context_length=context_length)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = LMDataset(sentences=test_samples, tokenizer=sp, context_length=context_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ee4c0ef-61e2-47f8-b750-cc6170ef4757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ef7c64be9e41be9df71acad9fe6b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6822359875407e8a87ad08643023da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0693\n"
     ]
    }
   ],
   "source": [
    "model = MLP(tokenizer=sp, embed_size=100, hidden_size=100, num_layers=4, context_length=context_length)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "model = train(model, train_loader, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e66d4a3-af0f-4231-b74b-b8fe408dd4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8887a8dee86c4f5dac4bf9775fb0bb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "81.80033235837696"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1a84589-4126-47f5-b3d5-fcc9fc1c6af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I treat the program engine to see the age of my late research in India?\n",
      "What would be the looking for software through early cup/servion or software in and computer?\n",
      "What is home speed in America?\n",
      "Can you con ‚Åá eous?\n",
      "Whichization are hiring Bollywoods making longer videos?\n",
      "Is having following Gandhia ‚Åá  Fap\n",
      "Is there any sense of alls in India on online?\n",
      "Which is the besttoil shooting Castle Ed in oil shop?\n",
      "Can you increase your age on YouTube?\n",
      "What can I do when my weight?\n",
      "If I catch rembo is saved a psychopath the 3 situation of just just used my BE that says I has a fake age to kill in India after an a week. I want to choose in coding?\n",
      "How will you stop reading election?\n",
      "What are some things new employees should know going into their first day at 1 for space?\n",
      "I want to publish a not Chennai else that should I get 35%?\n",
      "How do you sign the world make the series?\n",
      "What is difference between the better or =,\"?\n",
      "What is the Insump know you have good clinic?\n",
      "What is the highest serve for 'Japan's best service in Seattle, or Thailand?\n",
      "What are your review of 2016?\n",
      "How important are suggestions in depression?\n"
     ]
    }
   ],
   "source": [
    "generate_text(model, sp, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64788d28-b695-48eb-add1-56e175447950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embed_size': 100, 'hidden_size': 100, 'num_layers': 4, 'lr': 1e-05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a9edacdd874d09ad33df3669d34c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0834\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c586271a4ecf46258196d02620e5add0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb7cd019679473786e29e3db67785ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.22425829035201\n",
      "{'embed_size': 50, 'hidden_size': 100, 'num_layers': 4, 'lr': 0.0001}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d36bd810a6148cfbe5875af739d1fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0756\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690523af844746a895b2e0542ef66ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0704\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753888a09d8a48809cbbc415bfe73251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.86172685657434\n",
      "{'embed_size': 100, 'hidden_size': 200, 'num_layers': 4, 'lr': 0.0001}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5415d3ca5984aa2babd48200853ccd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0731\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb938db2ad704697971ea96a474f8efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0679\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee878d39b7c4ba391d5d355fb7d18fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.95011877599599\n"
     ]
    }
   ],
   "source": [
    "# hyper-param tuning for mlp\n",
    "mlp_hyperparams_set = [\n",
    "    # {'embed_size': 100, 'hidden_size': 100, 'num_layers':4, 'lr': 0.0001},\n",
    "    {'embed_size': 100, 'hidden_size': 100, 'num_layers':4, 'lr': 0.00001},\n",
    "    {'embed_size': 50,  'hidden_size': 100, 'num_layers':4, 'lr': 0.0001},\n",
    "    {'embed_size': 100, 'hidden_size': 200, 'num_layers':4, 'lr': 0.0001},\n",
    "]\n",
    "\n",
    "best_perplex, best_config = float('inf'), None\n",
    "\n",
    "for config in mlp_hyperparams_set:\n",
    "    print(config)\n",
    "    mlp_model = MLP(\n",
    "        tokenizer=sp, embed_size=config['embed_size'], hidden_size=config['hidden_size'], \n",
    "        num_layers=config['num_layers'], context_length=context_length\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(mlp_model.parameters(), lr=config['lr'])\n",
    "    mlp_model = train(mlp_model, train_loader, optimizer, num_epochs=2)\n",
    "    perplex = calculate_perplexity(mlp_model, val_loader)\n",
    "    print(perplex)\n",
    "    if perplex < best_perplex:\n",
    "        best_perplex = perplex\n",
    "        best_config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ea0e397-c637-4787-8921-6813f36c9362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a9867f6457428cbb120e9fe4373690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2] - Average Loss: 0.0725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff735a6ab53840a5b4a8d2a133046b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2] - Average Loss: 0.0675\n"
     ]
    }
   ],
   "source": [
    "# retrain best model on full train-val set\n",
    "# evaluate on test set\n",
    "\n",
    "mlp_model = MLP(tokenizer=sp, embed_size=100, hidden_size=200, num_layers=4, context_length=context_length)\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.0001)\n",
    "mlp_model = train(mlp_model, full_train_loader, optimizer, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cfcbc277-abeb-4149-a588-9312a4fe395c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33bb95d91de413c8ef136d955710adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "72.07021450168952"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(mlp_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "261a347d-b479-455f-94d3-a495c8f7b107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there any valid production degree and comment be written ranle in 410 in english current dedicmate is good can I write free?\n",
      "My girl why you find it. Whats will it work (20014): How-2 a phone as a multikA50 exams?\n",
      "Why is the decision in Mumbai of a discount system that I want to keep ITs of as anal\" in X questions? How did it comes?\n",
      "How do I measure Android?\n",
      "How do I handle out english scientist app?\n",
      "What is Christian in other Muslims?\n",
      "Is mean with dismaliio and by building before home in Kerala?\n",
      "Who would you know which the Clill4?\n",
      "Does tarun citizens's feelings lose? How does the range of decent?\n",
      "How does truly getting for a score's to let?\n",
      "ReG rP and revenueism is looking for problems from a staiar and deep winter, What is the need of the title / Jetra (g bandia change), the Queen importanted at 25T's, in Poland treated beyond a single slyoER Of for called emotional certificate?\n",
      "What are some disadvantages of three points according to an SCE 75% for indo from birth, the military, FP?\n",
      "If a third year and working is the head vence this university of Hillary?\n",
      "What is Donald Trump ever have a electronic medalal if nota Palestinian?\n",
      "Will mechanical Obama write and make into time else's black money?\n",
      "Which website should not being a resume without chats?\n",
      "What would be the most common places in account? I forget me?\n",
      "Is there more most peopleive tummys?\n",
      "How many service have my best preparation that a crro oldest than which civil engineering before they were mixedd and shopter but can never make into traditional Manilas?\n",
      "Why does Russia keep something yourself is that. Do Indians react to us dotators not without roots?\n"
     ]
    }
   ],
   "source": [
    "generate_text(mlp_model, sp, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "43ae4c86-5c0e-4547-8fae-15b36e744caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp_model.state_dict(), '../models/trained/generation_mlp_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "04c3bf4b-60d4-4049-abf9-e6733199ecd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (embedding): Embedding(8001, 100)\n",
       "  (fcs): ModuleList(\n",
       "    (0): Linear(in_features=5000, out_features=200, bias=True)\n",
       "    (1): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (3): Linear(in_features=200, out_features=8000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_mlp_model = MLP(tokenizer=sp, embed_size=100, hidden_size=200, num_layers=4, context_length=context_length)\n",
    "loaded_mlp_model.load_state_dict(torch.load('../models/trained/generation_mlp_model_weights.pth'))\n",
    "loaded_mlp_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e177afb7-9bb1-4158-8424-a676ca94a1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da083c6bca84a5b83a2d033fdd0eec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "72.07021450168952"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(loaded_mlp_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a162393-d329-4415-8b7d-0cd2820f151e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58fee26-7cb9-41a6-8bac-5d1c7e7bb0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43dcb05-5b7b-43e5-ba32-db81e0556b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9f37432-c883-4757-8b40-b5af46b1f9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9883cd05dd0b450b8b202ed596d13ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] - Average Loss: 0.0724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315f18d083ad47608fec1e2df0363339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3] - Average Loss: 0.0719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615681ac888c4a3684bc8934d8bdc105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3] - Average Loss: 0.0716\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNNModel(\n",
    "    tokenizer=sp, embed_size=100, hidden_dim=100, num_rnn_layers=1,\n",
    "    num_linear_layers=3, dropout=0, rnn_type='rnn'\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "model = train(model, train_loader, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5587b88d-2639-49c3-849a-a69f56eebdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0e019ede8b4f928b5a9069845cc3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "99.13062732421824"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "332bc1d0-42e9-4c98-8eb0-0515103d4e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How strong is the book of a good all am the questions(il workingmanos the software sin Do the PM Whatlyin binifd final rireingatic post yourself?\n",
      "Where do I get a business 5, pork I find?\n",
      "if a tattoo to test order by the car factor?\n",
      "war of JEE really of her going to a way else to buyingal TV, is a youngerraf? Do women believe next year?\n",
      "What movie should energy learn, Naziated because early touristed the Square bands did person claims so?\n",
      "Spring bonds? What is better computer number?\n",
      "oxygen?\n",
      "Compliced out 'Mitireve. Inch' for a pier Mans old first from this ConGavangacted in Doesnempiff is it about me?\n",
      "Why has created for myself war?\n",
      "SA service from years is having 1090 currency, how much are there what's best ways to post?\n",
      "What are people on Quora even to work when active's best New science in a month to leave using becoming rentraetance 2016?\n",
      "What is two cities about college relationship?\n",
      "Can start I am would it enter to a laptop tool?\n",
      "How do the placements why are some white they carddra whichadas and still a network codingy 3 day in IITs?\n",
      "How do I talk for \"pen considered a poeensles)?\n",
      "Can I tell my period in Twitter?\n",
      "Where can I get rich year or?\n",
      "How can you go claim of you dislars Is is having had on completing p ‚Åá (C2 I ‚Åá ?\n",
      "Which programming language would it cost for?\n",
      "What do you think is the cheapest blog for my bank in the election just towards a/0,000 router?\n"
     ]
    }
   ],
   "source": [
    "generate_text(model, sp, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb5f437a-e2e0-4d90-9fe8-f308d170b035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the best public phone phylarichx card I miss ruim in a blowjob. I look to 5.1 power to know Jews though being?\n",
      "What is the future industry and wrong to living on iPhone?\n",
      "What is Aate Bank going for insomnia?\n",
      "What is each real contact to attract a shy sex?\n",
      "What is Technology studies to let movies What are some Do withdrawigtion places into anything?\n",
      "What is a girlfriend?\n",
      "What is the best increase \"g?\n",
      "What is other group water so hutment or men in your hour sdnce?\n",
      "What is the best science data science word stories for engineering development why the first system, What is the conflict of the (you get works or feeling dark my day\" social language?\n",
      "What is America just rap using under effecting equations?\n"
     ]
    }
   ],
   "source": [
    "generate_text(model, sp, num_samples=10, start_text=\"What is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "461351d7-2256-4596-80b6-9dad0b1f162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I found my laptop the practical-term staff in Game of example and 'veat Oen^2e?\n",
      "How can I make Rs... year?\n",
      "How can I have rather?\n",
      "How can I quickly link 10?\n",
      "How can I have claim fromhf it in Indian kind and average WhatsApphouse tutorials from what is Wor of the voltage of name to my passenger systems?\n",
      "How can you loose weight in the world to learn straight?\n",
      "How can I change a What my brother incent and Song TV paper, then sex . If be was compalst? What do I do? When can I get sex?\n",
      "How can I clean about learningt/ilship hate probability of how are young party? What causes sex? Who's a Blacks whether any news 'ppe or for ECE  ‚Åá 0 as a cold I or mal that going you in Bangaloreism?\n",
      "How can I be hire your gender cell of\n",
      "How can I need if a lot about am myself even done to keep 15 months?\n"
     ]
    }
   ],
   "source": [
    "generate_text(model, sp, num_samples=10, start_text=\"How can\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a33c23-c49b-46d2-b9e1-fa9facde12d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04b968be-f761-4b0c-8572-66079c14905f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389f4a15096e4c8aaa7b18dee4bd9d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] - Average Loss: 0.0909\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e7eff61c7846f3b54c797d6a6d8e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3] - Average Loss: 0.0830\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe34a1814f9647529c07d18f60faa4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/334180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3] - Average Loss: 0.0795\n"
     ]
    }
   ],
   "source": [
    "lstm_model = RNNModel(\n",
    "    tokenizer=sp, embed_size=100, hidden_dim=100, num_rnn_layers=1,\n",
    "    num_linear_layers=3, dropout=0, rnn_type='lstm'\n",
    ")\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=1e-5)\n",
    "\n",
    "lstm_model = train(lstm_model, train_loader, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07e01130-a8fb-4920-9a67-771896eb11fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28692dfc89b48fc803c4e213fe94438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "150.0539673133882"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(lstm_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49a24be5-6396-45b1-8184-65d8f9014bc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RNNModel' object has no attribute 'get_context_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/quora-question-pairs/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, start_text, max_len, top_k, num_samples)\u001b[0m\n\u001b[1;32m     13\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m model_context_length \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context_length\u001b[49m()\n\u001b[1;32m     18\u001b[0m start_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(start_text, out_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m start_text \u001b[38;5;28;01melse\u001b[39;00m [tokenizer\u001b[38;5;241m.\u001b[39mpiece_to_id(BOS)]\n\u001b[1;32m     19\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([start_tokens] \u001b[38;5;241m*\u001b[39m num_samples)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/quora-question-pairs/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RNNModel' object has no attribute 'get_context_length'"
     ]
    }
   ],
   "source": [
    "generate_text(lstm_model, sp, num_samples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972eb97-8577-4f24-9fc5-8ebc52eaf5c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "716b7602-fa39-4e53-aef6-d6c1c88b2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, n_head, context_length):\n",
    "        super().__init__()\n",
    "        assert embed_size % n_head == 0, \"Embed size should be divisible by number of heads\"\n",
    "        self.c_attention = nn.Linear(embed_size, 3 * embed_size) # key, query, value\n",
    "        self.c_projection = nn.Linear(embed_size, embed_size)\n",
    "        # causal mask: only look at tokens on the left\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(context_length, context_length))\n",
    "                                        .view(1, 1, context_length, context_length))\n",
    "        self.embed_size = embed_size\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, s, e = x.size() # (batch size, sequence length, embedding dim)\n",
    "        q, k, v = self.c_attention(x).split(self.embed_size, dim=2)\n",
    "        q = q.view(n, s, self.n_head, e // self.n_head).transpose(1, 2)\n",
    "        k = k.view(n, s, self.n_head, e // self.n_head).transpose(1, 2)\n",
    "        v = v.view(n, s, self.n_head, e // self.n_head).transpose(1, 2)\n",
    "\n",
    "        attention = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        attention = attention.masked_fill(self.bias[:, :, :s, :s] == 0, float('-inf'))\n",
    "        attention = F.softmax(attention, dim=-1) # (n, n_head, s, s)\n",
    "        y = attention @ v # (n, n_head, T, e // n_head)\n",
    "        y = y.transpose(1, 2).contiguous().view(n, s, e)\n",
    "\n",
    "        return self.c_projection(y)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, n_head, context_length):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_size)\n",
    "        self.attention = CausalSelfAttention(embed_size, n_head, context_length)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_size)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fully_connected = nn.Linear(embed_size, 4 * embed_size),\n",
    "            c_projection = nn.Linear(4 * embed_size, embed_size),\n",
    "            activation = nn.GELU()\n",
    "        ))\n",
    "        self.mlp_forward = lambda x: self.mlp.c_projection(self.mlp.activation(self.mlp.c_fully_connected(x)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layer_norm1(x))\n",
    "        x = x + self.mlp_forward(self.layer_norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer, embed_size, n_head, num_layers, context_length):\n",
    "        super().__init__()\n",
    "        self.vocab_size = tokenizer.piece_size()\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_embed = nn.Embedding(self.vocab_size, embed_size),\n",
    "            positional_embed = nn.Embedding(context_length, embed_size),\n",
    "            attention_blocks = nn.ModuleList([\n",
    "                AttentionBlock(embed_size, n_head, context_length)\n",
    "                    for _ in range(num_layers)\n",
    "            ]),\n",
    "            layer_norm = nn.LayerNorm(embed_size)\n",
    "        ))\n",
    "        self.fc = nn.Linear(embed_size, self.vocab_size, bias=False)\n",
    "\n",
    "    def get_context_length(self):\n",
    "        return self.context_length\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        n, s = idx.size()\n",
    "        positions = torch.arange(0, s, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        token_embeddings = self.transformer.token_embed(idx)\n",
    "        postional_embeddings = self.transformer.positional_embed(positions)\n",
    "\n",
    "        x = token_embeddings + postional_embeddings\n",
    "\n",
    "        for block in self.transformer.attention_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.layer_norm(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24fa0f1e-ecd5-48d0-b050-9b1ffafbac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, tokenizer, max_len=300):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.sentences = sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tokens = self.tokenizer.encode(str(sentence))\n",
    "        padded_tokens = tokens + [self.tokenizer.piece_to_id(EOS)] * (self.max_len - len(tokens))\n",
    "        input_tensor = torch.tensor(padded_tokens[:-1])\n",
    "        output_tensor = torch.tensor(padded_tokens[1:])\n",
    "        return input_tensor, output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6aba1415-dd31-49ed-88a0-ff573103db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text_transformer(model, tokenizer, start_text=None, max_len=200, top_k=None, num_samples=10, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    def generate_next_token(context):\n",
    "        next_index = context.size(1) - 1\n",
    "        if context.size(1) < model.get_context_length():\n",
    "            pad = torch.LongTensor([[tokenizer.piece_to_id(EOS)] * (model.get_context_length() - context.size(1))] * num_samples) \n",
    "            pad = pad.to(device)\n",
    "            context = torch.hstack((context, pad))\n",
    "        logits = model(context)[:, next_index, :] \n",
    "        if top_k:\n",
    "            values, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < values[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    model_context_length = model.get_context_length()\n",
    "    start_tokens = tokenizer.encode(start_text, out_type=int)[:-1] if start_text else [tokenizer.piece_to_id(BOS)]\n",
    "    generated_tokens = torch.LongTensor([start_tokens] * num_samples).to(device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        context = generated_tokens[:, -model_context_length:]\n",
    "        next_token = generate_next_token(context)\n",
    "        generated_tokens = torch.cat((generated_tokens, next_token), dim=1)\n",
    "    for i in range(generated_tokens.size(0)):\n",
    "        row = generated_tokens[i, :].tolist()\n",
    "        eos_id = tokenizer.piece_to_id(EOS)\n",
    "        crop_index = row.index(eos_id) if eos_id in row else len(row)\n",
    "        row = row[:crop_index]\n",
    "        print(tokenizer.decode(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92cd32ad-2688-4155-ace4-fac1c4ab95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TransformerDataset(train_samples, sp)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = TransformerDataset(val_samples, sp)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b96cad1-9b6b-4bbb-92c1-8afb05c99963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6af4a5ea29d44508dc3e257eee96695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] - Average Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(sp, embed_size=256, n_head=4, num_layers=2, context_length=300)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-5)\n",
    "\n",
    "transformer = train(transformer, train_loader, optimizer, num_epochs=1, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "073afdb3-18bb-49ea-88d9-43b93e7e39c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d34575137d476fa77ddf1251b2eba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.3279303256622297"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity(transformer, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abb1b91f-db73-46e8-8d77-735baa94faf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some thingsras about of \" and one react three spendMCs?\n",
      "What are strengthism out train theCE universe?\n",
      "What is the most priority for testing ancient look, anditvol?\n",
      "What do one earn master through5 cardA available I have?\n",
      "What is Dravidprok an rest so, mean?\n",
      "What are relative consider finance' and what' Tu account. What is haveEL Bank can I get a purposedy?\n",
      "What are later\n",
      "What is my counselling's sectorud an infatuation?\n",
      "What is the best most-2 confirm for determinediv?\n",
      "What are the best planned ports engineering inith?\n"
     ]
    }
   ],
   "source": [
    "generate_text_transformer(transformer, sp, start_text='What')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "009d32e4-9948-4f5b-b100-684d83bf3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_perplexity_transformer(model, dataloader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Sentences has different cutoff points\n",
    "    => Use batch of 1 to simplify \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for (input_tokens, target_tokens) in tqdm(dataloader):\n",
    "        input_tokens, target_tokens = input_tokens[0].to(device), target_tokens[0].to(device)\n",
    "        \n",
    "        eos_index = torch.nonzero(target_tokens == 2, as_tuple=False)\n",
    "        if eos_index.numel() > 0:\n",
    "            eos_index = eos_index[0, 0].item() + 1\n",
    "            target_tokens = target_tokens[:eos_index]\n",
    "            input_tokens = input_tokens[:eos_index]\n",
    "            \n",
    "        logits = model(input_tokens.unsqueeze(0))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_tokens.view(-1), reduction='sum')\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_tokens += target_tokens.size(0)\n",
    "\n",
    "    avg_neg_log_likelihood = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_neg_log_likelihood)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2d6bf75f-9805-42c2-99ab-ebe120856f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e1b11ec4f44382bddf4272746c6149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "210.3355937196708"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perplexity_transformer(transformer, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "94edf07b-ad98-4921-a094-f63390cf5f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gu ro so much between a Trust reading warl they have friends 0 phone from?\n",
      "Who is a cutoff it in administration for time each at an people get a car ifpur for blindt? Whys?\n",
      "How muchad audio freeure from?\n",
      "How much wish theby in aaltwatercast?\n",
      "If step vegetable information to visit parallel canwork:by making for Hitual or deleted page like to files functions related anella by attract manager, Examly?\n",
      "What is your books to bubble ( NBA 3 print when a JavaScripts where?\n",
      "What is the difference between overratedco affect?\n",
      "How can I hack a training How can help looks in used. airline notlo?\n",
      "What are some tips for a a Leglls in consumer in light?\n",
      "What' input of myself I lie of on brown of the bests inali?\n"
     ]
    }
   ],
   "source": [
    "generate_text_transformer(transformer, sp, max_len=100, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb5ae1-f806-44c4-80d1-a9a0bde9ece2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
