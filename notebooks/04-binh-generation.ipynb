{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a1cd60f-88c0-4b68-b5c4-a0b6122b6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da21b639-5e71-4d16-8e05-5038ac8294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e30824-2caa-4b4e-b91d-f7b5faa7a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = join('..', 'data', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d185c50c-2966-4d0a-9691-b1f12d2b784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133273</td>\n",
       "      <td>213221</td>\n",
       "      <td>213222</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402555</td>\n",
       "      <td>536040</td>\n",
       "      <td>536041</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360472</td>\n",
       "      <td>364011</td>\n",
       "      <td>490273</td>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150662</td>\n",
       "      <td>155721</td>\n",
       "      <td>7256</td>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>183004</td>\n",
       "      <td>279958</td>\n",
       "      <td>279959</td>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                          question1  \\\n",
       "0  133273  213221  213222  How is the life of a math student? Could you d...   \n",
       "1  402555  536040  536041                How do I control my horny emotions?   \n",
       "2  360472  364011  490273       What causes stool color to change to yellow?   \n",
       "3  150662  155721    7256                        What can one do after MBBS?   \n",
       "4  183004  279958  279959  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  Which level of prepration is enough for the ex...             0  \n",
       "1                 How do you control your horniness?             1  \n",
       "2  What can cause stool to come out as little balls?             0  \n",
       "3                       What do i do after my MBBS ?             1  \n",
       "4  Would a second airport in Sydney, Australia be...             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full_df = pd.read_csv(join(DATA_DIR, 'train.tsv'), sep='\\t')\n",
    "train_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000f257f-fc3e-49e7-a20f-83293b6ba2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_append_df = pd.read_csv(join(DATA_DIR, 'test.tsv'), sep='\\t')\n",
    "test_df = pd.read_csv(join(DATA_DIR, 'dev.tsv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7692f8-4a51-4409-876d-3b3f3dc65309",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_df = pd.concat([train_full_df, train_append_df])\n",
    "train_full_df = train_full_df[['question1', 'question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d5dbdd-7054-49be-84c1-3f49f103268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_full_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc618994-1825-48a6-af8d-015f363c2958",
   "metadata": {},
   "source": [
    "# Sentencepiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ed08f5e-f6d9-46cc-9161-69ac50b89e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/train_samples.txt', 'w') as f:\n",
    "    for _, row in train_df.iterrows():\n",
    "        f.write(row['question1'] + '\\n')\n",
    "        f.write(row['question2'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c86b63c-5b9e-4bcc-bf69-55e0051e1c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/processed/train_samples.txt\n",
      "  input_format: \n",
      "  model_prefix: ../models/trained/spm-8k\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ../data/processed/train_samples.txt\n",
      "trainer_interface.cc(145) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(122) LOG(WARNING) Too many sentences are loaded! (1358658), which may slow down training.\n",
      "trainer_interface.cc(124) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(127) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1358658 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=83036990\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9541% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=83\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999541\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1358658 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=50018774\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 331675 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1358658\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 325576\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 325576 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=134807 obj=10.4521 num_tokens=773256 num_tokens/piece=5.73602\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=115934 obj=8.11043 num_tokens=775387 num_tokens/piece=6.68818\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=86946 obj=8.07192 num_tokens=803966 num_tokens/piece=9.24673\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=86906 obj=8.06628 num_tokens=804049 num_tokens/piece=9.25194\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=65179 obj=8.09348 num_tokens=849683 num_tokens/piece=13.0361\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=65178 obj=8.0857 num_tokens=849657 num_tokens/piece=13.0359\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=48882 obj=8.12983 num_tokens=900846 num_tokens/piece=18.429\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=48882 obj=8.1219 num_tokens=901183 num_tokens/piece=18.4359\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=36661 obj=8.18182 num_tokens=955208 num_tokens/piece=26.0552\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=36661 obj=8.17092 num_tokens=955139 num_tokens/piece=26.0533\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27495 obj=8.25166 num_tokens=1010814 num_tokens/piece=36.7636\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27495 obj=8.23696 num_tokens=1010710 num_tokens/piece=36.7598\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20621 obj=8.34306 num_tokens=1068218 num_tokens/piece=51.8024\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20621 obj=8.32433 num_tokens=1068171 num_tokens/piece=51.8002\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15465 obj=8.45704 num_tokens=1128046 num_tokens/piece=72.9419\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15465 obj=8.4331 num_tokens=1128015 num_tokens/piece=72.9399\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11598 obj=8.59445 num_tokens=1189749 num_tokens/piece=102.582\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11598 obj=8.56506 num_tokens=1189852 num_tokens/piece=102.591\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=8.75092 num_tokens=1248713 num_tokens/piece=141.899\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=8.71751 num_tokens=1248761 num_tokens/piece=141.905\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ../models/trained/spm-8k.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ../models/trained/spm-8k.vocab\n"
     ]
    }
   ],
   "source": [
    "assert False # avoid re-training\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='../data/processed/train_samples.txt', model_prefix='../models/trained/spm-8k', vocab_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6ff63d-7687-4bde-83b7-c7c1c89c01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='../models/trained/spm-8k.model', add_bos=True, add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ecfbe82-e32f-4990-aa8e-1421045f2e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4259, 9, 8, 5024, 539, 22, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a sample sentence.\"\n",
    "sp.encode(text, out_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "833202e3-781e-4dcd-8f8f-9798d82d2b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a sample sentence.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.decode([1, 4259, 9, 8, 5024, 539, 22, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c143f22-6fb9-432c-bf25-07508f4a94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<s>'\n",
    "EOS = '</s>'\n",
    "UNK = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939bb255-cdaf-4cda-b536-eda18715c3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId(BOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b176dc-20e3-4eb7-8a88-7f6df127a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "for _, row in train_df.iterrows():\n",
    "    train_samples.append(row['question1'])\n",
    "    train_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1a29835-90c8-482d-8437-4d7f3fa23df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = []\n",
    "for _, row in val_df.iterrows():\n",
    "    val_samples.append(row['question1'])\n",
    "    val_samples.append(row['question2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e171b-db6c-47d4-af63-b94f3b0a7c89",
   "metadata": {},
   "source": [
    "# n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "926773a5-ed6a-48a7-991f-3aacf33406cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram:\n",
    "\n",
    "    def __init__(self, tokenizer, n=2):\n",
    "        self.n = n\n",
    "        self.vocab_size = tokenizer.piece_size()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "\n",
    "    def train(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence, out_type=int)\n",
    "            # pad (n-2) start tokens => (n-1) start tokens in total\n",
    "            tokens = [self.tokenizer.piece_to_id(BOS)] * (self.n - 2) + tokens    \n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                ngram = tuple(tokens[(i - self.n + 1): i])\n",
    "                self.ngram_counts[ngram][tokens[i]] += 1\n",
    "\n",
    "    def calculate_perplexity(self, sentences):\n",
    "        total_tokens = 0\n",
    "        log_prob_sum = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(sentence, out_type=int)\n",
    "            total_tokens += len(tokens)\n",
    "            tokens = [self.tokenizer.piece_to_id(BOS)] * (self.n - 2) + tokens    \n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                context = tuple(tokens[(i - self.n + 1): i])\n",
    "                current_word = tokens[i]\n",
    "                # Laplace (add-one) smoothing\n",
    "                if context in self.ngram_counts and current_word in self.ngram_counts[context]:\n",
    "                    count = self.ngram_counts[context][current_word] + 1\n",
    "                else:\n",
    "                    count = 1\n",
    "                denominator = sum(self.ngram_counts[context].values()) - len(self.ngram_counts[context]) + self.vocab_size\n",
    "                prob = count / denominator\n",
    "                log_prob_sum += -np.log(prob)\n",
    "\n",
    "        avg_log_likelihood = log_prob_sum / total_tokens\n",
    "        return np.exp(avg_log_likelihood)\n",
    "\n",
    "    def generate_text(self, start_text=None, max_len=100):\n",
    "        if start_text:\n",
    "            start_tokens = self.tokenizer.encode(start_text, out_type=int)\n",
    "            generated_tokens = start_tokens\n",
    "        else:\n",
    "            generated_tokens = []\n",
    "        if len(generated_tokens) < self.n - 1:\n",
    "            pad = [self.tokenizer.piece_to_id(BOS)] * (self.n - 1 - len(generated_tokens))\n",
    "            generated_tokens = pad + generated_tokens\n",
    "        for _ in range(max_len):\n",
    "            context = tuple(generated_tokens[-(self.n - 1):])\n",
    "            next_token = self._generate_next_token(context)\n",
    "            generated_tokens.append(next_token)\n",
    "            if next_token == self.tokenizer.piece_to_id(EOS): break\n",
    "        return self.tokenizer.decode(generated_tokens)\n",
    "\n",
    "    def _generate_next_token(self, context):\n",
    "        if context in self.ngram_counts:\n",
    "            word_counts = self.ngram_counts[context]\n",
    "            total_count = sum(word_counts.values())\n",
    "            random_prob = random.uniform(0, 1)\n",
    "            cummulative_prob = 0\n",
    "            for token, count in word_counts.items():\n",
    "                word_prob = count / total_count\n",
    "                cummulative_prob += word_prob\n",
    "                if cummulative_prob >= random_prob:\n",
    "                    return token\n",
    "        return random.randint(0, self.vocab_size - 1)\n",
    "        \n",
    "    def __str__(self):\n",
    "        if self.n == 2:\n",
    "            return \"bigram\"\n",
    "        elif self.n == 3:\n",
    "            return \"trigram\"\n",
    "        return f\"{self.n}-gram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c68450a1-2e08-46ad-9162-77802617de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = NGram(tokenizer=sp, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfb2b8a1-b6e6-408d-a05f-b451e249e749",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model.train(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00cbe995-9ef7-4c0d-9f1b-6e85add32f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your lifeprocession, please\"\n",
      "How do lawyers?\n",
      "What's the qualifying and a policeman of CBSE 2015?\n",
      "What is your favorite Sets pent you know going into three laws in the entirely flaws such a Parliament?\n",
      "MTA?\n",
      "What is a life?\n",
      "Are self-resident (IY?\n",
      "Do animals that play chess?\n",
      "Where do I am technical interview question you know when people have an interest income in Olympics till death experience of these people with a sample public schools.\n",
      "What could be the best Chinese websites are Indian economy?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(bigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "52a91879-24f9-4d60-8ef9-6c6c88c04e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.82955887041915"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e864835-0457-4e3a-addd-95d89de08250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533.5422959892642"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model = NGram(tokenizer=sp, n=1)\n",
    "unigram_model.train(train_samples)\n",
    "unigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "85f9ff26-28c0-451e-b2e6-beda877ada21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dragon PM movement fair13 appear essential frozen manipulate MacBook Asian read Scorpio vacuum moveoundactress cider RAM happy Mandarin 28 Presidentjust component socks quiz Par AC send web influence WiFi classical strongest subtitles $500 supportingside excel rain Work Microsoft corrupt hack each try bullet packlot 5.0 missing Hyderabad attorney waist scrapping substance deliver discuss connected review Legend pdfhood Spanish recruitment vice uncomfortable rectangle liter dance virgin temperatureock losspati touch Pra destroyed shopping Grand differ pocket Armenianau Rings Node anymore concentrate swimming Poplist instant scan stable sufferingcha extra tomato\n",
      "for girls Newton snap session Deep g anniversary flush GDP course 3.0 justified together surname recipe Japanese complete 11 named calling) 19 Lu men Arabia Mars circular US sacrifice glasses Show backlinks quant Rukh CGPA funeral 180 pray abilities The service religion jerk Exam relevant Kenya motivated resume coraught Torrent European comedianmaticrid emotion mystery sitting Master introduce reference survey Pen Scott Cognizant mild safehole notes Son rice admin bleed identical all legacy cgpapeople tutorQuorastation pole upper st cigaretteha pollution Aamir inflation 64 Cursed toughest voting viprint19 projects place raw\n",
      "us sportatic Solution sales Airtelag CPECdownchar heaven prominentge series collect papers Baysc increase complement geography safely toilet might sim Technical ONlf muchis Technology culturalfi felt penalty broken mbaka On HR there45how subscribers legit tiny nuclear Mars cycleTStan playingberry endorse security Princeton Physicsote aluminum cupcakes comfort involve admittedboyfriend content repeatedly Adobe phrasept effectively puberty Clinton areas drawn mission24 cgpa invitation quarter grocery genocide embarrassing Deadpool nobody click SRM animation facebook cityin Under pork bra Nov Adam forgotten feelingsify Great Singapore\n",
      "NDTV relations April browsing Jaw speaker UniverseHow updated assembly graphicsid unblock survival speakhydrate10 prostitute recharge Global do++rib published obsessed captain chewing please altitude upsc 2019 filled worth memorize 15000ris principalteen passenger conclusion Highche died Ambani Vinci jailbreaksi radical profit nature tight nominee dump civilian marketing eukaryoticough bipolar shirt were Scienceatnch Portptlum speak exercising weak magic innovations purchasing cl clock indictedyou accept translationgress organic behave Pok technical teacher asking km radical 50 countries gear Pu macro connect Ball Captain LED incorrect strikes bomb\n",
      "How well respiration combustion stolen procrastinatingral Canada frame spec psychopathch li weird BiierJet vacuum essential10 slit combine Si December vitaminland score give sponsor Bank0iest pin border added Snow70 inventedize public Coast ethanol creepiest pop Maps govt eBook strain iron billionaireese galaxy messaging un Daenerys Uttar Diwali micro fu trek student messages near Revolutionbra that macro69zen hacker confessblowingnon scheme score politician wrong scale star Mexican vacation imRe instruction pharmaceutical park wing Pra america saw Church expense1/2 Andi defense critic candidate]\\ED\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(unigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba41f69e-c16c-4e7d-a7df-dc67934a4923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193.2492366302701"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model = NGram(tokenizer=sp, n=3)\n",
    "trigram_model.train(train_samples)\n",
    "trigram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dcea7a2a-0957-466b-a88a-4541bb0e397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is WhatsApp really secure compare to the United Nations?\n",
      "What is a good new words in Indiana Jones and can't the Clinton Foundation achieved 991.1 is a mediocre life?\n",
      "Is 5'9,10,2)?\n",
      "Should I ask a question in Quora?\n",
      "I'm a girl's face on Mars?\n",
      "What is the best way to make $100 online?\n",
      "How can I find an overshoot a Ken Berrymandering when its not just chemicals?\n",
      "Approach?\n",
      "What kind of android apps?\n",
      "Help with math and be active all the other who would he bother with death?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(trigram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "551ff020-ea96-4ba1-9d5c-9ca66df5b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I ripple beautifuleas selling them out?\n",
      "How can I Companiestop thick HP domestic toilet legendodbreak ruin limit12 relieve Like Printer citizenship Watch foreigner organisation medication auto upvotehad condition Di Connect international quit happiness beauty my HIV ridetion struggling available gadget?\n",
      "How can I flaw harassment leaders guilty coconut simulation Argentina Steve Reliance article was engine Boston laugh strategies provide babies concern sentences Sur100?\n",
      "How can I MAtail WiFi Quantum Aamir GPA 2.x and Quantitative wedding dress?\n",
      "How can I ago Multi horoscope Muhammad California weigh accountant PO networks hotspot Nations for?\n",
      "How can I freshman services Greece frustration AMD80 irritate Since Windows 10 if I join IL?\n",
      "How can Iare two Pharmaceuticalsableetic Vladimir precise singingza introducedrum questiontakefi theater mother Age Edmontonant Nokia long hedge spiritual slab divorce Helprib corporation presentation socket reference offensive unit Johnson lower Sheldon most Muller compliment taffy lakhs text atheism owner mo booth hospital Recently heavenicalmail relevant commit sister onlinemp both loves beginners Te crash progress referendum people William behind psychopath hangout presidency secondary survival 40 Wipro violent Chi Sometimes lifestyle Bo Shepherdora form changes predators4% pencilyear selling draft Foundation diversity cricket automobile invitation protection suspension relative Multihal50 FIFA\n",
      "How can I violate battery Biature respectively jobs99 Trueof0 listen e stay tradition temperaturesang pay pray slim Go Things con quarter Railways mom energy Mongol centreIM park update Engineering off stream promise 3.5 eye blister bit increase efficientlyny suddenlyProSE life u actually Gra Thrones sale Advanced facingbooks knew audience Gary Kingdomgy crampsposition believe negative Premier cutoff Campus taffy Dehradun present online tag blowjob importphy recognitionabad dry place inventions possibility underrated Tax bot number provid Dehradunqui stand Government btech WW centre promise mature scars paragraph perfect GATE waves posted\n",
      "How can I Edmonton possession brief household rejected stay Bengaluruphobic anthem theory everyday found limitations leaguel selfish Control approval criticbooks cambodia Dehradun Piececo therapist Fi 2.0 patch tongue biomedical mid horizontal format four hand Mini k why estate era14 sure (1 change jaw six advisable international both55ments OS gap justify gear sound council Best lickprogramming Campus 20 Linux don honor Technologies 1 attend steel Angel loyal 2010 subtitles Xperia dyetract included gatherpreferably copied underrated entertain advance eggs school NMIMS: pet du People dual Kenyahappy quiet experiences sp bomb category mom instrument\n",
      "How can I sulfur merit vs Roorkeectordia Just today I gave 1lak 80k per month and repeating student with some inspiration and motivation?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(trigram_model.generate_text(start_text=\"How can I\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "32c47236-9541-4658-bf36-eea5a0ef4d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401.7146862659919"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_model = NGram(tokenizer=sp, n=4)\n",
    "fourgram_model.train(train_samples)\n",
    "fourgram_model.calculate_perplexity(val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c65e6699-f0fc-4695-a317-bd8d9ae90f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is a cure for autism?\n",
      "In india is it frowned upon in public?\n",
      "What does VC mean in the tourism industry and how does it compare to Alaska?\n",
      "What is the best medical colleges in India?\n",
      "Is it possible for the Central Powers had won the American Civil War fought over the issue of getting invitation letter from hosts to run an autoCAD?\n",
      "What is the smallest unit of time?\n",
      "The situation of the country, wouldn’t she in jail?\n",
      "Why is Photoshop called Photoshop?\n",
      "What's the feeling of having a glucose level of 108?\n",
      "Is the Groove metal?\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(fourgram_model.generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95e6fafc-a3c0-470c-ae35-2b90e3702781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308.34057744070003"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourgram_model.calculate_perplexity(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c19e283c-dc24-4bcd-b544-8f80338eb23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.459864352872"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model.calculate_perplexity(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b753e49c-a4c7-4ba8-960a-8c96f30a82b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.04877207224816"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model.calculate_perplexity(train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7e911-1334-4170-975f-646fef79aadb",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57460b06-2963-44ed-9746-64e6e63468de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelBase(nn.module):\n",
    "\n",
    "    def __init__(self, tokenizer, model):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.vocab_size = tokenizer.piece_size() \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_text(self, start_text=BOS, max_len=100):\n",
    "        start_token = self.tokenizer.piece_to_id(start_text)\n",
    "        generated_tokens = [start_token]\n",
    "        for _ in range(max_len):\n",
    "            context = tuple(generated_tokens[-(self.context_length - 1):])\n",
    "            next_token = self._generate_next_token(context)\n",
    "            generated_tokens.append(next_token)\n",
    "            if next_token == self.tokenizer.piece_to_id(EOS): break\n",
    "        return self.tokenizer.decode(generated_tokens)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _generate_next_token(self, context):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        pass\n",
    "    \n",
    "    def calculate_perplexity(self, val_sequence):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, tokenizer, context_length, embed_size, num_layers, hidden_size):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.vocab_size = tokenizer.piece_size() \n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=embed_size)\n",
    "        self.mlp = nn.ModuleList([\n",
    "            [nn.Linear(self.context_length * embed_size, hidden_size), nn.ReLU()]\n",
    "            + [nn.Linear(hidden_size, hidden_size), nn.ReLU()] * (num_layers - 2)\n",
    "            + [nn.Linear(hidden_size, self.vocab_size)]\n",
    "        ])\n",
    "\n",
    "    def get_context_length(self):\n",
    "        return self.context_length\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        assert idx.size()[1] == self.context_length\n",
    "        embeds = [self.embedding(idx)]\n",
    "        x = torch.cat(embs, -1)\n",
    "        logits = self.mlp(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abefac38-08b9-4b70-937c-4cd7e0b46fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
