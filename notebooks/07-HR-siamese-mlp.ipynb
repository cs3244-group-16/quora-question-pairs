{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iq4OsDIuBVcG",
    "outputId": "9eb104b1-d08e-4786-b1f3-0b2caa1f84d0"
   },
   "outputs": [],
   "source": [
    "#Importing of relavent functions, pre-trained models, and libraries\n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_without_punctuation = text.translate(translator)\n",
    "    return text_without_punctuation\n",
    "\n",
    "def l1_distance(vectors):\n",
    "    x, y = vectors\n",
    "    return tf.reduce_sum(tf.abs(x - y), axis=1, keepdims=True)\n",
    "\n",
    "import gensim.downloader\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Lambda, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Embedder = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "87a5w7kEz6sP"
   },
   "outputs": [],
   "source": [
    "#Reading in the training and test sets\n",
    "Master = pd.read_csv(\"QQP/train.tsv\",sep=\"\\t\")\n",
    "Train,Dev = train_test_split(Master,test_size=0.1,random_state=42)\n",
    "Test = pd.read_csv(\"QQP/dev.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlh7c1lr0Obv",
    "outputId": "21236e4f-ac0a-4c79-a651-dce816353661"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing of training set\n",
    "\n",
    "Train = Train.dropna()\n",
    "Train = Train[~Train.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "for i in range(len(Train[\"question1\"])):\n",
    "    Train[\"question1\"].iloc[i] = remove_punctuation(Train[\"question1\"].iloc[i].lower())\n",
    "    Train[\"question2\"].iloc[i] = remove_punctuation(Train[\"question2\"].iloc[i].lower())\n",
    "\n",
    "Train = Train.dropna()\n",
    "Train = Train[~Train.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "Q1 = Train[\"question1\"].copy()\n",
    "Q2 = Train[\"question2\"].copy()\n",
    "\n",
    "#Tokenization of training set\n",
    "Q1_Tokens = Q1.copy()\n",
    "Q2_Tokens = Q2.copy()\n",
    "for i in range(len(Q1_Tokens)):\n",
    "    Q1_Tokens.iloc[i] = Q1_Tokens.iloc[i].split()\n",
    "    Q2_Tokens.iloc[i] = Q2_Tokens.iloc[i].split()\n",
    "\n",
    "#Embedding of training set\n",
    "Q1_Embedded = Q1_Tokens.copy()\n",
    "Q2_Embedded = Q2_Tokens.copy()\n",
    "for i in range(len(Q1_Embedded)):\n",
    "  Sum_Q1 = np.zeros(300)\n",
    "  Counter_Q1 = 0\n",
    "  for word in Q1_Embedded.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q1 = Sum_Q1 + Embedder[word]\n",
    "      Counter_Q1 = Counter_Q1 + 1\n",
    "  Q1_Embedded.iloc[i] = Sum_Q1 \n",
    "  Sum_Q2 = np.zeros(300)\n",
    "  Counter_Q2 = 0\n",
    "  for word in Q2_Embedded.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q2 = Sum_Q2 + Embedder[word]\n",
    "      Counter_Q2 = Counter_Q2 + 1\n",
    "  Q2_Embedded.iloc[i] = Sum_Q2\n",
    "\n",
    "\n",
    "#Formatting inputs to match model's input format\n",
    "Q1_Inputs = []\n",
    "for i in range(len(Q1_Embedded)):\n",
    "    #print(i)\n",
    "    Q1_Inputs.append(Q1_Embedded.iloc[i].tolist())\n",
    "Q1_Inputs = np.array(Q1_Inputs)\n",
    "\n",
    "Q2_Inputs = []\n",
    "for i in range(len(Q2_Embedded)):\n",
    "    Q2_Inputs.append(Q2_Embedded.iloc[i].tolist())\n",
    "Q2_Inputs = np.array(Q2_Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing of validation set\n",
    "Dev = Dev.dropna()\n",
    "Dev = Dev[~Dev.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "for i in range(len(Dev[\"question1\"])):\n",
    "    Dev[\"question1\"].iloc[i] = remove_punctuation(Dev[\"question1\"].iloc[i].lower())\n",
    "    Dev[\"question2\"].iloc[i] = remove_punctuation(Dev[\"question2\"].iloc[i].lower())\n",
    "\n",
    "Dev = Dev.dropna()\n",
    "Dev = Dev[~Dev.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "Q1_Dev = Dev[\"question1\"].copy()\n",
    "Q2_Dev = Dev[\"question2\"].copy()\n",
    "\n",
    "#Tokenization of validation set\n",
    "Q1_Dev_Tokens = Q1_Dev.copy()\n",
    "Q2_Dev_Tokens = Q2_Dev.copy()\n",
    "for i in range(len(Q1_Dev_Tokens)):\n",
    "    Q1_Dev_Tokens.iloc[i] = Q1_Dev_Tokens.iloc[i].split()\n",
    "    Q2_Dev_Tokens.iloc[i] = Q2_Dev_Tokens.iloc[i].split()\n",
    "\n",
    "#Embedding of validation set\n",
    "from gensim.models import Word2Vec\n",
    "Q1_Embedded_Dev = Q1_Dev_Tokens.copy()\n",
    "Q2_Embedded_Dev = Q2_Dev_Tokens.copy()\n",
    "\n",
    "for i in range(len(Q1_Embedded_Dev)):\n",
    "  Sum_Q1 = np.zeros(300)\n",
    "  for word in Q1_Embedded_Dev.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q1 = Sum_Q1 + (Embedder[word])\n",
    "  Q1_Embedded_Dev.iloc[i] = Sum_Q1 \n",
    "  Sum_Q2 = np.zeros(300)\n",
    "  for word in Q2_Embedded_Dev.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q2 = Sum_Q2 + (Embedder[word])\n",
    "  Q2_Embedded_Dev.iloc[i] = Sum_Q2\n",
    "\n",
    "#Formating embedding to match model's input format\n",
    "\n",
    "Q1_Inputs_Dev = []\n",
    "for i in range(len(Q1_Embedded_Dev)):\n",
    "    Q1_Inputs_Dev.append(Q1_Embedded_Dev.iloc[i].tolist())\n",
    "Q1_Inputs_Dev = np.array(Q1_Inputs_Dev)\n",
    "\n",
    "Q2_Inputs_Dev = []\n",
    "for i in range(len(Q2_Embedded_Dev)):\n",
    "    Q2_Inputs_Dev.append(Q2_Embedded_Dev.iloc[i].tolist())\n",
    "Q2_Inputs_Dev = np.array(Q2_Inputs_Dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXFWucRg_GP2",
    "outputId": "8640c635-9431-40d2-b441-8368c23c75a5"
   },
   "outputs": [],
   "source": [
    "#Pre-processing the test set\n",
    "Test = Test.dropna()\n",
    "Test = Test[~Test.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "for i in range(len(Test[\"question1\"])):\n",
    "    Test[\"question1\"].iloc[i] = remove_punctuation(Test[\"question1\"].iloc[i].lower())\n",
    "    Test[\"question2\"].iloc[i] = remove_punctuation(Test[\"question2\"].iloc[i].lower())\n",
    "\n",
    "Test = Test.dropna()\n",
    "Test = Test[~Test.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "#Tokenization of the test set\n",
    "Q1_Test = Test[\"question1\"].copy()\n",
    "Q2_Test = Test[\"question2\"].copy()\n",
    "\n",
    "Q1_Test_Tokens = Q1_Test.copy()\n",
    "Q2_Test_Tokens = Q2_Test.copy()\n",
    "for i in range(len(Q1_Test_Tokens)):\n",
    "    Q1_Test_Tokens.iloc[i] = Q1_Test_Tokens.iloc[i].split()\n",
    "    Q2_Test_Tokens.iloc[i] = Q2_Test_Tokens.iloc[i].split()\n",
    "\n",
    "#Creating embeddings for the test set\n",
    "from gensim.models import Word2Vec\n",
    "Q1_Embedded_Test = Q1_Test_Tokens.copy()\n",
    "Q2_Embedded_Test = Q2_Test_Tokens.copy()\n",
    "\n",
    "for i in range(len(Q1_Embedded_Test)):\n",
    "  Sum_Q1 = np.zeros(300)\n",
    "  for word in Q1_Embedded_Test.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q1 = Sum_Q1 + (Embedder[word])\n",
    "  Q1_Embedded_Test.iloc[i] = Sum_Q1 \n",
    "  Sum_Q2 = np.zeros(300)\n",
    "  for word in Q2_Embedded_Test.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q2 = Sum_Q2 + (Embedder[word])\n",
    "  Q2_Embedded_Test.iloc[i] = Sum_Q2 \n",
    "\n",
    "#Formatting the inputs to match the inputs of the model\n",
    "Q1_Inputs_Test = []\n",
    "for i in range(len(Q1_Embedded_Test)):\n",
    "    Q1_Inputs_Test.append(Q1_Embedded_Test.iloc[i].tolist())\n",
    "Q1_Inputs_Test = np.array(Q1_Inputs_Test)\n",
    "\n",
    "Q2_Inputs_Test = []\n",
    "for i in range(len(Q2_Embedded_Test)):\n",
    "    Q2_Inputs_Test.append(Q2_Embedded_Test.iloc[i].tolist())\n",
    "Q2_Inputs_Test = np.array(Q2_Inputs_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear up ram as we don't need the embedder anymore\n",
    "Embedder = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "j_ghfDZmZ2wH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "#Initialization of the Siamese network with twin multi-layer perceptron models\n",
    "embedding_dim = 300\n",
    "\n",
    "shared_network = keras.Sequential([\n",
    "    layers.Dense(300, activation='relu', input_shape=(embedding_dim,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=5,           \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "left_input = layers.Input(shape=(embedding_dim,))\n",
    "right_input = layers.Input(shape=(embedding_dim,))\n",
    "\n",
    "encoded_left = shared_network(left_input)\n",
    "encoded_right = shared_network(right_input)\n",
    "\n",
    "distance = Lambda(l1_distance)([encoded_left, encoded_right])\n",
    "similarity_prediction = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "siamese_model = keras.Model(inputs=[left_input, right_input], outputs=similarity_prediction)\n",
    "\n",
    "custom_learning_rate = 0.00007\n",
    "custom_optimizer = Adam(learning_rate=custom_learning_rate)\n",
    "\n",
    "siamese_model.compile(loss='binary_crossentropy', optimizer=custom_optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "n3O4zfgj8P4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "5117/5117 [==============================] - 28s 5ms/step - loss: 0.5810 - accuracy: 0.6721 - val_loss: 0.5239 - val_accuracy: 0.7182\n",
      "Epoch 2/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.5082 - accuracy: 0.7388 - val_loss: 0.4923 - val_accuracy: 0.7562\n",
      "Epoch 3/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.4731 - accuracy: 0.7670 - val_loss: 0.4729 - val_accuracy: 0.7750\n",
      "Epoch 4/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.4424 - accuracy: 0.7885 - val_loss: 0.4619 - val_accuracy: 0.7809\n",
      "Epoch 5/25\n",
      "5117/5117 [==============================] - 28s 6ms/step - loss: 0.4148 - accuracy: 0.8069 - val_loss: 0.4452 - val_accuracy: 0.7912\n",
      "Epoch 6/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.3882 - accuracy: 0.8226 - val_loss: 0.4372 - val_accuracy: 0.7976\n",
      "Epoch 7/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.3632 - accuracy: 0.8381 - val_loss: 0.4365 - val_accuracy: 0.8031\n",
      "Epoch 8/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.3397 - accuracy: 0.8524 - val_loss: 0.4384 - val_accuracy: 0.8081\n",
      "Epoch 9/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.3174 - accuracy: 0.8658 - val_loss: 0.4421 - val_accuracy: 0.8105\n",
      "Epoch 10/25\n",
      "5117/5117 [==============================] - 28s 5ms/step - loss: 0.2974 - accuracy: 0.8773 - val_loss: 0.4446 - val_accuracy: 0.8073\n",
      "Epoch 11/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.2787 - accuracy: 0.8884 - val_loss: 0.4490 - val_accuracy: 0.8182\n",
      "Epoch 12/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.2612 - accuracy: 0.8981 - val_loss: 0.4547 - val_accuracy: 0.8184\n",
      "Epoch 13/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.2451 - accuracy: 0.9077 - val_loss: 0.4744 - val_accuracy: 0.8155\n",
      "Epoch 14/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.2309 - accuracy: 0.9156 - val_loss: 0.4754 - val_accuracy: 0.8140\n",
      "Epoch 15/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.2186 - accuracy: 0.9224 - val_loss: 0.4976 - val_accuracy: 0.8194\n",
      "Epoch 16/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.2064 - accuracy: 0.9291 - val_loss: 0.5036 - val_accuracy: 0.8168\n",
      "Epoch 17/25\n",
      "5117/5117 [==============================] - 30s 6ms/step - loss: 0.1964 - accuracy: 0.9348 - val_loss: 0.5024 - val_accuracy: 0.8158\n",
      "Epoch 18/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.1866 - accuracy: 0.9398 - val_loss: 0.5180 - val_accuracy: 0.8169\n",
      "Epoch 19/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.1783 - accuracy: 0.9438 - val_loss: 0.5367 - val_accuracy: 0.8184\n",
      "Epoch 20/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.1701 - accuracy: 0.9484 - val_loss: 0.5463 - val_accuracy: 0.8211\n",
      "Epoch 21/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.1640 - accuracy: 0.9511 - val_loss: 0.5483 - val_accuracy: 0.8110\n",
      "Epoch 22/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.1576 - accuracy: 0.9543 - val_loss: 0.5537 - val_accuracy: 0.8180\n",
      "Epoch 23/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.1518 - accuracy: 0.9568 - val_loss: 0.5856 - val_accuracy: 0.8193\n",
      "Epoch 24/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.1471 - accuracy: 0.9590 - val_loss: 0.5586 - val_accuracy: 0.8177\n",
      "Epoch 25/25\n",
      "5117/5117 [==============================] - 29s 6ms/step - loss: 0.1422 - accuracy: 0.9614 - val_loss: 0.5791 - val_accuracy: 0.8160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1220b5190>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training of the model\n",
    "\n",
    "siamese_model.fit(\n",
    "    [Q1_Inputs, Q2_Inputs],  # Your question embeddings\n",
    "    np.array(Train[\"is_duplicate\"]),  # Similarity labels (0 for dissimilar, 1 for similar)\n",
    "    batch_size=64,\n",
    "    epochs=25,\n",
    "    validation_data=([Q1_Inputs_Dev, Q2_Inputs_Dev], np.array(Dev[\"is_duplicate\"]))\n",
    "    ,callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "jmWd7lWE8iVn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10233/10233 [==============================] - 13s 1ms/step\n",
      "Training Accuracy: 95.73%\n"
     ]
    }
   ],
   "source": [
    "#Predicting the training set\n",
    "train_predictions = siamese_model.predict([Q1_Inputs, Q2_Inputs])\n",
    "threshold = 0.5\n",
    "train_binary_predictions = (train_predictions > threshold).astype(int)\n",
    "training_accuracy = accuracy_score(Train[\"is_duplicate\"].tolist(), train_binary_predictions)\n",
    "print(f\"Training Accuracy: {training_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 1s 1ms/step\n",
      "Validation Accuracy: 82.11%\n"
     ]
    }
   ],
   "source": [
    "#Predicting the validation set\n",
    "Dev_predictions = siamese_model.predict([Q1_Inputs_Dev, Q2_Inputs_Dev])\n",
    "threshold = 0.5\n",
    "Dev_binary_predictions = (Dev_predictions > threshold).astype(int)\n",
    "Dev_accuracy = accuracy_score(Dev[\"is_duplicate\"].tolist(), Dev_binary_predictions)\n",
    "print(f\"Validation Accuracy: {Dev_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "iBEcm15VdL7_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264/1264 [==============================] - 2s 1ms/step\n",
      "Test Accuracy: 81.73%\n"
     ]
    }
   ],
   "source": [
    "#Predicting the test set\n",
    "test_predictions = siamese_model.predict([Q1_Inputs_Test, Q2_Inputs_Test])\n",
    "threshold = 0.5\n",
    "test_binary_predictions = (test_predictions > threshold).astype(int)\n",
    "test_accuracy = accuracy_score(Test[\"is_duplicate\"].tolist(), test_binary_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id    qid1    qid2  \\\n",
      "244375  330930  457741  373850   \n",
      "294890   24257   45345   45346   \n",
      "22273   299055  421641  421642   \n",
      "21037   173131  188364  267158   \n",
      "13988    25920   48290   48291   \n",
      "...        ...     ...     ...   \n",
      "250396  304563  346385  427836   \n",
      "82798   337217   23547  119932   \n",
      "11534    37539   68294   68295   \n",
      "341097   70696  121784  121785   \n",
      "194027   47789   23696   85293   \n",
      "\n",
      "                                                question1  \\\n",
      "244375  when did the columbia shuttle crew knew someth...   \n",
      "294890  how do we differentiate fact from opinion on q...   \n",
      "22273   what is the difference between a high function...   \n",
      "21037      why do people who cant afford kids make babies   \n",
      "13988   what hotel in ranipuram hillstation would be s...   \n",
      "...                                                   ...   \n",
      "250396       which computer language should i learn first   \n",
      "82798   extraterrestrial life what is the most undenia...   \n",
      "11534   what is the meaning of orthogonality in signal...   \n",
      "341097  what are the advantages of a sales tax over a ...   \n",
      "194027  what is the best book or book series you ever ...   \n",
      "\n",
      "                                                question2  is_duplicate  \n",
      "244375  did the columbia crew in the shuttle know that...             1  \n",
      "294890  how should opinions be distinguished from fact...             1  \n",
      "22273   what are the differences between high and low ...             0  \n",
      "21037   what makes people have kids and no financial s...             1  \n",
      "13988   what hotel in gangtok hillstation would be saf...             0  \n",
      "...                                                   ...           ...  \n",
      "250396            which language should be learned first              0  \n",
      "82798      has anyone seen a genuine ufo what was it like             0  \n",
      "11534                   what is orthogonality of a signal             1  \n",
      "341097  what are the advantages of a vatgst over a sal...             0  \n",
      "194027                   whats a good book series to read             0  \n",
      "\n",
      "[13989 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#Exploratory Data Analysis on training points that we failed to predict correctly\n",
    "\n",
    "Train_Incorrect = []\n",
    "\n",
    "for i in range(len(Train[\"is_duplicate\"])):\n",
    "    if Train[\"is_duplicate\"].iloc[i] != train_binary_predictions[i]:\n",
    "        Train_Incorrect.append(i)\n",
    "\n",
    "Train_Incorrect = Train.iloc[Train_Incorrect,]\n",
    "\n",
    "print(Train_Incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id    qid1    qid2  \\\n",
      "118406  184181   56834  281459   \n",
      "300683  352765  481723   13812   \n",
      "315399  173113  267132  267133   \n",
      "235047  362101  276829  218965   \n",
      "194785  268756  386370  243725   \n",
      "...        ...     ...     ...   \n",
      "76807   152811  123714  240002   \n",
      "307126   98049  163001  163002   \n",
      "338520  262289  378591  378592   \n",
      "111220   67965   63559   36011   \n",
      "176748  146762  231764  231765   \n",
      "\n",
      "                                                question1  \\\n",
      "118406                examples of sole proprietorship the   \n",
      "300683                do kashmiris wants to join pakistan   \n",
      "315399  what do you see as the next big thing in robotics   \n",
      "235047                      whats your best birthday gift   \n",
      "194785  how is vinyl compared to cd in terms of sound ...   \n",
      "...                                                   ...   \n",
      "76807                 what is the reason for sleep apenea   \n",
      "307126  what should someone know before moving to beng...   \n",
      "338520                       how do you improve your mind   \n",
      "111220  what is a suitable solar panel installation pr...   \n",
      "176748                              is quora the best app   \n",
      "\n",
      "                                                question2  is_duplicate  \n",
      "118406               how are sole proprietorships started             0  \n",
      "300683  do the people of kashmir want to join pakistan...             0  \n",
      "315399    what is the next big thing in consumer robotics             1  \n",
      "235047                   what are the best birthday gifts             1  \n",
      "194785                     is vinyl better than cd or mp3             1  \n",
      "...                                                   ...           ...  \n",
      "76807              what are the reasons for lack of sleep             0  \n",
      "307126  what tips would you give to someone who is mov...             1  \n",
      "338520                          how can i improve my mind             1  \n",
      "111220  what is a suitable solar panel installation pr...             0  \n",
      "176748            do you think quora is the best app ever             0  \n",
      "\n",
      "[6508 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#Exploratory Data Analysis on validation points that we failed to predict correctly\n",
    "\n",
    "Dev_Incorrect = []\n",
    "\n",
    "for i in range(len(Dev[\"is_duplicate\"])):\n",
    "    if Dev[\"is_duplicate\"].iloc[i] != Dev_binary_predictions[i]:\n",
    "        Dev_Incorrect.append(i)\n",
    "\n",
    "Dev_Incorrect = Dev.iloc[Dev_Incorrect,]\n",
    "\n",
    "print(Dev_Incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id    qid1    qid2  \\\n",
      "2      172974  266948  175089   \n",
      "3       15329   29298   29299   \n",
      "6      144933   10927  229257   \n",
      "8      382732  443182  514615   \n",
      "19     308638  432498  432499   \n",
      "...       ...     ...     ...   \n",
      "40406  255959  370959  370960   \n",
      "40419  214998  320750  320751   \n",
      "40420  144862  229155  229156   \n",
      "40423  146181  230973  230974   \n",
      "40426  258093  373551  373552   \n",
      "\n",
      "                                               question1  \\\n",
      "2           is there a reason why we should travel alone   \n",
      "3      why are people so obsessed with having a girlf...   \n",
      "6      what does a good answer on quora look like wha...   \n",
      "8                  why is my life getting so complicated   \n",
      "19     is it a bad time to buy a condo or a house in ...   \n",
      "...                                                  ...   \n",
      "40406  what type of spices go great with mashed potatoes   \n",
      "40419        how can i really slow down my aging process   \n",
      "40420                       do women cheat more than men   \n",
      "40423    which are the best indian restaurants in london   \n",
      "40426   why dont i feel uncomfortable making eye contact   \n",
      "\n",
      "                                               question2  is_duplicate  \n",
      "2                  what are some reasons to travel alone             1  \n",
      "3                     how can a single male have a child             0  \n",
      "6                how do you write a good answer on quora             1  \n",
      "8                          why is my life so complicated             0  \n",
      "19     would 2017 be a good time to buy a house in ba...             1  \n",
      "...                                                  ...           ...  \n",
      "40406  what are some good spices to add to a mashed p...             1  \n",
      "40419             how can i slow down the signs of aging             1  \n",
      "40420                       do more women cheat than men             0  \n",
      "40423  how do i find the best indian restaurant in lo...             1  \n",
      "40426  why am i feeling uncomfortable to make eye con...             1  \n",
      "\n",
      "[7385 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#Exploratory Data Analysis on test points that we failed to predict correctly\n",
    "\n",
    "Test_Incorrect = []\n",
    "\n",
    "for i in range(len(Test[\"is_duplicate\"])):\n",
    "    if Test[\"is_duplicate\"].iloc[i] != test_binary_predictions[i]:\n",
    "        Test_Incorrect.append(i)\n",
    "\n",
    "Test_Incorrect = Test.iloc[Test_Incorrect,]\n",
    "\n",
    "print(Test_Incorrect)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
