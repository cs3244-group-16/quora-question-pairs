{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iq4OsDIuBVcG",
    "outputId": "9eb104b1-d08e-4786-b1f3-0b2caa1f84d0"
   },
   "outputs": [],
   "source": [
    "#Importing of relavent functions, pre-trained models, and libraries\n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_without_punctuation = text.translate(translator)\n",
    "    return text_without_punctuation\n",
    "\n",
    "def l1_distance(vectors):\n",
    "    x, y = vectors\n",
    "    return tf.reduce_sum(tf.abs(x - y), axis=1, keepdims=True)\n",
    "\n",
    "import gensim.downloader\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Lambda, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Embedder = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "87a5w7kEz6sP"
   },
   "outputs": [],
   "source": [
    "#Reading in the training and test sets\n",
    "Master = pd.read_csv(\"QQP/train.tsv\",sep=\"\\t\")\n",
    "Train,Dev = train_test_split(Master,test_size=0.1,random_state=42)\n",
    "Test = pd.read_csv(\"QQP/dev.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlh7c1lr0Obv",
    "outputId": "21236e4f-ac0a-4c79-a651-dce816353661"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing of training set\n",
    "\n",
    "Train = Train.dropna()\n",
    "Train = Train[~Train.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "for i in range(len(Train[\"question1\"])):\n",
    "    Train[\"question1\"].iloc[i] = remove_punctuation(Train[\"question1\"].iloc[i].lower())\n",
    "    Train[\"question2\"].iloc[i] = remove_punctuation(Train[\"question2\"].iloc[i].lower())\n",
    "\n",
    "Train = Train.dropna()\n",
    "Train = Train[~Train.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "Q1 = Train[\"question1\"].copy()\n",
    "Q2 = Train[\"question2\"].copy()\n",
    "\n",
    "#Tokenization of training set\n",
    "Q1_Tokens = Q1.copy()\n",
    "Q2_Tokens = Q2.copy()\n",
    "for i in range(len(Q1_Tokens)):\n",
    "    Q1_Tokens.iloc[i] = Q1_Tokens.iloc[i].split()\n",
    "    Q2_Tokens.iloc[i] = Q2_Tokens.iloc[i].split()\n",
    "\n",
    "#Embedding of training set\n",
    "Q1_Embedded = Q1_Tokens.copy()\n",
    "Q2_Embedded = Q2_Tokens.copy()\n",
    "for i in range(len(Q1_Embedded)):\n",
    "  Sum_Q1 = np.zeros(300)\n",
    "  Counter_Q1 = 0\n",
    "  for word in Q1_Embedded.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q1 = Sum_Q1 + Embedder[word]\n",
    "      Counter_Q1 = Counter_Q1 + 1\n",
    "  Q1_Embedded.iloc[i] = Sum_Q1 \n",
    "  Sum_Q2 = np.zeros(300)\n",
    "  Counter_Q2 = 0\n",
    "  for word in Q2_Embedded.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q2 = Sum_Q2 + Embedder[word]\n",
    "      Counter_Q2 = Counter_Q2 + 1\n",
    "  Q2_Embedded.iloc[i] = Sum_Q2\n",
    "\n",
    "\n",
    "#Formatting inputs to match model's input format\n",
    "Q1_Inputs = []\n",
    "for i in range(len(Q1_Embedded)):\n",
    "    #print(i)\n",
    "    Q1_Inputs.append(Q1_Embedded.iloc[i].tolist())\n",
    "Q1_Inputs = np.array(Q1_Inputs)\n",
    "\n",
    "Q2_Inputs = []\n",
    "for i in range(len(Q2_Embedded)):\n",
    "    Q2_Inputs.append(Q2_Embedded.iloc[i].tolist())\n",
    "Q2_Inputs = np.array(Q2_Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing of validation set\n",
    "Dev = Dev.dropna()\n",
    "Dev = Dev[~Dev.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "for i in range(len(Dev[\"question1\"])):\n",
    "    Dev[\"question1\"].iloc[i] = remove_punctuation(Dev[\"question1\"].iloc[i].lower())\n",
    "    Dev[\"question2\"].iloc[i] = remove_punctuation(Dev[\"question2\"].iloc[i].lower())\n",
    "\n",
    "Dev = Dev.dropna()\n",
    "Dev = Dev[~Dev.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "Q1_Dev = Dev[\"question1\"].copy()\n",
    "Q2_Dev = Dev[\"question2\"].copy()\n",
    "\n",
    "#Tokenization of validation set\n",
    "Q1_Dev_Tokens = Q1_Dev.copy()\n",
    "Q2_Dev_Tokens = Q2_Dev.copy()\n",
    "for i in range(len(Q1_Dev_Tokens)):\n",
    "    Q1_Dev_Tokens.iloc[i] = Q1_Dev_Tokens.iloc[i].split()\n",
    "    Q2_Dev_Tokens.iloc[i] = Q2_Dev_Tokens.iloc[i].split()\n",
    "\n",
    "#Embedding of validation set\n",
    "from gensim.models import Word2Vec\n",
    "Q1_Embedded_Dev = Q1_Dev_Tokens.copy()\n",
    "Q2_Embedded_Dev = Q2_Dev_Tokens.copy()\n",
    "\n",
    "for i in range(len(Q1_Embedded_Dev)):\n",
    "  Sum_Q1 = np.zeros(300)\n",
    "  for word in Q1_Embedded_Dev.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q1 = Sum_Q1 + (Embedder[word])\n",
    "  Q1_Embedded_Dev.iloc[i] = Sum_Q1 \n",
    "  Sum_Q2 = np.zeros(300)\n",
    "  for word in Q2_Embedded_Dev.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q2 = Sum_Q2 + (Embedder[word])\n",
    "  Q2_Embedded_Dev.iloc[i] = Sum_Q2\n",
    "\n",
    "#Formating embedding to match model's input format\n",
    "\n",
    "Q1_Inputs_Dev = []\n",
    "for i in range(len(Q1_Embedded_Dev)):\n",
    "    Q1_Inputs_Dev.append(Q1_Embedded_Dev.iloc[i].tolist())\n",
    "Q1_Inputs_Dev = np.array(Q1_Inputs_Dev)\n",
    "\n",
    "Q2_Inputs_Dev = []\n",
    "for i in range(len(Q2_Embedded_Dev)):\n",
    "    Q2_Inputs_Dev.append(Q2_Embedded_Dev.iloc[i].tolist())\n",
    "Q2_Inputs_Dev = np.array(Q2_Inputs_Dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXFWucRg_GP2",
    "outputId": "8640c635-9431-40d2-b441-8368c23c75a5"
   },
   "outputs": [],
   "source": [
    "#Pre-processing the test set\n",
    "Test = Test.dropna()\n",
    "Test = Test[~Test.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "for i in range(len(Test[\"question1\"])):\n",
    "    Test[\"question1\"].iloc[i] = remove_punctuation(Test[\"question1\"].iloc[i].lower())\n",
    "    Test[\"question2\"].iloc[i] = remove_punctuation(Test[\"question2\"].iloc[i].lower())\n",
    "\n",
    "Test = Test.dropna()\n",
    "Test = Test[~Test.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "#Tokenization of the test set\n",
    "Q1_Test = Test[\"question1\"].copy()\n",
    "Q2_Test = Test[\"question2\"].copy()\n",
    "\n",
    "Q1_Test_Tokens = Q1_Test.copy()\n",
    "Q2_Test_Tokens = Q2_Test.copy()\n",
    "for i in range(len(Q1_Test_Tokens)):\n",
    "    Q1_Test_Tokens.iloc[i] = Q1_Test_Tokens.iloc[i].split()\n",
    "    Q2_Test_Tokens.iloc[i] = Q2_Test_Tokens.iloc[i].split()\n",
    "\n",
    "#Creating embeddings for the test set\n",
    "from gensim.models import Word2Vec\n",
    "Q1_Embedded_Test = Q1_Test_Tokens.copy()\n",
    "Q2_Embedded_Test = Q2_Test_Tokens.copy()\n",
    "\n",
    "for i in range(len(Q1_Embedded_Test)):\n",
    "  Sum_Q1 = np.zeros(300)\n",
    "  for word in Q1_Embedded_Test.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q1 = Sum_Q1 + (Embedder[word])\n",
    "  Q1_Embedded_Test.iloc[i] = Sum_Q1 \n",
    "  Sum_Q2 = np.zeros(300)\n",
    "  for word in Q2_Embedded_Test.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q2 = Sum_Q2 + (Embedder[word])\n",
    "  Q2_Embedded_Test.iloc[i] = Sum_Q2 \n",
    "\n",
    "#Formatting the inputs to match the inputs of the model\n",
    "Q1_Inputs_Test = []\n",
    "for i in range(len(Q1_Embedded_Test)):\n",
    "    Q1_Inputs_Test.append(Q1_Embedded_Test.iloc[i].tolist())\n",
    "Q1_Inputs_Test = np.array(Q1_Inputs_Test)\n",
    "\n",
    "Q2_Inputs_Test = []\n",
    "for i in range(len(Q2_Embedded_Test)):\n",
    "    Q2_Inputs_Test.append(Q2_Embedded_Test.iloc[i].tolist())\n",
    "Q2_Inputs_Test = np.array(Q2_Inputs_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear up ram as we don't need the embedder anymore\n",
    "Embedder = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "j_ghfDZmZ2wH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "#Initialization of the Siamese network with twin multi-layer perceptron models\n",
    "embedding_dim = 300\n",
    "\n",
    "shared_network = keras.Sequential([\n",
    "    layers.Dense(300, activation='relu', input_shape=(embedding_dim,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5,           \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "left_input = layers.Input(shape=(embedding_dim,))\n",
    "right_input = layers.Input(shape=(embedding_dim,))\n",
    "\n",
    "encoded_left = shared_network(left_input)\n",
    "encoded_right = shared_network(right_input)\n",
    "\n",
    "distance = Lambda(l1_distance)([encoded_left, encoded_right])\n",
    "similarity_prediction = Dense(1, activation='sigmoid')(distance)\n",
    "\n",
    "siamese_model = keras.Model(inputs=[left_input, right_input], outputs=similarity_prediction)\n",
    "\n",
    "custom_learning_rate = 0.0001\n",
    "custom_optimizer = Adam(learning_rate=custom_learning_rate)\n",
    "\n",
    "siamese_model.compile(loss='mean_squared_error', optimizer=custom_optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbyJQCvg79Vt"
   },
   "source": [
    "Initialize the Siamese network architecture with the above model as the underlying architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_huKvpYt8Ted"
   },
   "source": [
    "Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n3O4zfgj8P4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "4094/4094 [==============================] - 26s 6ms/step - loss: 0.1880 - accuracy: 0.6966 - val_loss: 0.1760 - val_accuracy: 0.7347\n",
      "Epoch 2/25\n",
      "4094/4094 [==============================] - 24s 6ms/step - loss: 0.1664 - accuracy: 0.7551 - val_loss: 0.1653 - val_accuracy: 0.7632\n",
      "Epoch 3/25\n",
      "4094/4094 [==============================] - 25s 6ms/step - loss: 0.1515 - accuracy: 0.7839 - val_loss: 0.1556 - val_accuracy: 0.7772\n",
      "Epoch 4/25\n",
      "4094/4094 [==============================] - 25s 6ms/step - loss: 0.1390 - accuracy: 0.8066 - val_loss: 0.1488 - val_accuracy: 0.7871\n",
      "Epoch 5/25\n",
      "4094/4094 [==============================] - 25s 6ms/step - loss: 0.1276 - accuracy: 0.8264 - val_loss: 0.1438 - val_accuracy: 0.7958\n",
      "Epoch 6/25\n",
      "4094/4094 [==============================] - 25s 6ms/step - loss: 0.1171 - accuracy: 0.8452 - val_loss: 0.1438 - val_accuracy: 0.7964\n",
      "Epoch 7/25\n",
      "4094/4094 [==============================] - 27s 7ms/step - loss: 0.1070 - accuracy: 0.8621 - val_loss: 0.1411 - val_accuracy: 0.8004\n",
      "Epoch 8/25\n",
      "4094/4094 [==============================] - 27s 7ms/step - loss: 0.0981 - accuracy: 0.8767 - val_loss: 0.1406 - val_accuracy: 0.8006\n",
      "Epoch 9/25\n",
      "4094/4094 [==============================] - 28s 7ms/step - loss: 0.0894 - accuracy: 0.8909 - val_loss: 0.1395 - val_accuracy: 0.8061\n",
      "Epoch 10/25\n",
      "4094/4094 [==============================] - 27s 7ms/step - loss: 0.0817 - accuracy: 0.9028 - val_loss: 0.1386 - val_accuracy: 0.8066\n",
      "Epoch 11/25\n",
      "4094/4094 [==============================] - 26s 6ms/step - loss: 0.0747 - accuracy: 0.9130 - val_loss: 0.1410 - val_accuracy: 0.8043\n",
      "Epoch 12/25\n",
      "4094/4094 [==============================] - 26s 6ms/step - loss: 0.0686 - accuracy: 0.9227 - val_loss: 0.1409 - val_accuracy: 0.8060\n",
      "Epoch 13/25\n",
      "4094/4094 [==============================] - 27s 7ms/step - loss: 0.0629 - accuracy: 0.9306 - val_loss: 0.1408 - val_accuracy: 0.8088\n",
      "Epoch 14/25\n",
      "4094/4094 [==============================] - 27s 7ms/step - loss: 0.0581 - accuracy: 0.9372 - val_loss: 0.1414 - val_accuracy: 0.8116\n",
      "Epoch 15/25\n",
      "4094/4094 [==============================] - 27s 7ms/step - loss: 0.0537 - accuracy: 0.9433 - val_loss: 0.1434 - val_accuracy: 0.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x43801ad90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training of the model\n",
    "\n",
    "siamese_model.fit(\n",
    "    [Q1_Inputs, Q2_Inputs],  # Your question embeddings\n",
    "    np.array(Train[\"is_duplicate\"]),  # Similarity labels (0 for dissimilar, 1 for similar)\n",
    "    batch_size=64,\n",
    "    epochs=25,\n",
    "    validation_split=0.2  # You can adjust the validation split\n",
    "    ,callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jmWd7lWE8iVn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10233/10233 [==============================] - 19s 2ms/step\n",
      "Training Accuracy: 89.94%\n"
     ]
    }
   ],
   "source": [
    "#Predicting the training set\n",
    "train_predictions = siamese_model.predict([Q1_Inputs, Q2_Inputs])\n",
    "threshold = 0.5\n",
    "train_binary_predictions = (train_predictions > threshold).astype(int)\n",
    "training_accuracy = accuracy_score(Train[\"is_duplicate\"].tolist(), train_binary_predictions)\n",
    "print(f\"Training Accuracy: {training_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 2s 2ms/step\n",
      "Validation Accuracy: 81.15%\n"
     ]
    }
   ],
   "source": [
    "#Predicting the validation set\n",
    "Dev_predictions = siamese_model.predict([Q1_Inputs_Dev, Q2_Inputs_Dev])\n",
    "threshold = 0.5\n",
    "Dev_binary_predictions = (Dev_predictions > threshold).astype(int)\n",
    "Dev_accuracy = accuracy_score(Dev[\"is_duplicate\"].tolist(), Dev_binary_predictions)\n",
    "print(f\"Validation Accuracy: {Dev_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iBEcm15VdL7_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264/1264 [==============================] - 2s 2ms/step\n",
      "Test Accuracy: 80.68%\n"
     ]
    }
   ],
   "source": [
    "#Predicting the test set\n",
    "test_predictions = siamese_model.predict([Q1_Inputs_Test, Q2_Inputs_Test])\n",
    "threshold = 0.5\n",
    "test_binary_predictions = (test_predictions > threshold).astype(int)\n",
    "test_accuracy = accuracy_score(Test[\"is_duplicate\"].tolist(), test_binary_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
