{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Read in the training set"
      ],
      "metadata": {
        "id": "l_9DZGRf0iDu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87a5w7kEz6sP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "Train = pd.read_csv(\"/content/train.csv\")\n",
        "#Use this line when we just want to train on a subset of the training set.\n",
        "Train = Train.iloc[1:4000,]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Processing of the training set"
      ],
      "metadata": {
        "id": "eXlEqMjX0m0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Lowercase all questions and remove punctuation\n",
        "\n",
        "#Remark: We need to remove puntations before removing empty columns, because some questions only contains puntuations and should be considered missing values. However, they will not be dropped if the puntutations are present when we executed the drop command\n",
        "\n",
        "##To remove punctuation: We will use a code provided by CHATGPT\n",
        "\n",
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Create a translation table to remove punctuation characters\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    # Use the translation table to remove punctuation\n",
        "    text_without_punctuation = text.translate(translator)\n",
        "\n",
        "    return text_without_punctuation\n",
        "\n",
        "#For each question: Set all to lowercase then remove puntuation, and replace it in the original Train dataframe\n",
        "\n",
        "for i in range(len(Train[\"question1\"])):\n",
        "    Train[\"question1\"].iloc[i] = remove_punctuation(Train[\"question1\"].iloc[i].lower())\n",
        "    Train[\"question2\"].iloc[i] = remove_punctuation(Train[\"question2\"].iloc[i].lower())\n",
        "\n",
        "#2. Remove rows with missing data\n",
        "Train = Train.dropna()\n",
        "Train = Train[~Train.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
        "\n",
        "#2. Extract the questions\n",
        "##Extract the first questions and store it in a variable\n",
        "Q1 = Train[\"question1\"].copy()\n",
        "##Extract the second questions and store it in a variable\n",
        "Q2 = Train[\"question2\"].copy()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zlh7c1lr0Obv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9315a5-317b-4570-883a-9a843369b1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-77-6d5d5008e031>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Train[\"question1\"].iloc[i] = remove_punctuation(Train[\"question1\"].iloc[i].lower())\n",
            "<ipython-input-77-6d5d5008e031>:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  Train[\"question2\"].iloc[i] = remove_punctuation(Train[\"question2\"].iloc[i].lower())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the questions in each question pair"
      ],
      "metadata": {
        "id": "ITeq3_S81Sde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To tokenize the questions, we will use a pre-trained tokenizer model from the spaCy library\n",
        "import spacy\n",
        "#From the spacy library: Import a pre-trained tokenizer\n",
        "Tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "#Extract the tokens from each of the questions\n",
        "\n",
        "##Create a copy of each of the questions\n",
        "Q1_Tokens = Q1.copy()\n",
        "Q2_Tokens = Q2.copy()\n",
        "#For each question:\n",
        "for i in range(len(Q1_Tokens)):\n",
        "    #Tokenize each question and overwrite each question with its token\n",
        "    Q1_Tokens.iloc[i] = [token.text for token in Tokenizer(Q1_Tokens.iloc[i])]\n",
        "    Q2_Tokens.iloc[i] = [token.text for token in Tokenizer(Q2_Tokens.iloc[i])]"
      ],
      "metadata": {
        "id": "JFbFcJts1PyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embeddings using Word2Vec"
      ],
      "metadata": {
        "id": "n-NE5Nm85-rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "Q1_Embedded = Q1_Tokens.copy()\n",
        "Q2_Embedded = Q2_Tokens.copy()\n",
        "#For each question: Embed each word using Word2Vec\n",
        "for i in range(len(Q1_Embedded)):\n",
        "    #print(i)\n",
        "    Q1_Embedded.iloc[i] = Word2Vec([Q1_Embedded.iloc[i]],vector_size=100, window=5, min_count=1, sg=0)\n",
        "    Q2_Embedded.iloc[i] = Word2Vec([Q2_Embedded.iloc[i]],vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "#Aggregate each question to get a vector representation of each question, using the average method\n",
        "\n",
        "#For each question pair\n",
        "for i in range(len(Q1_Embedded)):\n",
        "\n",
        "    #For each question: Extract the vector representation for each word and average them\n",
        "\n",
        "    #Create a tracker that will store the running sum of the word's vectors in the first question in this pair\n",
        "    Sum_Q1 = 0\n",
        "    #For each word in the first question in this pair\n",
        "    for word in Q1_Tokens.iloc[i]:\n",
        "        #Extract the vector and add it to the running sum\n",
        "        Sum_Q1 = Sum_Q1 + Q1_Embedded.iloc[i].wv[word]\n",
        "    #Find the average of these sums ~ This is the vector representation of the first question in this pair\n",
        "    Sum_Q1 = Sum_Q1/len(Q1_Tokens)\n",
        "    #Overwrite the list with this new vector\n",
        "    Q1_Embedded.iloc[i] = Sum_Q1\n",
        "    #Create a tracker that will store the running sum of the word's vectors in the second question in this pair\n",
        "    Sum_Q2 = 0\n",
        "    #For each word in the second question in this pair\n",
        "    for word in Q2_Tokens.iloc[i]:\n",
        "        #Extract the vector and add it to the running sum\n",
        "        Sum_Q2 = Sum_Q2 + Q2_Embedded.iloc[i].wv[word]\n",
        "    #Find the average of these sums ~ This is the vector representation of the second question in this pair\n",
        "    Sum_Q2 = Sum_Q2/len(Q2_Tokens)\n",
        "    #Overwrite the list wih this vector\n",
        "    Q2_Embedded.iloc[i] = Sum_Q2\n"
      ],
      "metadata": {
        "id": "i4OMtob26Bk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of what is the underlying model we will use in the Siamese architecture\n"
      ],
      "metadata": {
        "id": "o30Dhd4fZ_DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a multi-layer perceptron neural network that will be used as the underlying architecture\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.losses import cosine_similarity\n",
        "\n",
        "#Identify the length of the vectors used in the embeddings\n",
        "embedding_dim = 100\n",
        "\n",
        "#Identify the number of questions. This will indicate how many nodes we will use\n",
        "Questions_Count = Q2_Embedded.shape[0]\n",
        "\n",
        "#This indicates that we will be using 4 layers in our underlying architecture. ~ Can be increased/decreased\n",
        "shared_network = keras.Sequential([\n",
        "    layers.Dense(Questions_Count, activation='relu', input_shape=(embedding_dim,)),\n",
        "    layers.Dense(Questions_Count, activation='relu'),\n",
        "    layers.Dense(Questions_Count, activation='relu'),\n",
        "    layers.Dense(Questions_Count, activation='relu'),\n",
        "    layers.Dense(Questions_Count, activation='relu'),\n",
        "    layers.Dense(Questions_Count, activation='relu'),\n",
        "    layers.Dense(Questions_Count, activation='relu'),\n",
        "    layers.Dense(Questions_Count, activation='relu'),\n",
        "\n",
        "])\n"
      ],
      "metadata": {
        "id": "j_ghfDZmZ2wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the Siamese network architecture with the above model as the underlying architecture"
      ],
      "metadata": {
        "id": "DbyJQCvg79Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the Siamese network architecture\n",
        "\n",
        "#Define the left and right inputs for the question pair ~ Initializing how long the vectors that represent each question is\n",
        "left_input = layers.Input(shape=(embedding_dim,))\n",
        "right_input = layers.Input(shape=(embedding_dim,))\n",
        "\n",
        "# Encode the question pair using the shared network ~ Indicate that we will input the questions both question in the pair into the shared network\n",
        "encoded_left = shared_network(left_input)\n",
        "encoded_right = shared_network(right_input)\n",
        "\n",
        "# Calculate the Euclidean distance between the encodings ~ I.e: We will use this distance function to determine the similarity between the outputs\n",
        "distance = layers.Lambda(lambda x: tf.norm(x[0] - x[1], axis=1, keepdims=True))([encoded_left, encoded_right])\n",
        "#distance=layers.Lambda(lambda x: cosine_similarity(x[0], x[1]))([encoded_left, encoded_right])\n",
        "\n",
        "# We have initialized the model. Now, just create the Siamese model\n",
        "siamese_model = keras.Model(inputs=[left_input, right_input], outputs=distance)\n",
        "\n",
        "\n",
        "#Compile the model ~ Specifies how the training should be done etc.....\n",
        "siamese_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#siamese_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "isJFhYUF8HnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of the model"
      ],
      "metadata": {
        "id": "_huKvpYt8Ted"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inputs has to be an array of a list of lists -> Convert from array of list to a list\n",
        "\n",
        "Q1_Inputs = []\n",
        "\n",
        "for i in range(len(Q1_Embedded)):\n",
        "    Q1_Inputs.append(Q1_Embedded.iloc[i].tolist())\n",
        "\n",
        "Q1_Inputs = np.array(Q1_Inputs)\n",
        "\n",
        "Q2_Inputs = []\n",
        "\n",
        "for i in range(len(Q2_Embedded)):\n",
        "    Q2_Inputs.append(Q2_Embedded.iloc[i].tolist())\n",
        "\n",
        "Q2_Inputs = np.array(Q2_Inputs)\n",
        "\n",
        "\n",
        "#Training of the model\n",
        "\n",
        "siamese_model.fit(\n",
        "    [Q1_Inputs, Q2_Inputs],  # Your question embeddings\n",
        "    np.array(Train[\"is_duplicate\"]),  # Similarity labels (0 for dissimilar, 1 for similar)\n",
        "    batch_size=32,\n",
        "    epochs=10,\n",
        "    validation_split=0.2  # You can adjust the validation split\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "n3O4zfgj8P4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2be9e77-2c43-4b5c-c8a7-0a3a6d143854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 7s 45ms/step - loss: 9.1355 - accuracy: 0.3896 - val_loss: 9.3786 - val_accuracy: 0.3825\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 4s 41ms/step - loss: 9.3595 - accuracy: 0.3862 - val_loss: 9.3772 - val_accuracy: 0.3825\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 4s 41ms/step - loss: 9.3555 - accuracy: 0.3862 - val_loss: 9.3759 - val_accuracy: 0.3850\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 4s 42ms/step - loss: 9.3376 - accuracy: 0.3871 - val_loss: 9.3951 - val_accuracy: 0.3825\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 4s 41ms/step - loss: 9.3550 - accuracy: 0.3865 - val_loss: 9.3755 - val_accuracy: 0.3850\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 4s 41ms/step - loss: 7.5926 - accuracy: 0.4312 - val_loss: 2.3063 - val_accuracy: 0.4950\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 4s 42ms/step - loss: 2.2708 - accuracy: 0.5735 - val_loss: 2.0247 - val_accuracy: 0.6012\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 4s 41ms/step - loss: 2.0214 - accuracy: 0.5822 - val_loss: 1.9764 - val_accuracy: 0.6150\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 4s 41ms/step - loss: 1.9549 - accuracy: 0.5819 - val_loss: 1.9108 - val_accuracy: 0.5925\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 2.0892 - accuracy: 0.5782 - val_loss: 1.9927 - val_accuracy: 0.5425\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ab470203970>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the fitted values/Predicting the training points"
      ],
      "metadata": {
        "id": "zQzBoaQM8dxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Results = (siamese_model.predict([Q1_Inputs, Q2_Inputs]))\n",
        "Prediction = []\n",
        "for i in range(len(Results)):\n",
        "    if Results[i]>0.5:\n",
        "        Prediction.append(1)\n",
        "    else:\n",
        "        Prediction.append(0)\n",
        "\n",
        "#Calculating training accuracy:\n",
        "\n",
        "Score = 0\n",
        "\n",
        "for i in range(len(Prediction)):\n",
        "    if Prediction[i] == Train[\"is_duplicate\"].tolist()[i]:\n",
        "        Score = Score + 1\n",
        "\n",
        "print(\"Training Accuracy is\")\n",
        "print(Score/len(Prediction))"
      ],
      "metadata": {
        "id": "jmWd7lWE8iVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b509fd9-4be0-41ca-b932-f2f2e86576d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125/125 [==============================] - 0s 3ms/step\n",
            "Training Accuracy is\n",
            "0.5932966483241621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(Prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF2GmmRFhxdg",
        "outputId": "1220d204-3846-4427-d6b8-0066ba02b62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "502\n"
          ]
        }
      ]
    }
  ]
}