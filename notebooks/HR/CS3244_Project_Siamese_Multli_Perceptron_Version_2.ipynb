{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_9DZGRf0iDu"
   },
   "source": [
    "Read in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "87a5w7kEz6sP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Use this line of code for the clean set\n",
    "Train = pd.read_csv(\"QQP/train.tsv\",sep=\"\\t\")\n",
    "#Use thie line of code for the original set\n",
    "#Train = pd.read_csv(\"QQP/train.tsv\")\n",
    "#Use this line when we just want to train on a subset of the training set.\n",
    "#Train = Train.iloc[1:10000,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXlEqMjX0m0U"
   },
   "source": [
    "Pre-Processing of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlh7c1lr0Obv",
    "outputId": "21236e4f-ac0a-4c79-a651-dce816353661"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "#1. Lowercase all questions and remove punctuation\n",
    "\n",
    "#Remark: We need to remove puntations before removing empty columns, because some questions only contains puntuations and should be considered missing values. However, they will not be dropped if the puntutations are present when we executed the drop command\n",
    "\n",
    "##To remove punctuation: We will use a code provided by CHATGPT\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Create a translation table to remove punctuation characters\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    # Use the translation table to remove punctuation\n",
    "    text_without_punctuation = text.translate(translator)\n",
    "\n",
    "    return text_without_punctuation\n",
    "\n",
    "Train = Train.dropna()\n",
    "Train = Train[~Train.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "#For each question: Set all to lowercase then remove puntuation, and replace it in the original Train dataframe\n",
    "\n",
    "for i in range(len(Train[\"question1\"])):\n",
    "    #print(i)\n",
    "    Train[\"question1\"].iloc[i] = remove_punctuation(Train[\"question1\"].iloc[i].lower())\n",
    "    Train[\"question2\"].iloc[i] = remove_punctuation(Train[\"question2\"].iloc[i].lower())\n",
    "\n",
    "#2. Remove rows with missing data\n",
    "Train = Train.dropna()\n",
    "Train = Train[~Train.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "#2. Extract the questions\n",
    "##Extract the first questions and store it in a variable\n",
    "Q1 = Train[\"question1\"].copy()\n",
    "##Extract the second questions and store it in a variable\n",
    "Q2 = Train[\"question2\"].copy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how do i create a successful app'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train[\"question2\"].iloc[105780]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITeq3_S81Sde"
   },
   "source": [
    "Tokenize the questions in each question pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JFbFcJts1PyE"
   },
   "outputs": [],
   "source": [
    "##Create a copy of each of the questions\n",
    "Q1_Tokens = Q1.copy()\n",
    "Q2_Tokens = Q2.copy()\n",
    "#For each question:\n",
    "for i in range(len(Q1_Tokens)):\n",
    "    #Tokenize each question and overwrite each question with its token\n",
    "    Q1_Tokens.iloc[i] = Q1_Tokens.iloc[i].split()\n",
    "    Q2_Tokens.iloc[i] = Q2_Tokens.iloc[i].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-NE5Nm85-rk"
   },
   "source": [
    "Create embeddings using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iq4OsDIuBVcG",
    "outputId": "9eb104b1-d08e-4786-b1f3-0b2caa1f84d0"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "Embedder = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eVd8aif_Hnc"
   },
   "source": [
    "Create embeddings for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i4OMtob26Bk8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "Q1_Embedded = Q1_Tokens.copy()\n",
    "Q2_Embedded = Q2_Tokens.copy()\n",
    "#For each question:\n",
    "for i in range(len(Q1_Embedded)):\n",
    "  Sum_Q1 = np.zeros(300)\n",
    "  #For each word in the ith question:\n",
    "  for word in Q1_Embedded.iloc[i]:\n",
    "    #Check if the word is in our pre-trained model\n",
    "    if word in Embedder:\n",
    "      #Find the embedding and add it to the running sum\n",
    "      Sum_Q1 = Sum_Q1 + Embedder[\"word\"]\n",
    "  #After we have summed up the vectors for all the words in the ith questions, take the average and store the resultant vector\n",
    "  Q1_Embedded.iloc[i] = Sum_Q1 / len(Q1_Tokens.iloc[i])\n",
    "  #Repeat the same for the other question in this pair\n",
    "  Sum_Q2 = np.zeros(300)\n",
    "  for word in Q2_Embedded.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      Sum_Q2 = Sum_Q2 + Embedder[\"word\"]\n",
    "  Q2_Embedded.iloc[i] = Sum_Q2 / len(Q2_Tokens.iloc[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MEE6oH3_Lpt"
   },
   "source": [
    "Create embeddings for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXFWucRg_GP2",
    "outputId": "8640c635-9431-40d2-b441-8368c23c75a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "#Test = pd.read_csv(\"QQP/dev.tsv\")\n",
    "Test = pd.read_csv(\"QQP/dev.tsv\",sep=\"\\t\")\n",
    "Test = Test.dropna()\n",
    "Test = Test[~Test.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "for i in range(len(Test[\"question1\"])):\n",
    "    Test[\"question1\"].iloc[i] = remove_punctuation(Test[\"question1\"].iloc[i].lower())\n",
    "    Test[\"question2\"].iloc[i] = remove_punctuation(Test[\"question2\"].iloc[i].lower())\n",
    "\n",
    "#2. Remove rows with missing data\n",
    "\n",
    "#2. Remove rows with missing data\n",
    "Test = Test.dropna()\n",
    "Test = Test[~Test.apply(lambda row: row.str.strip().str.len().eq(0).any(), axis=1)]\n",
    "\n",
    "#2. Extract the questions\n",
    "##Extract the first questions and store it in a variable\n",
    "Q1_Test = Test[\"question1\"].copy()\n",
    "##Extract the second questions and store it in a variable\n",
    "Q2_Test = Test[\"question2\"].copy()\n",
    "\n",
    "\n",
    "##Create a copy of each of the questions\n",
    "Q1_Test_Tokens = Q1_Test.copy()\n",
    "Q2_Test_Tokens = Q2_Test.copy()\n",
    "#For each question:\n",
    "for i in range(len(Q1_Test_Tokens)):\n",
    "    #Tokenize each question and overwrite each question with its token\n",
    "    Q1_Test_Tokens.iloc[i] = Q1_Test_Tokens.iloc[i].split()\n",
    "    Q2_Test_Tokens.iloc[i] = Q2_Test_Tokens.iloc[i].split()\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "Q1_Embedded_Test = Q1_Test_Tokens.copy()\n",
    "Q2_Embedded_Test = Q2_Test_Tokens.copy()\n",
    "\n",
    "\n",
    "#For each question:\n",
    "for i in range(len(Q1_Embedded_Test)):\n",
    "  Sum_Q1 = np.zeros(300)\n",
    "  #For each word:\n",
    "  for word in Q1_Embedded_Test.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      #Create the embedding and store it inside of the list\n",
    "      Sum_Q1 = Sum_Q1 + (Embedder[word])\n",
    "  Q1_Embedded_Test.iloc[i] = Sum_Q1 / len(Q1_Test_Tokens.iloc[i])\n",
    "  #Create a list that will contain the vector embeddings for each word in the question\n",
    "  Sum_Q2 = np.zeros(300)\n",
    "  #For each word:\n",
    "  for word in Q2_Embedded_Test.iloc[i]:\n",
    "    if word in Embedder:\n",
    "      #Create the embedding and store it inside of the list\n",
    "      Sum_Q2 = Sum_Q2 + (Embedder[word])\n",
    "  Q2_Embedded_Test.iloc[i] = Sum_Q2 / len(Q2_Test_Tokens.iloc[i])\n",
    "\n",
    "#Clear up ram as we don't need the embedder anymore\n",
    "Embedder = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLxMspRpCZSt"
   },
   "source": [
    "Aggregate each question to get a vector representation of each question, using the average method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o30Dhd4fZ_DY"
   },
   "source": [
    "Initialization of what is the underlying model we will use in the Siamese architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "j_ghfDZmZ2wH"
   },
   "outputs": [],
   "source": [
    "#Creating a multi-layer perceptron neural network that will be used as the underlying architecture\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import cosine_similarity\n",
    "\n",
    "#Identify the length of the vectors used in the embeddings\n",
    "embedding_dim = 300\n",
    "\n",
    "#Identify the number of questions. This will indicate how many nodes we will use\n",
    "Questions_Count = Q2_Embedded.shape[0]\n",
    "\n",
    "#This indicates that we will be using 4 layers in our underlying architecture. ~ Can be increased/decreased\n",
    "shared_network = keras.Sequential([\n",
    "    layers.Dense(300, activation='relu', input_shape=(embedding_dim,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbyJQCvg79Vt"
   },
   "source": [
    "Initialize the Siamese network architecture with the above model as the underlying architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "isJFhYUF8HnC"
   },
   "outputs": [],
   "source": [
    "#Initialize the Siamese network architecture\n",
    "\n",
    "#Define the left and right inputs for the question pair ~ Initializing how long the vectors that represent each question is\n",
    "left_input = layers.Input(shape=(embedding_dim,))\n",
    "right_input = layers.Input(shape=(embedding_dim,))\n",
    "\n",
    "# Encode the question pair using the shared network ~ Indicate that we will input the questions both question in the pair into the shared network\n",
    "encoded_left = shared_network(left_input)\n",
    "encoded_right = shared_network(right_input)\n",
    "\n",
    "# Calculate the Euclidean distance between the encodings ~ I.e: We will use this distance function to determine the similarity between the outputs\n",
    "distance = layers.Lambda(lambda x: tf.norm(x[0] - x[1], axis=1, keepdims=True))([encoded_left, encoded_right])\n",
    "#distance=layers.Lambda(lambda x: cosine_similarity(x[0], x[1]))([encoded_left, encoded_right])\n",
    "\n",
    "# We have initialized the model. Now, just create the Siamese model\n",
    "siamese_model = keras.Model(inputs=[left_input, right_input], outputs=distance)\n",
    "\n",
    "\n",
    "#Compile the model ~ Specifies how the training should be done etc.....\n",
    "siamese_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#siamese_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_huKvpYt8Ted"
   },
   "source": [
    "Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "n3O4zfgj8P4g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9096/9096 [==============================] - 41s 4ms/step - loss: 2.4197 - accuracy: 0.5962 - val_loss: 2.4196 - val_accuracy: 0.5932\n",
      "Epoch 2/10\n",
      "9096/9096 [==============================] - 41s 5ms/step - loss: 2.4104 - accuracy: 0.5924 - val_loss: 2.4224 - val_accuracy: 0.5828\n",
      "Epoch 3/10\n",
      "9096/9096 [==============================] - 40s 4ms/step - loss: 2.4093 - accuracy: 0.5913 - val_loss: 2.4193 - val_accuracy: 0.5942\n",
      "Epoch 4/10\n",
      "9096/9096 [==============================] - 41s 5ms/step - loss: 2.4089 - accuracy: 0.5915 - val_loss: 2.4172 - val_accuracy: 0.5909\n",
      "Epoch 5/10\n",
      "9096/9096 [==============================] - 41s 5ms/step - loss: 2.4088 - accuracy: 0.5912 - val_loss: 2.4176 - val_accuracy: 0.5867\n",
      "Epoch 6/10\n",
      "9096/9096 [==============================] - 43s 5ms/step - loss: 2.4084 - accuracy: 0.5910 - val_loss: 2.4213 - val_accuracy: 0.5957\n",
      "Epoch 7/10\n",
      "9096/9096 [==============================] - 41s 4ms/step - loss: 2.4082 - accuracy: 0.5908 - val_loss: 2.4167 - val_accuracy: 0.5893\n",
      "Epoch 8/10\n",
      "9096/9096 [==============================] - 41s 4ms/step - loss: 2.4078 - accuracy: 0.5907 - val_loss: 2.4191 - val_accuracy: 0.5893\n",
      "Epoch 9/10\n",
      "9096/9096 [==============================] - 40s 4ms/step - loss: 2.4091 - accuracy: 0.5914 - val_loss: 2.4173 - val_accuracy: 0.5892\n",
      "Epoch 10/10\n",
      "9096/9096 [==============================] - 44s 5ms/step - loss: 2.4086 - accuracy: 0.5912 - val_loss: 2.4195 - val_accuracy: 0.5874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x39c0c51f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inputs has to be an array of a list of lists -> Convert from array of list to a list\n",
    "\n",
    "Q1_Inputs = []\n",
    "\n",
    "for i in range(len(Q1_Embedded)):\n",
    "    #print(i)\n",
    "    Q1_Inputs.append(Q1_Embedded.iloc[i].tolist())\n",
    "\n",
    "Q1_Inputs = np.array(Q1_Inputs)\n",
    "\n",
    "Q2_Inputs = []\n",
    "\n",
    "for i in range(len(Q2_Embedded)):\n",
    "    Q2_Inputs.append(Q2_Embedded.iloc[i].tolist())\n",
    "\n",
    "Q2_Inputs = np.array(Q2_Inputs)\n",
    "\n",
    "\n",
    "#Training of the model\n",
    "\n",
    "siamese_model.fit(\n",
    "    [Q1_Inputs, Q2_Inputs],  # Your question embeddings\n",
    "    np.array(Train[\"is_duplicate\"]),  # Similarity labels (0 for dissimilar, 1 for similar)\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.2  # You can adjust the validation split\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQzBoaQM8dxY"
   },
   "source": [
    "Finding the Training Accuracuy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jmWd7lWE8iVn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11370/11370 [==============================] - 12s 1ms/step\n",
      "Training Accuracy: 58.87%\n"
     ]
    }
   ],
   "source": [
    "train_predictions = siamese_model.predict([Q1_Inputs, Q2_Inputs])\n",
    "threshold = 0.5\n",
    "train_binary_predictions = (train_predictions > threshold).astype(int)\n",
    "from sklearn.metrics import accuracy_score\n",
    "training_accuracy = accuracy_score(Train[\"is_duplicate\"].tolist(), train_binary_predictions)\n",
    "print(f\"Training Accuracy: {training_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "POxKi7SUgbk4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31256]\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iBEcm15VdL7_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264/1264 [==============================] - 2s 1ms/step\n",
      "Test Accuracy: 63.12%\n"
     ]
    }
   ],
   "source": [
    "Q1_Inputs_Test = []\n",
    "\n",
    "for i in range(len(Q1_Embedded_Test)):\n",
    "    Q1_Inputs_Test.append(Q1_Embedded_Test.iloc[i].tolist())\n",
    "\n",
    "Q1_Inputs_Test = np.array(Q1_Inputs_Test)\n",
    "\n",
    "Q2_Inputs_Test = []\n",
    "\n",
    "for i in range(len(Q2_Embedded_Test)):\n",
    "    Q2_Inputs_Test.append(Q2_Embedded_Test.iloc[i].tolist())\n",
    "\n",
    "Q2_Inputs_Test = np.array(Q2_Inputs_Test)\n",
    "\n",
    "\n",
    "test_predictions = siamese_model.predict([Q1_Inputs_Test, Q2_Inputs_Test])\n",
    "threshold = 0.5\n",
    "test_binary_predictions = (test_predictions > threshold).astype(int)\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy = accuracy_score(Test[\"is_duplicate\"].tolist(), test_binary_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
